---
title: "CindexSimulationMetabric2"
output: html_document
date: "2025-02-27"
---

## Concordance index multiverse.

### Simulation data: Non-informative Weibull.  

Load libraries

```{r Load libraries, include=FALSE}
# Survival metrics
library(reticulate)
library(arrow)
library(caret)
library(riskRegression)
library(prodlim)
library(pec)
library(survival)
library(rhdf5)
library(randomForestSRC)
library(survAUC)
library(Hmisc)
library(dplyr)
# Plotting
library(gridExtra)
# Parallelization
library(doFuture)
library(future)
library(progressr)
library(foreach)
library(MASS)
library(flexsurv)
library(furrr)
library(pysurvivalR)
library(survivalmodels)
library(tidyverse)
library(patchwork)
library(stringr) 
```



```{r Setup, include=FALSE}
#Sys.unsetenv("RETICULATE_PYTHON")
Sys.setenv(OMP_NUM_THREADS = "1")       # Limits OpenMP to 1 thread
Sys.setenv(NUMBA_NUM_THREADS = "1")     # Limits Numba to 1 thread
Sys.setenv(MKL_NUM_THREADS = "1")       # Limits Intel MKL to 1 thread
Sys.setenv(KMP_WARNINGS = "0")          # Disables OpenMP warnings
Sys.setenv(OPENBLAS_NUM_THREADS = "1")  # Limits OpenBLAS to 1 thread

# #Sys.setenv(NUMBA_DISABLE_JIT = "0")  
# library(reticulate)

#use_condaenv("/opt/homebrew/Caskroom/miniforge/base/envs/py-rstudio", required=TRUE)
#use_virtualenv("~/.virtualenvs/venv-DeSurv_python_R")
#use_python("/opt/homebrew/Caskroom/miniforge/base/envs/venv-DeSurv3/bin/python3.9")
#py_config()
```

### Introduction

https://pmc.ncbi.nlm.nih.gov/articles/PMC7731987/
https://cran.r-project.org/web/packages/SurvRegCensCov/vignettes/weibull.pdf
https://onlinelibrary.wiley.com/doi/epdf/10.1002/9781118032985.ch2?saml_referrer
https://en.wikipedia.org/wiki/Weibull_distribution#Alternative_parameterizations
https://www.mas.ncl.ac.uk/~nmf16/teaching/mas3311/week09.pdf

A parametric weibull regression model can be parametrize in various ways. Often the **Weibull accelerated failure time** is used, and can be performed with \pkg{survreg} in \prog{R}. The distribution is being cast into a location-scale framework following chapter 2.2 of Kalbfleisch and Prentice: 


$$\log T = Y = \mu + \boldsymbol{\alpha}^T \mathbf{z} + \sigma W,$$
where $\mathbf{z}$ are set of covariates, and $W$ has the extreme value distribution.

However, the alternative Weibull proportional hazard parametrization is more commonly used in medicine and it can be defined in terms of the following baseline hazard: 

$$h(x|\mathbf{z}) = (\gamma \lambda t^{\gamma - 1}) \exp(\boldsymbol{\beta}^T \mathbf{z}).$$

where the parameters align with the Weibull AFT as follows: 

\begin{align*}
\gamma & =  1/\sigma, \\
\lambda & = \exp(-\mu/\sigma), \\
\boldsymbol{\beta} & = -\boldsymbol{\alpha}/\sigma,
\end{align*}


We can simulate times by using the following:
Then the ratio of times for a covariate with value $z_1$ versus values $z_0$, with parameter estimate $\beta$,  can then be computed as:

The $p$th percentile of the (covariate-adjusted) Weibull distribution occurs at 
$$t_p = \left[ \frac{-\log p}{\lambda e^{\boldsymbol{\beta}^T \mathbf{z}}} \right]^{1/\gamma}.$$

To estimate the oracle, we need the survival probabilities per patient, per dataset: 

$$S(t|z) = e^{-\lambda t^{\gamma} e^{\beta^{T}z}}$$

Where parameters estimated during the data generation such $gamma$ and $lambda$ are required. Therefore, they are stored during synthetic data generation. Subsequently, the RMST is estimated for each patient based on the survival probabilities, and utilized to estimate the oracle C-index. 



```{r Load functions}
source("./CindexHelperFunctions.R")
```


```{r Load simulated datasets}

# Load the synthetic datasets and restore attributes
datasets <- load_synthetic_datasets("./Datasets/Synthetic/10d_synthetic_WT_WTc.rds")

datasets_per_lambda <- 100
factors <- c(0, 0.5, 1.5, 3, 7, 13)
```

### Crossvalidation

Only with CoxPH

```{r Cross-validation, message=FALSE}

set.seed(123)

# Number of folds
K <- 5  
n <- nrow(datasets[[1]][[1]])  

# List of numeric column for standarization
numeric_cols <- c("X1", "X2", "X3", "X4", "X9")

# Shuffle and create folds
#indices <- sample(seq_len(n))
#folds <- cut(indices, breaks = K, labels = FALSE)

stacked_predictions <- list() # to store predictions ifo
stacked_all_betas <- list() # to store oracle info

# Loop through datasets
for (i in seq_along(datasets)){
  lambda_list <- list()
  betas_list <- list()
  
  for (j in seq_along(datasets[[i]])) {
    # Extract dataset
    dat <- data.frame(datasets[[i]][[j]])
    attributes(dat) <- attributes(datasets[[i]][[j]])
    n <- nrow(dat)

    if (!any(is.infinite(dat$censoring_time))) { # if no censoring 
      folds <- createFolds(dat$status, k = K, list = TRUE) 
    } else {
      indices <- sample(seq_len(n))
      folds <- cut(indices, breaks = K, labels = FALSE)
    }
    # Interesting intervals for prediction
    mb_t_max <- round(max(dat$observed_time))
    ts_scaled <- seq(0, mb_t_max, 1)
    fold_list <- list()
    fold_list_beta <- list()
    
    for (k in seq_len(K)) {
      # Define test and training indices
      if (!any(is.infinite(dat$censoring_time))) { # if no censoring 
        mb_test_idx  <- folds[[k]]
      } else {
        mb_test_idx  <- which(folds == k)
      }
      mb_train_idx <- setdiff(seq_len(nrow(dat)), mb_test_idx)
          
      # Subset the dataset
      mb_train <- dat[mb_train_idx, ]
      mb_test  <- dat[mb_test_idx, ]
      
      # Scale continuous 
      means <- sapply(mb_train[, numeric_cols], mean)
      sds   <- sapply(mb_train[, numeric_cols], sd)
    
      mb_train[, numeric_cols] <- scale(mb_train[, numeric_cols],
                                        center = means, scale = sds)
      mb_test[, numeric_cols]  <- scale(mb_test[, numeric_cols],
                                        center = means, scale = sds)
      
      # Extract only the covariates used for predictions
      test_covariates <- mb_test[, c("X1", "X2", "X3", "X4",
                                     "X5", "X6", "X7", "X8", "X9")]

      # Fit models
      cat("Dataset:", i, ".", j, '\n')
      cat("Fold:", k, '\n')
      #cat("tmax:", t_train_max, "\n")
      cat('Train set dimensions:', dim(mb_train), '\n')
      cat('Train set event:', mean(mb_train$status == 1)*100, '\n')
      cat('Train set censoring:', mean(mb_train$status == 0)*100, '\n')
      cat('Test set dimensions:', dim(mb_test), '\n')
      cat('Train set event:', mean(mb_test$status == 1)*100, '\n')
      cat('Test set censoring:', mean(mb_test$status == 0)*100, '\n')
      cat('\n')
      
      # 2) Predict risk Cox PH
      # Fit with cox ph
      cox_ph <- survival::coxph(Surv(observed_time, status) ~ X1 + X2 + 
                                  X3 + X4 + X5 + X6 + 
                                  X7 + X8 + X9, 
                                   data = mb_train)
      
      # Tracking model uncertainty
      coefs <- summary(cox_ph)$coefficients
      conf  <- confint(cox_ph)
      #bh <- basehaz(cox_ph, centered = FALSE)

      # Predict survival object
      sf <- survfit(cox_ph, newdata = mb_test)
      surv_cox <- t(sf$surv)
      time_grid <- sf$time
      
      #Interpolate
      surv_cox_int <- apply(surv_cox, 1, function(s){
         approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
      })
      
      surv_cox_int <- t(surv_cox_int)
      rownames(surv_cox_int) <- rownames(mb_test)
      colnames(surv_cox_int)  <- ts_scaled
      
      fold_results <- data.frame(dataset = i, 
                                 version = j,
                                 cv_fold = k,
                             patients_ids = rownames(mb_test),
                             event_time = mb_test$time, 
                             observed_time = mb_test$observed_time, ### observed_time now!
                             observed_status = mb_test$status,
                             lambda = attributes(dat)$lambda, 
                             cp = attributes(dat)$cp,
                             seed = attributes(dat)$seed,
                             CoxPH = as.data.frame(surv_cox_int),
                             covariates = test_covariates)
      # Append fold results to the list
      fold_list[[k]] <- fold_results
      
      
      ### oracle
      # Info from the data generation process 
      lambda0 = attributes(dat)$lambda0
      gamma = attributes(dat)$gamma
      betas = attributes(dat)$beta # data generation betas, not estimated by Cox
      mu = attributes(dat)$mu
      sigma = attributes(dat)$sigma
      alpha = attributes(dat)$alpha

      X_mat <- as.matrix(mb_test[, paste0("X", 1:9)]) # covariates

      # Compute linear predictor and lambda_i
      lp_oracle <- X_mat %*% as.vector(betas) # computes beta*x_i
      lambda_i  <- as.numeric(lambda0 * exp(lp_oracle)) # lambda_i = lambda_0 * exp(bet*X_i)
      # Compute survival for the times of interest
      S_mat <- outer(lambda_i, ts_scaled, function(l, t) exp(- (l * t^gamma)))

      fold_beta_results <- data.frame(
                            dataset = i,
                            version = j,
                            cv_fold = k,
                            patients_ids = rownames(mb_test),
                            event_time = mb_test$time, 
                            observed_time = mb_test$observed_time, ### observed_time now!
                            observed_status = mb_test$status,
                            oracle_surv = as.data.frame(S_mat)
                            
                          )
    fold_list_beta[[k]] <- fold_beta_results
    
    }
    # Cox-predictions
    stacked       <- do.call(rbind, fold_list)
    # Oracle
    stacked_betas <- do.call(rbind, fold_list_beta)
    
    # Compute measures for Cox
    df <- stacked[, 1:10]
    res <- compute_measures(stacked, "CoxPH")
    df[[paste0("ExpMort.CoxPH")]] <- res$exp_mort
    df[[paste0("RMST.CoxPH")]] <- res$rmst
    
    surv_mat <- as.data.frame(res$surv_mat)
    df <- cbind(df, surv_mat)
    lambda_list[[j]] <- df
    
    # Compute measures for Oracle
    res <- compute_measures(stacked_betas, "oracle_surv")
    stacked_betas[[paste0("Oracle.RMST.CoxPH")]] <- res$rmst
    betas_list[[j]]  <- stacked_betas
    
  }
  
  stacked_predictions[[i]] <- lambda_list
  
  stacked_all_betas[[i]] <- betas_list
  
}


#saveRDS(stacked_predictions, "./Results/WeibullSyntheticStackedPredictions.rds")

#stacked_predictions <- readRDS( "./Results/WeibullSyntheticStackedPredictions.rds")
```


### C-index point estimates

```{r Load results checkpoint}
#results_per_lambda <- readRDS("./Results/WeibullSynthetic.rds")
datasets_per_lambda <- 100
factors <- c(0, 0.5, 1.5, 3, 7, 13)
```

Here we will focus only in point estimates (we do not perform bootstrap). 


```{r C-index Point estimates, eval=FALSE}

results_rmst_point_estim <- vector("list", length = datasets_per_lambda)
results_rmst_point_estim2 <- vector("list", length = datasets_per_lambda)
results_surv_point_estim <- vector("list", length = datasets_per_lambda)

results_rmst_oracle <- vector("list", length = datasets_per_lambda)
results_rmst_oracle2 <- vector("list", length = datasets_per_lambda)
results_surv_oracle <- vector("list", length = datasets_per_lambda)

cps <- list()

model <- "CoxPH"
rmst <- "RMST.CoxPH"

results_per_lambda <- vector("list", length = length(factors))

for (l in seq_along(stacked_predictions)) { 
  cat("Censoring set up ", l, "\n")
  sub <- stacked_predictions[[l]]
  # Loop through each dataset
  for (dataset in seq_along(sub)) {
    # Merge to get the oracle
    subset_preds <- sub[[dataset]]
    subset_oracle <- stacked_all_betas[[l]][[dataset]]
    # add the oracle
    subset <- merge(subset_preds, subset_oracle[, c("patients_ids", "Oracle.RMST.CoxPH")], 
                    by.x = "patients_ids", by.y = "patients_ids", all=TRUE)
    # Get the size of dataset
    dataset_size <- nrow(subset[[dataset]])
    # Get lambda 
    #lambda = unique(subset[[dataset]]$lambda)
    # Get censoring-percentage
    #cps[[dataset]] = unique(subset[[dataset]]$cp)

    rmst <- "RMST.CoxPH"
    
    rmst_oracle <- "Oracle.RMST.CoxPH"
    
    cat("Point estimate for lambda dataset ", dataset, "and column ", rmst, "\n")
    
    # At maximum uncensored times as tau  
    results_rmst_point_estim[[dataset]][[rmst]] <- metrics.wrapper(predicted =
                                                           subset[[rmst]], 
                                                censoring = subset$observed_status, 
                                                time = subset$observed_time, 
                                                implementation = 
                                                list("Hmisc::rcorr.cens",
                                                    "pec::cindex", 
                                                    "survival.n", 
                                                    "survival.n/G2"), 
                                                eval.times = "resample_max_uncensored_time")
    
    results_rmst_oracle[[dataset]][[rmst]] <- metrics.wrapper(predicted =
                                                           subset_oracle[[rmst_oracle]],
                                                censoring = rep(1, nrow(subset_oracle)),
                                                time = subset_oracle$event_time,
                                                implementation =
                                                list("Hmisc::rcorr.cens",
                                                    "pec::cindex",
                                                    "survival.n",
                                                    "survival.n/G2"),
                                                eval.times = "resample_max_uncensored_time")
    # At 100 as tau
    results_rmst_point_estim2[[dataset]][[rmst]] <- metrics.wrapper(predicted =
                                                           subset[[rmst]], 
                                                censoring = subset$observed_status, 
                                                time = subset$observed_time, 
                                                implementation = 
                                                list("Hmisc::rcorr.cens",
                                                    "pec::cindex",
                                                    "survival.n",
                                                    "survival.n/G2"), 
                                                eval.times = 100)
    
    results_rmst_oracle2[[dataset]][[rmst]] <- metrics.wrapper(predicted =
                                                           subset_oracle[[rmst_oracle]],
                                                censoring = rep(1, nrow(subset_oracle)),
                                                time = subset_oracle$event_time,
                                                implementation =
                                                list("Hmisc::rcorr.cens",
                                                    "pec::cindex",
                                                    "survival.n",
                                                    "survival.n/G2"),
                                                eval.times = 100)
    

    cat("Point estimate for Distribution for Model = ", model, "\n")
      
    surv_mat = subset[, grep("^CoxPH", names(subset), 
                                        value = TRUE), drop = FALSE]
    
    surv_mat_oracle = subset_oracle[, grep("^oracle_surv", names(subset_oracle),
                                                      value = TRUE), drop = FALSE]
    

    colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
    colnames(surv_mat_oracle) <- as.numeric(sub(".*\\V", "", colnames(surv_mat_oracle)))
    
    results_surv_point_estim[[dataset]][[model]] <- metrics.wrapper(surv_matrix = surv_mat, 
                                                      censoring = subset$observed_status, 
                                                      time = subset$observed_time, 
                                                      implementation = list("pycox.Ant", 
                                                                            "pycox.Adj.Ant"))

    results_surv_oracle[[dataset]][[model]] <- metrics.wrapper(surv_matrix = surv_mat_oracle,
                                                  censoring = rep(1, nrow(subset_oracle)),
                                                  time = subset_oracle$event_time,
                                                  implementation = list("pycox.Ant",
                                                                        "pycox.Adj.Ant"))
  }

  results_per_lambda[[l]] <- list(results_rmst_point_estim,
                                  results_surv_point_estim, 
                                  results_rmst_point_estim2, 
                                  results_rmst_oracle,
                                  results_surv_oracle, 
                                  results_rmst_oracle2
                                  )
}

#saveRDS(results_per_lambda, "./Results/WeibullSynthetic.rds")

```



### Plot results

```{r Calculate the censoring percentages}

# Load the synthetic datasets and restore attributes
datasets <- load_synthetic_datasets("./Datasets/Synthetic/10d_synthetic_WT_WTc.rds")

cp_by_lambda <- list()

for (i in seq_along(datasets)) {
  for (j in seq_along(datasets[[i]])) {
    cp <- attributes(datasets[[i]][[j]])$cp
    lambda_val <- attributes(datasets[[i]][[j]])$lambda
    lambda_str <- as.character(lambda_val) 
    cp_by_lambda[[lambda_str]] <- c(cp_by_lambda[[lambda_str]], cp)
  }
}

cp_summary <- data.frame(
  Lambda = as.numeric(names(cp_by_lambda)),
  MinCP = sapply(cp_by_lambda, min),
  MaxCP = sapply(cp_by_lambda, max)
)

cp_summary <- cp_summary[order(cp_summary$Lambda), ]

cp_summary

cp_summary$CP_Label <- ifelse(cp_summary$MinCP == cp_summary$MaxCP, paste0(cp_summary$MaxCP, "%"), paste0( cp_summary$MinCP, "%–", cp_summary$MaxCP, "%"))

```


```{r Extract information}
all_results <- list()
surv_entries <- list()
rmst_entries <- list()
rmst_entries2 <- list()
surv_entries_or <- list()
rmst_entries_or <- list()
rmst_entries_or2 <- list()

for (l in seq_along(results_per_lambda)) {


  list_rsmt_pe <- results_per_lambda[[l]][[1]]
  list_surv_pe <- results_per_lambda[[l]][[2]]
  list_rmst_pe2 <- results_per_lambda[[l]][[3]]
  list_rsmt_or <- results_per_lambda[[l]][[4]]
  list_surv_or <- results_per_lambda[[l]][[5]]
  list_rsmt_or2 <- results_per_lambda[[l]][[6]]
  
  
  lambda <- factors[[l]]

  raw_rmst <- extract_rmst_entries_synthetic2(list_rmst_pe = list_rsmt_pe,
                                             lambda = lambda, model ="CoxPH")
  raw_surv <- extract_surv_entries_synthetic2(list_surv_pe = list_surv_pe,
                                             lambda = lambda,  model ="CoxPH")
  raw_rmst2 <- extract_rmst_entries_synthetic2(list_rmst_pe = list_rmst_pe2,
                                             lambda = lambda,  model ="CoxPH")
  raw_rmst_or <- extract_rmst_entries_synthetic2(list_rmst_pe = list_rsmt_or,
                                            lambda = lambda,  model ="Oracle")
  raw_surv_or <- extract_surv_entries_synthetic2(list_surv_pe = list_surv_or,
                                            lambda = lambda, model ="Oracle")
  raw_rmst_or2 <- extract_rmst_entries_synthetic2(list_rmst_pe = list_rsmt_or2,
                                            lambda = lambda,  model ="Oracle")
  
  # Flatten each entry safely
  rmst_entries[[l]] <- do.call(rbind, raw_rmst)
  rmst_entries2[[l]] <- do.call(rbind, raw_rmst2)
  surv_entries[[l]] <- do.call(rbind, raw_surv)
  rmst_entries_or[[l]] <- do.call(rbind, raw_rmst_or)
  surv_entries_or[[l]] <- do.call(rbind, raw_surv_or)
  rmst_entries_or2[[l]] <- do.call(rbind, raw_rmst_or2)

  rmst_entries[[l]]$Measure <- "Point Estim."
  rmst_entries2[[l]]$Measure <- "Point Estim."
  surv_entries[[l]]$Measure <- "Point Estim."
  rmst_entries_or[[l]]$Measure <- "Oracle"
  surv_entries_or[[l]]$Measure <- "Oracle"
  rmst_entries_or2[[l]]$Measure <- "Oracle"


  c_s <- c("Hmisc::rcorr.cens",
           "pysurvival",
           "SurvMetrics::Cindex", 
           "lifelines",
           "sksurv.censored")
  
  rmst_entries[[l]]$Measure <- ifelse(rmst_entries[[l]]$Metric %in% c_s, "tilde(C)~(RMST)",
                                      "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))")
  rmst_entries2[[l]]$Measure <- "tilde(C)[tau]~(RMST~tau==100~months)"
  surv_entries[[l]]$Measure <- "tilde(C)[td]~(Survival~Distribution)"
  
  
  rmst_entries[[l]]$Notation <- ifelse(rmst_entries[[l]]$Metric %in% c_s, "tilde(C)~(RMST)",
                                       "tilde(C)[tau]~(RMST~tau)")
  rmst_entries2[[l]]$Notation <- "tilde(C)[tau]~(RMST~tau)"
  surv_entries[[l]]$Notation <- "tilde(C)[td]~(Survival~Distribution)"
  
  rmst_entries_or[[l]]$Measure <- ifelse(rmst_entries_or[[l]]$Metric %in% c_s, "tilde(C)~(RMST)",
                                      "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))")
  rmst_entries_or2[[l]]$Measure <- "tilde(C)[tau]~(RMST~tau==100~months)"
  surv_entries_or[[l]]$Measure <- "tilde(C)[td]~(Survival~Distribution)"
  
  
  rmst_entries_or[[l]]$Notation <- ifelse(rmst_entries_or[[l]]$Metric %in% c_s, "tilde(C)~(RMST)",
                                       "tilde(C)[tau]~(RMST~tau)")
  rmst_entries_or2[[l]]$Notation <- "tilde(C)[tau]~(RMST~tau)"
  surv_entries_or[[l]]$Notation <- "tilde(C)[td]~(Survival~Distribution)"
  
  
  all_results[[l]] <- bind_rows(rmst_entries[[l]],
                                rmst_entries2[[l]],
                                surv_entries[[l]],
                                rmst_entries_or[[l]],
                                rmst_entries_or2[[l]],
                                surv_entries_or[[l]]
                                )
}

final_df <- do.call(rbind, all_results)

final_df <- merge(final_df, cp_summary[, c("Lambda", "CP_Label")], by = "Lambda")


imp_map <- data.frame(
  Metric = c("Hmisc::rcorr.cens", "SurvMetrics::Cindex", "lifelines", "pysurvival",
             "sksurv.censored", "pec::cindex", "survC1::Est.Cval", "survival.n",
             "survival.n/G2", "pycox.Ant", "pycox.Adj.Ant", "sksurv.ipcw"),
  Implementation = c("Hmisc", "SurvMetrics", "lifelines", "pysurvival", "sksurv.censored",
  "pec", "survC1", 'survival.n', 'survival.n/G2',
  "pycox.Ant", "pycox.Adj.Ant", "sksurv.ipcw"),
  stringsAsFactors = FALSE
)

final_df <- merge(final_df, imp_map, by = "Metric", all.x = TRUE)


#saveRDS(final_df, "./Results/SyntheticObjects/10d_c_ind_results_processed_TcW_123.rds")
```


```{r Plot, fig.width=10, fig.height=11}

ggplot(final_df,
       aes(x = as.factor(Lambda), y = Cindex, fill = Measure)) +
  stat_summary(fun = median, geom = "point", shape = 21, 
               size = 2, fill = "black", color = "black", 
               position = position_dodge(width = 0.9)) +
  geom_violin(alpha = 0.2, color = "black") +
  facet_wrap(~Implementation + Measure + Model, labeller = label_parsed) +
  scale_fill_discrete(labels = scales::parse_format()) +  # enables LaTeX-style legend
  labs(
       y = "C-index",
       x = "Censoring percentage",
       fill = "Input Type") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "bottom")
  
```


```{r Plot with oracle fig.width=8, fig.height=9}

dat_plot <- final_df
oracle_means <- dat_plot |>
  filter(Model == "Oracle") |>
  group_by(Implementation, Measure) |>
  summarise(
    oracle_mean = mean(Cindex, na.rm = TRUE),
    oracle_sd   = sd(Cindex, na.rm = TRUE),
    .groups = "drop"
  )

ggplot(dat_plot[dat_plot$Model == "CoxPH", ], 
       aes(x = CP_Label, y = Cindex, fill = Notation)) +
  geom_violin(alpha = 0.2, color = "black") +
  stat_summary(
    fun = median, geom = "point", shape = 21,
    size = 2, fill = "black", color = "black",
    position = position_dodge(width = 0.9)
  ) +
  # Oracle SD band
  geom_rect(
    data = oracle_means,
    aes(xmin = -Inf, xmax = Inf,
        ymin = oracle_mean - oracle_sd,
        ymax = oracle_mean + oracle_sd),
    fill = "grey40", alpha = 0.1, inherit.aes = FALSE
  ) +
  # Oracle horizontal mean line
  geom_hline(
    data = oracle_means,
    aes(yintercept = oracle_mean),
    color = "black", linetype = "dashed", linewidth = 0.7,
    inherit.aes = FALSE
  ) +
  facet_wrap(~Implementation + Measure, labeller = label_parsed) +
  scale_fill_discrete(labels = scales::parse_format()) +
  labs(y = "C-index", x = "Censoring percentage") +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )

```

```{r Plot subset fig.width=8, fig.height=9}

final_df <- final_df[final_df$Model == "CoxPH", ]

dat <- final_df[(final_df$Metric %in% c("Hmisc::rcorr.cens", "pec::cindex", "pycox.Ant")),]

notation_levels <- levels(factor(dat$Notation)) 

ggplot(dat,
       aes(x = CP_Label, y = Cindex, fill = Notation)) +
  stat_summary(fun = median, geom = "point", shape = 21, 
               size = 2, fill = "black", color = "black", 
               position = position_dodge(width = 0.9)) +
  geom_violin(alpha = 0.2, color = "black") +
  facet_wrap(~Implementation + Measure, labeller = label_parsed) +
  scale_fill_discrete(labels = scales::parse_format()) +  
  labs(
       y = "C-index",
       x = "Censoring percentage") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "bottom")
```

```{r Difference to the 0% censoring fig.width=8, fig.height=9}

final_df <- final_df[final_df$Model == "CoxPH", ]

final_df$GroupID <- paste(final_df$Implementation, final_df$Measure, final_df$Dataset, sep = "_")

df_wide <- reshape(
  final_df[, c("GroupID", "Lambda", "Cindex")],
  timevar = "Lambda",
  idvar = "GroupID",
  direction = "wide"
)

group_split <- do.call(rbind, strsplit(df_wide$GroupID, "_"))
df_wide$Implementation <- group_split[, 1]
df_wide$Measure <- group_split[, 2]
df_wide$Dataset <- group_split[, 3]


df_wide <- df_wide[, c("Implementation", "Measure", "Dataset", sort(names(df_wide)[grep("Cindex.", names(df_wide))]))]

# from previous dat
notation_levels <- levels(factor(dat$Notation)) 

lambda_cols <- grep("^Cindex\\.", names(df_wide), value = TRUE)
lambda_vals <- sub("Cindex\\.", "", lambda_cols)
lambda_vals <- sort(as.numeric(lambda_vals))

# Generate all diffs from baseline (lambda = 0)
for (l in lambda_vals[lambda_vals != 0]) {
  df_wide[[paste0("Diff_0_vs", l)]] <- df_wide[[paste0("Cindex.", l)]] - df_wide$Cindex.0
}

diff_cols <- grep("^Diff_0_vs", names(df_wide), value = TRUE)

# Melt into long format
df_diff_long <- reshape(
  df_wide[, c("Implementation", "Measure", "Dataset", diff_cols)],
  varying = diff_cols,
  v.names = "CindexDiff",
  timevar = "Lambda",
  times = sub("Diff_0_vs", "", diff_cols),
  direction = "long"
)


dat <- df_diff_long[(df_diff_long$Implementation %in% c("Hmisc", "pec", "pycox.Ant")),]

# Manual relabeling
dat$DeltaCP_Label <- dplyr::case_when(
  dat$Lambda == "0.5" ~ "14.2%–21.1%",
  dat$Lambda == "1.5" ~ "30.8%–39.7%",
  dat$Lambda == "3"   ~ "43.3%–51.2%",
  dat$Lambda == "7"   ~ "58.4%–65.8%",
  dat$Lambda == "13"  ~ "68.6%–75.7%",
  TRUE ~ NA_character_
)

# Manual relabeling
dat$Notation <- dplyr::case_when(
  dat$Implementation == "pec" ~ "tilde(C)[tau]~(RMST~tau)",
  dat$Implementation == "pycox.Ant"   ~ "tilde(C)[td]~(Survival~Distribution)",
  dat$Implementation == "Hmisc" ~ "tilde(C)~(RMST)",
  TRUE ~ NA_character_
)

pastel_colors <- c(
  "tilde(C)[tau]~(RMST~tau)" = "#FB9A99",   
  "tilde(C)[td]~(Survival~Distribution)" = "#B2DF8A",
  "tilde(C)~(RMST)" = "#A6CEE3"
)

ggplot(dat, aes(x = as.factor(DeltaCP_Label), y = CindexDiff, fill = Notation)) +
  geom_violin(alpha = 0.2, color = "black") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  stat_summary(fun = median, geom = "point", shape = 21, 
             size = 2, fill = "black", color = "black", 
             position = position_dodge(width = 0.9)) +
  geom_violin(alpha = 0.2) +
  facet_wrap(~Implementation + Measure, labeller = label_parsed) +
  scale_fill_manual(values = pastel_colors, labels = scales::parse_format())+
  labs(
    x = "Censoring percentage",
    y = "C-index difference from baseline (0% censoring)"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )

```



Looking at the comparable/concordant of survival.n and survival.n/G2


```{r Comparable and concordant pairs info}
pairs <- list()

for (l in seq_along(results_per_lambda)) {

  list_rmst <- results_per_lambda[[l]][[1]]

  lambda <- factors[[l]]

  raw_rmst <- extract_rmst_pairs(list_rmst, lambda)

  pairs[[l]] <- raw_rmst

}

df_pairs <- do.call(rbind, pairs)
df_pairs <- data.frame(lapply(df_pairs, as.numeric))

df_pairs$survival.n.G2.Comparable <- df_pairs$survival.n.G2.CP + df_pairs$survival.n.G2.DP
df_pairs$survival.n.Comparable <- df_pairs$survival.n.CP + df_pairs$survival.n.DP

df_pairs$survival.n.G2.Concordant <- df_pairs$survival.n.G2.CP 
df_pairs$survival.n.Concordant <- df_pairs$survival.n.CP
```


```{r Ploting comparable concordant, fig.width=8, fig.height=10}
# Reshape to long format
df_long <- df_pairs[,
  c("survival.n.G2.Concordant", "survival.n.G2.Comparable", 
         "survival.n.Concordant", "survival.n.Comparable", "lambda", "dataset")] %>%
  pivot_longer(
    cols = starts_with("survival.n"),
    names_to = "Metric",
    values_to = "Value"
  ) %>%
  mutate(
    Group = case_when(
      str_detect(Metric, "G2") ~ "survival.n/G2",
      TRUE ~ "survival.n"
    )
  )

df_long <- df_long %>%
  mutate(
    PairType = case_when(
      grepl("Comparable", Metric) ~ "Comparable",
      grepl("Concordant", Metric) ~ "Concordant",
      TRUE ~ "Other"
    )
  )

df_long$Group <- factor(df_long$Group, levels = c(
  "survival.n",
  "survival.n/G2"
))

#df_long$cp_value <- cp_list[match(df_long$lambda, factors)]

facet_common <- facet_wrap(~Implementation + Measure, labeller = label_parsed)

dat <- final_df[(final_df$Metric %in% c("survival.n", "survival.n/G2") & 
                   (final_df$Measure %in% "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))")),]


lambda_label_map <- dat[, c("Lambda", "CP_Label")] |> 
  distinct() |> 
  rename(lambda_numeric = Lambda)

p2 <- ggplot(dat,
       aes(x = CP_Label, y = Cindex, fill = Notation)) +
  stat_summary(fun = median, geom = "point", shape = 21, 
               size = 2, fill = "black", color = "black", 
               position = position_dodge(width = 0.9)) +
  geom_violin(alpha = 0.2, color = "black") +
  facet_wrap(~Implementation + Measure, labeller = label_parsed) +
  ylim(0.45, 0.75) + 
  scale_fill_discrete(labels = scales::parse_format()) +  # enables LaTeX-style legend
  labs(
       y = "C-index",
       x = "Censoring percentage",
       fill = "Input Type") +
  theme_minimal(base_size = 12)  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")


# Add CP_Label to df_long
df_long <- df_long |>
  left_join(lambda_label_map, by = c("lambda" = "lambda_numeric"))
df_long$Measure <- "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))"

p1 <- ggplot(df_long, aes(x = CP_Label, y = Value, color = PairType)) +
  geom_boxplot(alpha = 0.3, fill = "white", outlier.shape = NA, size = 0.7) +
  facet_wrap(~ Group + Measure, labeller = label_parsed) +
  scale_color_manual(
    values = c("Comparable" = "#377EB8",   # blue
               "Concordant" = "#E41A1C"),  # red
    labels = c("Comparable" = "Weighted Sum Comparable",
               "Concordant" = "Weighted Sum Concordant")
  ) +
  labs(x = "", y = "Number of pairs", color = NULL) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  )


p2

p1
```

