---
title: "5-fold Cross-validation for METABRIC"
output: html_document
date: "2025-04-15"
---

# Introduction

Here we perform a 5-fold cross-validation of METABRIC, and compute the C-index with multiple implementations and input transformations. 

First we load the libraries

```{r Load libraries}
# Survival metrics
library(reticulate)
library(arrow)
library(caret)
library(riskRegression)
library(prodlim)
library(pec)
library(survival)
library(rhdf5)
library(randomForestSRC)
library(Hmisc)
library(dplyr)
# Plotting
library(gridExtra)
# Parallelization
library(doFuture)
library(future)
library(progressr)
library(foreach)
library(MASS)
library(flexsurv)
library(furrr)
library(pysurvivalR)
library(survivalmodels)
library(tidyr)
library(ggplot2)
library(patchwork)
```


```{r setup}
#Sys.unsetenv("RETICULATE_PYTHON")
Sys.setenv(OMP_NUM_THREADS = "1")       # Limits OpenMP to 1 thread
Sys.setenv(NUMBA_NUM_THREADS = "1")     # Limits Numba to 1 thread
Sys.setenv(MKL_NUM_THREADS = "1")       # Limits Intel MKL to 1 thread
Sys.setenv(KMP_WARNINGS = "0")          # Disables OpenMP warnings
Sys.setenv(OPENBLAS_NUM_THREADS = "1")  # Limits OpenBLAS to 1 thread

# #Sys.setenv(NUMBA_DISABLE_JIT = "0")  
# library(reticulate)

#use_condaenv("/opt/homebrew/Caskroom/miniforge/base/envs/py-rstudio", required=TRUE)
#use_virtualenv("~/.virtualenvs/venv-DeSurv_python_R")
#use_python("/opt/homebrew/Caskroom/miniforge/base/envs/venv-DeSurv3/bin/python3.9")
#py_config()
```

We also load a set of helper functions used throughout our analysis:

```{r load functions}
source("./CindexHelperFunctions.R")
```

## Load the data

Here, we load the pre-processed METABRIC data that was generated using the 
`preprocessing_biocportal_metabric.R` script.

```{r load data}
# read the processed file
mb <- read.table("./Datasets/metabric_preprocess_multiverse.csv", 
                 sep = "," , header = TRUE)

# Keep the id as index
rownames(mb) <- mb$PATIENT_ID

# Remove the id column
mb <- mb[,2:12]

# Mapping
var_map <- c(
  "MKI67" = "X1",
  "EGFR" = "X2",
  "ERBB2" = "X3",
  "PGR" = "X4",
  "HORMONE_THERAPY" = "X5",
  "RADIO_THERAPY" = "X6",
  "CHEMOTHERAPY" = "X7",
  "ER_IHC" = "X8",
  "AGE_AT_DIAGNOSIS" = "X9",
  "OS_MONTHS" = "time",
  "OS_STATUS" = "status"
)

# Mapping colnames
names(mb)[names(mb) %in% names(var_map)] <- var_map[names(mb)[names(mb) %in% names(var_map)]]


summary(mb$time)


#mb <- mb[order(mb$time, -mb$status),]
```


The data contains information about `r nrow(mb)` individuals, with 
`r sum(mb$status == 1)` events and `r sum(mb$status == 0)` censored 
observations. The survival times are measured in months.

## Evaluation of predictive performance using a data holdout approach

Here, we will evaluate the predictive performance of various survival models 
using a holdout approach. We will split the data into a training set (80%) and a 
test set (20%), fit the models on the training set, and then evaluate their 
performance on the test set. We repeated this 5 times to ensure robustness of 
the results. The data splits used here match the ones used in the 5-fold 
cross-validation results. For each data split, bootstrap samples of the test set
will be generated to estimate the uncertainty of the estimated C-index values. 

### Model hyperparameters

For the hyperparameters of deep learning models that have been trained with 
`survivalmodels` package is relevant to look at the pytorch optimizer: https://raphaels1.github.io/survivalmodels/reference/get_pycox_optim.html 

Also the parameters for each model:

1) Deephit: https://raphaels1.github.io/survivalmodels/reference/deephit.html
2) DeepSurv: https://raphaels1.github.io/survivalmodels/reference/deepsurv.html
3) CoxTime: https://raphaels1.github.io/survivalmodels/reference/coxtime.html

In our analysis, we consider the same hyperparameter values used when analysing
the METABRIC in the original publications for DeepHit, DeepSurv, and CoxTime. 
For RSF, we use the default hyperparameters from the `randomForestSRC` package.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Hyper-parameter}     & \textbf{DeepSurv} & \textbf{Cox-Time} & \textbf{DeepHit}  &  \textbf{RSF} \\ \midrule
Optimizer                    & adam              & adam             & adam              & - \\
Activation                   & selu              & relu             & relu              & -\\
\# Dense Layers              & 1                 & 2                & 2                 & - \\
\# Nodes / Layer             & 41                & 32, 32           & 32, 32            & 100 trees\\
Learning Rate                & 0.0103            & 0.01             & 0.001             & - \\
Dropout                      & 0.1601            & 0.1              & 0.6               & - \\
LR Decay                     & 0.00417           & 0                & 0                 & - \\
Batch Norm                   & True             & True             & True              & - \\
Batch Size                   & 256               & 256              & 50                & - \\
Epochs                       & 500               & 512              & 100               & - \\
Early Stopping               & False             & True             & True              & - \\
mod\_alpha                   & -                & -               & 0.2               & - \\
sigma                        & -                & -               & 0.1               & - \\
cuts                         & -                & -               & 300               & - \\ \bottomrule
\end{tabular}
\caption{Hyperparameters across DeepSurv, Cox-Time, DeepHit and RSF. For DeepSurv, Cox-Time and DeepHit, hyperparameters were specified as in the analysis of the METABRIC data presented in the corresponding original publications.}
\label{tab:hyperparams}
\end{table}


### Model training

As the training of the models can take a long time, we save the results to an 
Rds file which we load here. We can use the results from 5-fold cross-validation and analyse each fold separately. 

You can load the results:

```{r load results}
# stored now in here.
stacked_predictions <-  readRDS("./Results/5foldCV_MetabricStackedPredictions.rds")

survival_curves <- readRDS("./Results/5foldCV_MetabricSurvivalCurves.rds")
```

Alternatively, perform the validation as follows:

We perform 5-fold cross-validation with stratification of outcomes between folds. We standardised numerical variables. Moreover, we interpolate predictions to a common time-grid. 

```{r model predictions, eval=FALSE}
## Reproducibility
reset_torch_seed <- function(seed = 123) {
  reticulate::py_run_string(sprintf("
import torch
import numpy as np
import random
torch.manual_seed(%d)
np.random.seed(%d)
random.seed(%d)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
", seed, seed, seed))
}

set.seed(123)
survivalmodels::set_seed(seed_R=123, seed_np = 123, seed_torch = 123) 

### Crossval
# Number of folds
K <- 5 
n <- nrow(mb)  

# Interesting times for comparison
#ts <- sort(unique(round(mb$time)))
all_predictions <- list()

# List of numeric column for standarization
numeric_cols <- c("X1", "X2", "X3", "X4", "X9")

# Create empty list for survival curves
survival_curves <- vector("list", K)

# Shuffle indices and create folds
#indices <- sample(seq_len(n))
#folds <- cut(indices, breaks = K, labels = FALSE)
folds <- createFolds(mb$status, k = K, list = TRUE) # return test sets

# Interesting intervals for prediction
mb_t_max <- round(max(mb$time))
ts_scaled <- seq(0, mb_t_max, 1)

for (k in 1:K) {

  # Define test and training indices
  #mb_test_idx  <- which(folds == k)
  #mb_train_idx <- setdiff(seq_len(n), mb_test_idx)
  
  # Define test and training indices
  mb_test_idx  <- folds[[k]]
  mb_train_idx <- setdiff(seq_len(nrow(mb)), mb_test_idx)
  
  # Subset the dataset
  mb_train <- mb[mb_train_idx, ]
  mb_test  <- mb[mb_test_idx, ]
  
  t_train_max <- max(mb_train$time)
  
  # Scale continuous 
  means <- sapply(mb_train[, numeric_cols], mean)
  sds   <- sapply(mb_train[, numeric_cols], sd)

  mb_train[, numeric_cols] <- scale(mb_train[, numeric_cols],
                                    center = means, scale = sds)
  mb_test[, numeric_cols]  <- scale(mb_test[, numeric_cols],
                                    center = means, scale = sds)

  mb_train$time <- mb_train$time
  mb_test$time <- mb_test$time
  
  # Extract only the covariates used for predictions
  test_covariates <- mb_test[, c("X1", "X2", "X3", "X4",
                                 "X5", "X6", "X7", "X8", "X9")]

  # Fit models
  #cat("Dataset:", i, ".", j, '\n')
  cat("Fold:", k, '\n')
  cat("tmax:", t_train_max, "\n")
  cat('Train set dimensions:', dim(mb_train), '\n')
  cat('Train set event:', mean(mb_train$status == 1)*100, '\n')
  cat('Train set censoring:', mean(mb_train$status == 0)*100, '\n')
  cat('Test set dimensions:', dim(mb_test), '\n')
  cat('Test set event:', mean(mb_test$status == 1)*100, '\n')
  cat('Test set censoring:', mean(mb_test$status == 0)*100, '\n')
  cat('\n')
  
  # 1) Random Forest:
  # Fit the model
  rf <- randomForestSRC::rfsrc(Surv(time, status) ~ X1 + X2 +
                                 X3 + X4 + X5 + X6 + 
                                 X7 + X8 + X9, 
                               data = mb_train, 
                               ntree = 100, 
                               ntime = 1400)
  # Predict survival object
  surv_rf_object <- predict(rf, newdata = mb_test, type = "surv")
  surv_rf <- surv_rf_object$survival
  time_grid <- surv_rf_object$time.interest
  # Interpolate to have same times in all survival curves
  surv_rf_int <- apply(surv_rf, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_rf_int <- t(surv_rf_int)
  rownames(surv_rf_int) <- rownames(mb_test)
  colnames(surv_rf_int) <- ts_scaled
  
  # 2) Predict risk Cox PH
  # Fit with cox ph 
  cox_ph <- survival::coxph(Surv(time, status) ~ X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                               data = mb_train)
  
  # Predict survival object
  sf <- survfit(cox_ph, newdata = mb_test)
  surv_cox <- t(sf$surv)
  time_grid <- sf$time 
  #Interpolate
  surv_cox_int <- apply(surv_cox, 1, function(s){
     approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  
  surv_cox_int <- t(surv_cox_int)
  rownames(surv_cox_int) <- rownames(mb_test)
  colnames(surv_cox_int)  <- ts_scaled

  reset_torch_seed()
  
  # 3) DeepSurv
  # Hyperparameters: https://github.com/jaredleekatzman/DeepSurv/blob/41eed003e5b892c81e7855e400861fa7a2d9da4f/experiments/deepsurv/models/metabric_IHC4_clinical_adam_0.json
  deepsurv_model <- survivalmodels::deepsurv(Surv(time, status) ~ 
                                               X1 + X2 + 
                                               X3 + X4 + X5 + X6 + 
                                               X7 + X8 + X9, 
                                             data = mb_train, 
                                             frac = 0.2, 
                                             dropout = 0.160087890625,
                                             optimizer = "adam",
                                             activation = "selu", 
                                             batch_norm = TRUE,
                                             num_nodes = c(41), 
                                             batch_size = 256L, 
                                             learning_rate = 0.010289691253027908, 
                                             lr_decay = 0.0041685546875, 
                                             momentum =  0.8439658203125,
                                             #weight_decay = 10.890986328125, # this is too high it cannot be 
                                             epochs = 500) 
  # Predict survival object
  surv_deepsurv <- predict(deepsurv_model,
                           newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deepsurv))
  # Interpolate
  surv_deepsurv_int <- apply(surv_deepsurv, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deepsurv_int <- t(surv_deepsurv_int)
  rownames(surv_deepsurv_int) <- rownames(mb_test)
  colnames(surv_deepsurv_int) <- ts_scaled
  
  reset_torch_seed()
  # 4) Coxtime
  # Hyperparameters from:
  # https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/cox-time.ipynb
  coxtime_model <- survivalmodels::coxtime(Surv(time, status) ~ 
                                             X1 + X2 + X3 + 
                                             X4 + X5 + X6 + 
                                              X7 + X8 + X9, 
                                           data = mb_train, 
                                           optimizer = "adam",
                                           learning_rate = 0.01,       
                                           betas = c(0.9, 0.999), # defaults
                                           activation = "relu",
                                           num_nodes = c(32L, 32L),
                                           batch_norm = TRUE,
                                           dropout = 0.1,
                                           batch_size = 256,
                                           epochs = 512,
                                           early_stopping = TRUE, 
                                           frac = 0.2)
  
  # Predict survival object
  surv_coxtime <- predict(coxtime_model,
                          newdata = mb_test, type = "surv")
  time_grid <-  as.numeric(colnames(surv_coxtime))
  # Interpolate
  surv_coxtime_int <- apply(surv_coxtime, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_coxtime_int <- t(surv_coxtime_int)
  rownames(surv_coxtime_int) <- rownames(mb_test)
  colnames(surv_coxtime_int) <- ts_scaled
  
  reset_torch_seed()
  
  # 5) Deephit
  # Hyperparameters: https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/deephit.ipynb and DeepHit paper, and also double checked with DeSurv paper (some parameters are slightly different)
  deephit_model <- survivalmodels::deephit(Surv(time, status) ~ 
                             X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                             data = mb_train, 
                             optimizer = "adam",
                             activation = "relu", # in DeSurv, ipynb
                             num_nodes = c(32L, 32L), # in ipynb and DeSurv
                             batch_norm = TRUE, # in ipynb
                             dropout = 0.6, #  0.1 in ipynb and 0.6 in DeepHit paper
                             #cuts = 1400, # in DeSurv and DeepHit, 10 and interpolation in ipynb
                             mod_alpha = 0.2, # in DeSurv, ipynb
                             early_stopping = TRUE, # in DeepHit and ipynb
                             #tolerance = 3,
                             cuts = 1400,
                             sigma = 0.1, # in DeSurv and ipynb
                             batch_size = 50, # 50 in Deephit, 256 in ipynb
                             epochs = 100, # in ipynb
                             #verbose = TRUE,
                             learning_rate = 0.001, # 0.001 in Deephit, 0.01 in ipynb
                             #betas = c(0.9, 0.999), # default adam
                             frac = 0.2) # validation 

  # Survival object
  surv_deephit <- predict(deephit_model,
                          newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deephit))# * t_train_max
  # Interpolate
  surv_deephit_int <- apply(surv_deephit, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deephit_int <- t(surv_deephit_int)
  rownames(surv_deephit_int) <- rownames(mb_test)
  colnames(surv_deephit_int) <- ts_scaled


  # Gather the survival survs too
  survival_curves[[k]] <- list(patients_ids = rownames(mb_test),
                               test_time = mb_test$time,
                               test_status = mb_test$status,
                               RSF = as.data.frame(surv_rf_int),
                               CoxPH = as.data.frame(surv_cox_int),
                               DeepHit = as.data.frame(surv_deephit_int),
                               DeepSurv = as.data.frame(surv_deepsurv_int),
                               Coxtime = as.data.frame(surv_coxtime_int),
                               covariates = test_covariates)
  
  fold_results <- data.frame(cv_fold = k,
                             patients_ids = rownames(mb_test), 
                             test_time = mb_test$time,
                             test_status = mb_test$status,
                             RSF = as.data.frame(surv_rf_int),
                             CoxPH = as.data.frame(surv_cox_int),
                             DeepHit = as.data.frame(surv_deephit_int), 
                             DeepSurv = as.data.frame(surv_deepsurv_int),
                             CoxTime = as.data.frame(surv_coxtime_int), 
                             covariates = test_covariates)
    

  # Append fold results to the list
 all_predictions[[paste("Fold", k)]] <- fold_results

}

folds_stacked_predictions <- do.call(rbind, all_predictions)


model_names <- c("RSF", "CoxPH", "DeepHit", "DeepSurv", "CoxTime")

df <- folds_stacked_predictions[, 1:4]

for (model in model_names) {
  res <- compute_measures(folds_stacked_predictions, model)
  df[[paste0("ExpMort.", model)]] <- res$exp_mort
  df[[paste0("RMST.", model)]] <- res$rmst
  
  surv_mat <- as.data.frame(res$surv_mat)
  
  df <- cbind(df, surv_mat)
}

stacked_predictions <- df


#saveRDS(stacked_predictions, "./Results/5foldCV_MetabricStackedPredictions.rds")
#saveRDS(survival_curves, "./Results/5foldCV_MetabricSurvivalCurves.rds")
```

### Plotting the survival curves

```{r survival curves, fig.height=11, fig.width=10}

p1 <- plot_survival_curves(survival_curves[[1]]$DeepHit, title = "DeepHit", seed=123)
p2 <- plot_survival_curves(survival_curves[[1]]$DeepSurv, title = "DeepSurv", seed=123)
p3 <- plot_survival_curves(survival_curves[[1]]$CoxPH, title = "CoxPH", seed=123)
p4 <- plot_survival_curves(survival_curves[[1]]$Coxtime, title = "CoxTime", seed=123)
p5 <- plot_survival_curves(survival_curves[[1]]$RSF, title = "RSF", seed=123)

(p1 | p2) / (p3 | p4) / (p5 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

```


### Bootstrapping:

Need to be able to calculate the C-index in 4 different ways: 

1) Comparing probabilities at a specific time point: risk at t

2) With Expected mortality transformation

3) With the survival distribution. 

4) With the RMST


First create bootstrap samples of 1000 patients each

```{r create samples}

set.seed(123)

# Number of bootstrapps
n_bootstraps <- 100
# Size of boostrapped sample
size_sample <- 1000

# re sample by patient id
resample_indices <- replicate(n_bootstraps, 
                              sample(stacked_predictions$patients_ids, 
                                     size = size_sample, replace = TRUE), 
                              simplify = FALSE)
```


#### Study 1: Risk at t

To evaluate how C-index varies depending on different time points t, and different values of tau:

```{r select columns for bootstrap}

subset_risk <- list()

ts = sort(unique(round(mb$time)))
for (r in seq_along(resample_indices)) {
  subset_risk[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                 model_names = "all",
                 input_type = "RiskAtT",
                 specific_time = append(10, ts[seq(51, length(ts), by = 50)]),
                 bootstrap_patient_ids = resample_indices[[r]])
}
```


```{r cindex estimate boostrap and point estimate}
### need to create a list for all the models? 
selected_risk <- colnames(subset_risk[[1]])[4:length(colnames(subset_risk[[1]]))]

# Create lists
results_risk_tau_10y <- list()
results_risk_tau_maxUT <- list()
results_risk_tau_10y_pe <- list()
results_risk_tau_maxUT_pe <- list()
# Loop
for (riskT in selected_risk) {
   cat("Calculating bootstrap at eval.time = ", median(ts), 
       "for Model = ", riskT, "\n")
   # Predictions at different times, tau at median times. 
   results_risk_tau_10y[[riskT]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = riskT,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("Hmisc::rcorr.cens",
                                                        "pec::cindex"),
                                  eval.times = 120,
                                  sampled_data = subset_risk)
   results_risk_tau_10y_pe[[riskT]] <- metrics.wrapper(predicted = 1-stacked_predictions[[riskT]], 
                                                          censoring = stacked_predictions$test_status, 
                                                          time = stacked_predictions$test_time, 
                                                          implementation = list("Hmisc::rcorr.cens",
                                                                                "pec::cindex"), 
                                                          eval.times = 120)
   # Predictions at different times, tau at maximum uncensored time
   cat("Calculating bootstrap at eval.time max uncensored time by samplefor Model = ", riskT, "\n")
   results_risk_tau_maxUT[[riskT]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = riskT,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("Hmisc::rcorr.cens",
                                                        "pec::cindex"),
                                  eval.times = "resample_max_uncensored_time", 
                                  sampled_data = subset_risk)
   results_risk_tau_maxUT_pe[[riskT]] <- metrics.wrapper(predicted = 1-stacked_predictions[[riskT]], 
                                                          censoring = stacked_predictions$test_status, 
                                                          time = stacked_predictions$test_time, 
                                                          implementation = list("Hmisc::rcorr.cens",
                                                                                "pec::cindex"), 
                                                          eval.times = "resample_max_uncensored_time")

}


```

```{r save object, include=FALSE}
# saveRDS(
#    list(
#      # results_risk_t = results_risk_t,
#      results_risk_tau_maxUT = results_risk_tau_maxUT,
#      results_risk_tau_10y = results_risk_tau_10y,
#      results_risk_tau_maxUT_pe = results_risk_tau_maxUT_pe,
#      results_risk_tau_10y_pe = results_risk_tau_10y_pe
#    ),
#    "./Results/results_5fold_multipletimes.rds"
#  )

#obj <- readRDS("./Results/results_5fold_multipletimes.rds")

#results_risk_t = obj$results_risk_t
#results_risk_tau_maxUT = obj$results_risk_tau_maxUT
#results_risk_tau_10y = obj$results_risk_tau_10y
#results_risk_tau_maxUT_pe = obj$results_risk_tau_maxUT_pe
#results_risk_tau_10y_pe = obj$results_risk_tau_10y_pe
```

```{r Plot formatting}
# Make format for plotting
#risk_t_plot        <- make_risk_plot_entries(results_risk_t)
risk_tau_maxU_plot <- make_risk_plot_entries(results_risk_tau_maxUT, results_risk_tau_maxUT_pe)
risk_tau_10_plot   <- make_risk_plot_entries(results_risk_tau_10y, results_risk_tau_10y_pe)

```


```{r Plot risk at max uncensored, fig.height=8, fig.width=10}


ggplot(risk_tau_maxU_plot[(risk_tau_maxU_plot$Metric == "Hmisc::rcorr.cens" | risk_tau_maxU_plot$Metric == "pec::cindex"),], aes(x = as.numeric(Time), y = cindex, color = Metric, group = Metric)) +
  geom_line(linewidth = 1, position = position_dodge(width = 0.3)) +
  geom_errorbar(
    position = position_dodge(width = 0.4),
    aes(ymin = lower, ymax = upper)
  ) +
  geom_point(position = position_dodge(0.4)) +
  facet_wrap(~ Model, ncol = 2) +
  ylim(0.5, 0.72) +
  theme_minimal(base_size = 12) +
  labs(x = "Time (t)", y = "C-index", color = "Implementation",
       title = "") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r Plot risk at 10 years, fig.height=8, fig.width=10}

ggplot(risk_tau_10_plot[(risk_tau_10_plot$Metric == "Hmisc::rcorr.cens" | risk_tau_10_plot$Metric == "pec::cindex"),], aes(x = as.numeric(Time), y = cindex, color = Metric, group = Metric)) +
  geom_line(linewidth = 1, position = position_dodge(width = 0.3)) +
  geom_errorbar(
    position = position_dodge(width = 0.4),
    aes(ymin = lower, ymax = upper)
  ) +
  geom_point(position = position_dodge(0.4)) +
  facet_wrap(~ Model, ncol = 2) +
  ylim(0.5, 0.72) +
  theme_minimal(base_size = 12) +
  labs(x = "Time (t)", y = "C-index", color = "Model",
       title = "") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```

#### Study 2: RMST, Expected mortality and Survival distribution as input

For the paper, digestible analysis C, C_tau and Ctd

```{r load, include=FALSE}
# saveRDS(
#    list(
#      results_risk1 = results_risk1,
#      results_risk2 = results_risk2,
#      results_risk3 = results_risk3,
#      results_expm1 = results_expm1,
#      results_expm2 = results_expm2,
#      results_expm3 = results_expm3,
#      results_surv = results_surv, 
#      results_risk1_point_estim = results_risk1_point_estim,
#      results_risk2_point_estim = results_risk2_point_estim,
#      results_risk3_point_estim = results_risk3_point_estim,
#      results_expm1_point_estim = results_expm1_point_estim,
#      results_expm2_point_estim = results_expm2_point_estim,
#      results_expm3_point_estim = results_expm3_point_estim,
#      results_surv_point_estim = results_surv_point_estim
#      
#    ),
#    "./Results/results_5fold_facet_plots.rds"
#  )

# results <- readRDS("./Results/results_5fold_facet_plots.rds")
# 
# results_risk1 = results$results_risk1
# results_risk2 = results$results_risk2
# results_risk3 = results$results_risk3
# results_expm1 = results$results_expm1
# results_expm2 = results$results_expm2
# results_expm3 = results$results_expm3
# results_surv  = results$results_surv
# 
# 
# results_risk1_point_estim = results$results_risk1_point_estim
# results_risk2_point_estim = results$results_risk2_point_estim
# results_risk3_point_estim = results$results_risk3_point_estim
# results_expm1_point_estim = results$results_expm1_point_estim
# results_expm2_point_estim = results$results_expm2_point_estim
# results_expm3_point_estim = results$results_expm3_point_estim
# results_surv_point_estim = results$results_surv_point_estim
# 

```


```{r select columns}

subset_expm <- list()
subset_surv <- list()
subset_risk <- list()

for (r in seq_along(resample_indices)) {
  subset_expm[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                 model_names = "all",
                 input_type = "ExpectedMortality",
                 bootstrap_patient_ids = resample_indices[[r]])
  subset_surv[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                model_names = "all",
                input_type = "Distribution",
                bootstrap_patient_ids = resample_indices[[r]])
  subset_risk[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions,
                      model_names = "all",
                      input_type = "RMST",
                      bootstrap_patient_ids = resample_indices[[r]])
  
}
```


```{r cindex expected mortality}
select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
results_expm1 <- list()
results_expm1_point_estim <- list()
for (exps in select_exps) {
   cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
  
   results_expm1[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = exps,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("Hmisc::rcorr.cens", 
                                                        "pysurvival",
                                                        "SurvMetrics::Cindex", 
                                                        "lifelines", 
                                                        "sksurv.censored"),
                                  eval.times = NULL, #C
                                  sampled_data = subset_expm)
   results_expm1_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                          censoring = stacked_predictions$test_status, 
                                                          time = stacked_predictions$test_time, 
                                                          implementation = 
                                                          list("Hmisc::rcorr.cens",
                                                               "pysurvival",
                                                               "SurvMetrics::Cindex", 
                                                               "lifelines", 
                                                               "sksurv.censored"), 
                                                          eval.times = NULL)
}

select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
results_expm2 <- list()
results_expm2_point_estim <- list()
for (exps in select_exps) {
   cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
   results_expm2[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = exps,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("survC1::Est.Cval",
                                                        "pec::cindex",
                                                        "survival.n", 
                                                        "survival.n/G2"),
                                  eval.times = "resample_max_uncensored_time", # C tau.
                                  sampled_data = subset_expm)
  results_expm2_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                      censoring = stacked_predictions$test_status, 
                                                      time = stacked_predictions$test_time, 
                                                      implementation = 
                                                      list("survC1::Est.Cval",
                                                        "pec::cindex",
                                                        "survival.n", 
                                                        "survival.n/G2"), 
                                                      eval.times =
                                                        "resample_max_uncensored_time")
}

select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
results_expm3 <- list()
results_expm3_point_estim <- list()
for (exps in select_exps) {
   cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
   results_expm3[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = exps,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("survC1::Est.Cval",
                                                        "pec::cindex",
                                                        "survival.n", 
                                                        "survival.n/G2"),
                                  eval.times = 120, #C
                                  sampled_data = subset_expm)
   results_expm3_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                    censoring = stacked_predictions$test_status, 
                                                    time = stacked_predictions$test_time, 
                                                    implementation = 
                                                    list("survC1::Est.Cval",
                                                      "pec::cindex",
                                                      "survival.n", 
                                                      "survival.n/G2"), 
                                                    eval.times = 120)
}
```
```{r cindex rmst}
select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
results_risk1 <- list() # boostrap results
results_risk1_point_estim <- list() # point estimate results
for (risk in select_risk) {
   cat("Calculating bootstrap for RMST for Model = ", risk, "\n")
   # Bootstrap
   results_risk1[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = risk,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("Hmisc::rcorr.cens",
                                                        "pysurvival",
                                                        "SurvMetrics::Cindex",
                                                        "lifelines",
                                                        "sksurv.censored"),
                                  eval.times = NULL, #C
                                  sampled_data = subset_risk)
   # Point estimate
   results_risk1_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                        censoring = stacked_predictions$test_status,
                                                        time = stacked_predictions$test_time,
                                                        implementation = list("Hmisc::rcorr.cens",
                                                        "pysurvival",
                                                        "SurvMetrics::Cindex",
                                                        "lifelines",
                                                        "sksurv.censored"),
                                                        eval.times = NULL)

}

select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
results_risk2 <- list() # boostrap results
results_risk2_point_estim <- list() # point estimate
for (risk in select_risk) {
   cat("Calculating bootstrap for  RMST for Model = ", risk, "\n")
  # Bootstrap
   results_risk2[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = risk,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("pec::cindex",
                                                        "survC1::Est.Cval",
                                                        "survival.n",
                                                        "survival.n/G2"),
                                  #eval.times = round(max(mb$time[mb$status == 1])),
                                  eval.times = "resample_max_uncensored_time",
                                  sampled_data = subset_risk)
    # Point estimate
   results_risk2_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                    censoring = stacked_predictions$test_status,
                                                    time = stacked_predictions$test_time,
                                                    implementation = list("pec::cindex",
                                                        "survC1::Est.Cval",
                                                        "survival.n",
                                                        "survival.n/G2"),
                                                    #eval.times =  round(max(mb$time[mb$status == 1])),
                                                    eval.times = "resample_max_uncensored_time")
}

select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
results_risk3 <- list()
results_risk3_point_estim <- list()
for (risk in select_risk) {
  cat("Calculating bootstrap for  RMST for Model = ", risk, "\n")
  # Bootstrap
  results_risk3[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = risk,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("pec::cindex",
                                                        "survC1::Est.Cval",
                                                        "survival.n",
                                                        "survival.n/G2"),
                                  eval.times = 120, # 10 years
                                  sampled_data = subset_risk)
   # Point estimate
   results_risk3_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                              censoring = stacked_predictions$test_status,
                                              time = stacked_predictions$test_time,
                                              implementation = list("pec::cindex",
                                                  "survC1::Est.Cval",
                                                  "survival.n",
                                                  "survival.n/G2"),
                                              eval.times =  120)
}
```

```{r cindex survival distribution}
model_names <- unique(sub("\\..*", "", grep("^[A-Za-z]+\\.\\d+$",
                                            colnames(subset_surv[[1]]), value = TRUE)))
results_surv <- list()
results_surv_point_estim <- list()
for (model in model_names) {
  # Antolinis
  cat("Calculating bootstrap for Distribution for Model = ", model, "\n")
  results_surv[[model]] <-  bootstrap.metric(metrics.wrapper,
                                  dataset=list(
                                     predicted = model,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("pycox.Ant",
                                                        "pycox.Adj.Ant"),
                                  sampled_data = subset_surv)
  surv_mat = stacked_predictions[, grep(paste0("^", model), names(stacked_predictions), 
                                          value = TRUE), drop = FALSE]
  colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
  results_surv_point_estim[[model]] <- metrics.wrapper(surv_matrix = surv_mat, 
                                                    censoring = stacked_predictions$test_status, 
                                                    time = stacked_predictions$test_time, 
                                                    implementation = list("pycox.Ant", 
                                                                          "pycox.Adj.Ant"))
}
```


```{r Extract info and Plot,  fig.width=12, fig.height=15}
# Extract the plots
expm_df_plot1 <- make_expm_plot_entries(results_expm1,  results_expm1_point_estim)
expm_df_plot2 <- make_expm_plot_entries(results_expm2,  results_expm2_point_estim)
expm_df_plot3 <- make_expm_plot_entries(results_expm3,  results_expm3_point_estim)
risk_df_plot1 <- make_rmst_plot_entries(results_risk1,  results_risk1_point_estim)
risk_df_plot2 <- make_rmst_plot_entries(results_risk2,  results_risk2_point_estim)
risk_df_plot3 <- make_rmst_plot_entries(results_risk3,  results_risk3_point_estim)
surv_df_plot  <- make_rmst_plot_entries(results_surv,   results_surv_point_estim)

expm_df_plot1$Notation <- "C"
expm_df_plot2$Notation <- "C_tau"
expm_df_plot3$Notation <- "C_tau_10"  
risk_df_plot1$Notation <- "C_risk"
risk_df_plot2$Notation <- "C_risk_tau"
risk_df_plot3$Notation <- "C_risk_tau_10"  
surv_df_plot$Notation  <- "C_td" 


df_plot <- bind_rows(expm_df_plot1, risk_df_plot1, 
                     expm_df_plot2, risk_df_plot2, 
                     expm_df_plot3, risk_df_plot3,
                     surv_df_plot)

df_plot$Metric <- sub("::.*", "", df_plot$Metric)

df_plot$Notation <- factor(df_plot$Notation,
  levels = c("C", "C_risk", "C_tau_10", "C_risk_tau_10", "C_tau", "C_risk_tau", "C_td"),
  labels = c(
    "tilde(C)~(Expected~Mortality)",
    "tilde(C)~(RMST)",
    "tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
    "tilde(C)[tau]~(RMST~tau==10~years)",
    "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
    "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))",
    "tilde(C)[td]~(Survival~Distribution)"
  )
)

df_plot$Model[df_plot$Model == "CoxPH"] <- "CPH"
#df_plot$Model <- factor(df_plot$Model, levels = c("DeepSurv", "CoxTime", "CoxPH", "RSF", "DeepHit"))
df_plot$Model <- factor(df_plot$Model, levels = c("RSF", "CoxTime", 
                                                   "DeepSurv", "CPH", "DeepHit"))

df_plot$Metric <- factor(df_plot$Metric, levels = unique(df_plot$Metric))
# Get numeric positions of each Metric for vertical lines
metric_levels <- levels(df_plot$Metric)
n_metrics <- length(metric_levels)

# Define where to put the vertical lines between metrics
separator_positions <- seq(1.5, n_metrics - 0.5, by = 1)

### Version with color
 ggplot(df_plot, aes(x = Metric, y = cindex, color = Model)) +
  geom_pointrange(aes(ymin = lower, ymax = upper),
                  position = position_dodge(width = 0.4), size = 0.5) +
  facet_wrap(~ Notation, nrow = 4, ncol = 2, scales = "free_x", labeller = label_parsed)+
  geom_vline(xintercept = separator_positions, 
              linetype = "dashed", color = "grey70") +
  ylim(0.5, 0.75) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
  labs(title = "",
       y = "C-index", x = NULL) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom",
  panel.grid.major.x = element_blank(),
  panel.grid.minor.x = element_blank())
 

```

```{r Look at cindex distributions }
hist(as.numeric(results_expm2[["ExpMort.RSF"]][["batch.metrics"]][, "pec::cindex"]))
hist(as.numeric(results_expm2[["ExpMort.CoxPH"]][["batch.metrics"]][, "pec::cindex"]))
hist(as.numeric(results_expm2[["ExpMort.DeepHit"]][["batch.metrics"]][, "pec::cindex"]))


hist(as.numeric(results_surv[["RSF"]][["batch.metrics"]][, "pycox.Ant"]))
hist(as.numeric(results_surv[["CoxPH"]][["batch.metrics"]][, "pycox.Ant"]))
hist(as.numeric(results_surv[["DeepHit"]][["batch.metrics"]][, "pycox.Ant"]))
```
