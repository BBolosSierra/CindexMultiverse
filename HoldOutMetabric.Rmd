---
title: "HoldOutMetabric"
output: html_document
date: "2025-04-20"
---

# Concordance index multiverse - METABRIC analysis (hold-out)

## Setup the environment

Load libraries

```{r, include=FALSE}
# Survival metrics
library(reticulate)
library(arrow)
library(caret)
library(riskRegression)
library(prodlim)
library(pec)
library(survival)
library(rhdf5)
library(randomForestSRC)
library(survAUC)
library(Hmisc)
library(dplyr)
# Plotting
library(gridExtra)
# Parallelization
library(doFuture)
library(future)
library(progressr)
library(foreach)
library(MASS)
library(flexsurv)
library(furrr)
# install.packages("./pysurvivalR.tar.gz", repos = NULL, type = "source")
library(pysurvivalR)
library(survivalmodels)
library(dplyr)
library(tidyr)
library(patchwork)
```


```{r setup, include=FALSE}
#Sys.unsetenv("RETICULATE_PYTHON")
Sys.setenv(OMP_NUM_THREADS = "1")       # Limits OpenMP to 1 thread
Sys.setenv(NUMBA_NUM_THREADS = "1")     # Limits Numba to 1 thread
Sys.setenv(MKL_NUM_THREADS = "1")       # Limits Intel MKL to 1 thread
Sys.setenv(KMP_WARNINGS = "0")          # Disables OpenMP warnings
Sys.setenv(OPENBLAS_NUM_THREADS = "1")  # Limits OpenBLAS to 1 thread

# #Sys.setenv(NUMBA_DISABLE_JIT = "0")  
# library(reticulate)

#use_condaenv("/opt/homebrew/Caskroom/miniforge/base/envs/py-rstudio", required=TRUE)
#use_virtualenv("~/.virtualenvs/venv-DeSurv_python_R")
#use_python("/opt/homebrew/Caskroom/miniforge/base/envs/venv-DeSurv3/bin/python3.9")
#py_config()
```

We also load a set of helper functions used throuhgout our analysis:

```{r}
source("./CindexHelperFunctions.R")
```

## Load the data

Here, we load the pre-processed METABRIC data that was generated using the 
`preprocessing_biocportal_metabric.R` script.

```{r load data}
# read the processed file
mb <- read.table("./Datasets/metabric_preprocess_multiverse.csv", 
                 sep = "," , header = TRUE)

# Keep the id as index
rownames(mb) <- mb$PATIENT_ID

# Remove the id column
mb <- mb[,2:12]

# Mapping
var_map <- c(
  "MKI67" = "X1",
  "EGFR" = "X2",
  "ERBB2" = "X3",
  "PGR" = "X4",
  "HORMONE_THERAPY" = "X5",
  "RADIO_THERAPY" = "X6",
  "CHEMOTHERAPY" = "X7",
  "ER_IHC" = "X8",
  "AGE_AT_DIAGNOSIS" = "X9",
  "OS_MONTHS" = "time",
  "OS_STATUS" = "status"
)

# Mapping colnames
names(mb)[names(mb) %in% names(var_map)] <- var_map[names(mb)[names(mb) %in% names(var_map)]]


summary(mb$time)


#mb <- mb[order(mb$time, -mb$status),]
```


The data contains information about `r nrow(mb)` individuals, with 
`r sum(mb$status == 1)` events and `r sum(mb$status == 0)` censored 
observations. The survival times are measured in months.

## Evaluation of predictive performance using a data holdout approach

Here, we will evaluate the predictive performance of various survival models 
using a holdout approach. We will split the data into a training set (80%) and a 
test set (20%), fit the models on the training set, and then evaluate their 
performance on the test set. We repeated this 5 times to ensure robustness of 
the results. The data splits used here match the ones used in the 5-fold 
cross-validation results. For each data split, bootstrap samples of the test set
will be generated to estimate the uncertainty of the estimated C-index values. 

### Model hyperparameters

For the hyperparameters of deep learning models that have been trained with 
`survivalmodels` package is relevant to look at the pytorch optimizer: https://raphaels1.github.io/survivalmodels/reference/get_pycox_optim.html 

Also the parameters for each model:

1) Deephit: https://raphaels1.github.io/survivalmodels/reference/deephit.html
2) DeepSurv: https://raphaels1.github.io/survivalmodels/reference/deepsurv.html
3) CoxTime: https://raphaels1.github.io/survivalmodels/reference/coxtime.html

In our analysis, we consider the same hyperparameter values used when analysing
the METABRIC in the original publications for DeepHit, DeepSurv, and CoxTime. 
For RSF, we use the default hyperparameters from the `randomForestSRC` package.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Hyper-parameter}     & \textbf{DeepSurv} & \textbf{Cox-Time} & \textbf{DeepHit}  &  \textbf{RSF} \\ \midrule
Optimizer                    & adam              & adam             & adam              & - \\
Activation                   & selu              & relu             & relu              & -\\
\# Dense Layers              & 1                 & 2                & 2                 & - \\
\# Nodes / Layer             & 41                & 32, 32           & 32, 32            & 100 trees\\
Learning Rate                & 0.0103            & 0.01             & 0.001             & - \\
Dropout                      & 0.1601            & 0.1              & 0.6               & - \\
LR Decay                     & 0.00417           & 0                & 0                 & - \\
Batch Norm                   & True             & True             & True              & - \\
Batch Size                   & 256               & 256              & 50                & - \\
Epochs                       & 500               & 512              & 100               & - \\
Early Stopping               & False             & True             & True              & - \\
mod\_alpha                   & -                & -               & 0.2               & - \\
sigma                        & -                & -               & 0.1               & - \\
cuts                         & -                & -               & 300               & - \\ \bottomrule
\end{tabular}
\caption{Hyperparameters across DeepSurv, Cox-Time, DeepHit and RSF. For DeepSurv, Cox-Time and DeepHit, hyperparameters were specified as in the analysis of the METABRIC data presented in the corresponding original publications.}
\label{tab:hyperparams}
\end{table}


### Model training

As the training of the models can take a long time, we save the results to an 
Rds file which we load here. We can use the results from 5-fold cross-validation and analyse each fold separately. 


```{r load results}
folds_stacked_predictions <-  readRDS("./Results/5FoldStackedPredictions.rds")

survival_curves <- readRDS("./Results/5FoldStackedPredictionsCurves.rds")
```


Run the following if were to train on a random split. 


```{r model predictions, eval=FALSE}
## Reproducibility
reset_torch_seed <- function(seed = 123) {
  reticulate::py_run_string(sprintf("
import torch
import numpy as np
import random
torch.manual_seed(%d)
np.random.seed(%d)
random.seed(%d)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
", seed, seed, seed))
}

set.seed(123)
survivalmodels::set_seed(seed_R=123, seed_np = 123, seed_torch = 123) 

### Crossval
# Number of folds
K <- 5 
n <- nrow(mb)  

# Interesting times for comparison
#ts <- sort(unique(round(mb$time)))
all_predictions <- list()

# List of numeric column for standarization
numeric_cols <- c("X1", "X2", "X3", "X4", "X9")

# Create empty list for survival curves
survival_curves <- vector("list", K)

# Shuffle indices and create folds
#indices <- sample(seq_len(n))
#folds <- cut(indices, breaks = K, labels = FALSE)
folds <- createFolds(mb$status, k = K, list = TRUE) # return test sets

# Interesting intervals for prediction
mb_t_max <- round(max(mb$time))
ts_scaled <- seq(0, mb_t_max, 1)

for (k in 1:K) {

  # Define test and training indices
  #mb_test_idx  <- which(folds == k)
  #mb_train_idx <- setdiff(seq_len(n), mb_test_idx)
  
  # Define test and training indices
  mb_test_idx  <- folds[[k]]
  mb_train_idx <- setdiff(seq_len(nrow(mb)), mb_test_idx)
  
  # Subset the dataset
  mb_train <- mb[mb_train_idx, ]
  mb_test  <- mb[mb_test_idx, ]
  
  t_train_max <- max(mb_train$time)
  
  # Scale continuous 
  means <- sapply(mb_train[, numeric_cols], mean)
  sds   <- sapply(mb_train[, numeric_cols], sd)

  mb_train[, numeric_cols] <- scale(mb_train[, numeric_cols],
                                    center = means, scale = sds)
  mb_test[, numeric_cols]  <- scale(mb_test[, numeric_cols],
                                    center = means, scale = sds)

  mb_train$time <- mb_train$time
  mb_test$time <- mb_test$time
  
  # Extract only the covariates used for predictions
  test_covariates <- mb_test[, c("X1", "X2", "X3", "X4",
                                 "X5", "X6", "X7", "X8", "X9")]

  # Fit models
  #cat("Dataset:", i, ".", j, '\n')
  cat("Fold:", k, '\n')
  cat("tmax:", t_train_max, "\n")
  cat('Train set dimensions:', dim(mb_train), '\n')
  cat('Train set event:', mean(mb_train$status == 1)*100, '\n')
  cat('Train set censoring:', mean(mb_train$status == 0)*100, '\n')
  cat('Test set dimensions:', dim(mb_test), '\n')
  cat('Test set event:', mean(mb_test$status == 1)*100, '\n')
  cat('Test set censoring:', mean(mb_test$status == 0)*100, '\n')
  cat('\n')
  
  # 1) Random Forest:
  # Fit the model
  rf <- randomForestSRC::rfsrc(Surv(time, status) ~ X1 + X2 +
                                 X3 + X4 + X5 + X6 + 
                                 X7 + X8 + X9, 
                               data = mb_train, 
                               ntree = 100, 
                               ntime = 1400)
  # Predict survival object
  surv_rf_object <- predict(rf, newdata = mb_test, type = "surv")
  surv_rf <- surv_rf_object$survival
  time_grid <- surv_rf_object$time.interest
  # Interpolate to have same times in all survival curves
  surv_rf_int <- apply(surv_rf, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_rf_int <- t(surv_rf_int)
  rownames(surv_rf_int) <- rownames(mb_test)
  colnames(surv_rf_int) <- ts_scaled
  # Calculate expected mortality
  second_smallest_survival_prob = unique(sort(as.vector(surv_rf_int), partial = 2))[2]/10
  rf_exp_mort <- rowSums(-log(pmax(surv_rf_int, second_smallest_survival_prob)))
  rf_risk_score <- -rowSums(surv_rf_int)
  
  # 2) Predict risk Cox PH
  # Fit with cox ph 
  cox_ph <- survival::coxph(Surv(time, status) ~ X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                               data = mb_train)
  
  # Predict survival object
  sf <- survfit(cox_ph, newdata = mb_test)
  surv_cox <- t(sf$surv)
  time_grid <- sf$time 
  #Interpolate
  surv_cox_int <- apply(surv_cox, 1, function(s){
     approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  
  surv_cox_int <- t(surv_cox_int)
  rownames(surv_cox_int) <- rownames(mb_test)
  colnames(surv_cox_int)  <- ts_scaled
  # Calculate expected mortality
  second_smallest_survival_prob = unique(sort(as.vector(surv_cox_int), partial = 2))[2]/10
  cox_exp_mort <- rowSums(-log(pmax(surv_cox_int, second_smallest_survival_prob)))
  cox_risk_score <- -rowSums(surv_cox_int)

  reset_torch_seed()
  
  # 3) DeepSurv
  # Hyperparameters: https://github.com/jaredleekatzman/DeepSurv/blob/41eed003e5b892c81e7855e400861fa7a2d9da4f/experiments/deepsurv/models/metabric_IHC4_clinical_adam_0.json
  deepsurv_model <- survivalmodels::deepsurv(Surv(time, status) ~ 
                                               X1 + X2 + 
                                               X3 + X4 + X5 + X6 + 
                                               X7 + X8 + X9, 
                                             data = mb_train, 
                                             frac = 0.2, 
                                             dropout = 0.160087890625,
                                             optimizer = "adam",
                                             activation = "selu", 
                                             batch_norm = TRUE,
                                             num_nodes = c(41), 
                                             batch_size = 256L, 
                                             learning_rate = 0.010289691253027908, 
                                             lr_decay = 0.0041685546875, 
                                             momentum =  0.8439658203125,
                                             #weight_decay = 10.890986328125, # this is too high it cannot be 
                                             epochs = 500) 
  # Predict survival object
  surv_deepsurv <- predict(deepsurv_model,
                           newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deepsurv))
  # Interpolate
  surv_deepsurv_int <- apply(surv_deepsurv, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deepsurv_int <- t(surv_deepsurv_int)
  rownames(surv_deepsurv_int) <- rownames(mb_test)
  colnames(surv_deepsurv_int) <- ts_scaled
  # Calculate expected mortality
  second_smallest_survival_prob = unique(sort(as.vector(surv_deepsurv_int), partial = 2))[2]/10
  deepsurv_exp_mort <- rowSums(-log(pmax(surv_deepsurv_int, second_smallest_survival_prob)))
  deepsurv_risk_score <- -rowSums(surv_deepsurv_int)

  reset_torch_seed()
  # 4) Coxtime
  # Hyperparameters from:
  # https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/cox-time.ipynb
  coxtime_model <- survivalmodels::coxtime(Surv(time, status) ~ 
                                             X1 + X2 + X3 + 
                                             X4 + X5 + X6 + 
                                              X7 + X8 + X9, 
                                           data = mb_train, 
                                           optimizer = "adam",
                                           learning_rate = 0.01,       
                                           betas = c(0.9, 0.999), # defaults
                                           activation = "relu",
                                           num_nodes = c(32L, 32L),
                                           batch_norm = TRUE,
                                           dropout = 0.1,
                                           batch_size = 256,
                                           epochs = 512,
                                           early_stopping = TRUE, 
                                           frac = 0.2)
  
  # Predict survival object
  surv_coxtime <- predict(coxtime_model,
                          newdata = mb_test, type = "surv")
  time_grid <-  as.numeric(colnames(surv_coxtime))
  # Interpolate
  surv_coxtime_int <- apply(surv_coxtime, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_coxtime_int <- t(surv_coxtime_int)
  rownames(surv_coxtime_int) <- rownames(mb_test)
  colnames(surv_coxtime_int) <- ts_scaled
  # Calculate expected mortality
  second_smallest_survival_prob = unique(sort(as.vector(surv_coxtime_int), partial = 2))[2]/10
  coxtime_exp_mort <- rowSums(-log(pmax(surv_coxtime_int, second_smallest_survival_prob)))
  coxtime_risk_score <- -rowSums(surv_coxtime_int)
  
  reset_torch_seed()
  
  # 5) Deephit
  # Hyperparameters: https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/deephit.ipynb and DeepHit paper, and also double checked with DeSurv paper (some parameters are slightly different)
  deephit_model <- survivalmodels::deephit(Surv(time, status) ~ 
                             X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                             data = mb_train, 
                             optimizer = "adam",
                             activation = "relu", # in DeSurv, ipynb
                             num_nodes = c(32L, 32L), # in ipynb and DeSurv
                             batch_norm = TRUE, # in ipynb
                             dropout = 0.6, #  0.1 in ipynb and 0.6 in DeepHit paper
                             #cuts = 1400, # in DeSurv and DeepHit, 10 and interpolation in ipynb
                             mod_alpha = 0.2, # in DeSurv, ipynb
                             early_stopping = TRUE, # in DeepHit and ipynb
                             #tolerance = 3,
                             cuts = 1400,
                             sigma = 0.1, # in DeSurv and ipynb
                             batch_size = 50, # 50 in Deephit, 256 in ipynb
                             epochs = 100, # in ipynb
                             #verbose = TRUE,
                             learning_rate = 0.001, # 0.001 in Deephit, 0.01 in ipynb
                             #betas = c(0.9, 0.999), # default adam
                             frac = 0.2) # validation 

  # Survival object
  surv_deephit <- predict(deephit_model,
                          newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deephit))# * t_train_max
  # Interpolate
  surv_deephit_int <- apply(surv_deephit, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deephit_int <- t(surv_deephit_int)
  rownames(surv_deephit_int) <- rownames(mb_test)
  colnames(surv_deephit_int) <- ts_scaled
  # Calculate expected mortality
  second_smallest_survival_prob = unique(sort(as.vector(surv_deephit_int), partial = 2))[2]/10
  deephit_exp_mort <- rowSums(-log(pmax(surv_deephit_int, second_smallest_survival_prob)))
  deephit_risk_score <- -rowSums(surv_coxtime_int)

  # Gather the survival survs too
  survival_curves[[k]] <- list(patients_ids = rownames(mb_test),
                               test_time = mb_test$time,
                               test_status = mb_test$status,
                               RSF = as.data.frame(surv_rf_int),
                               CoxPH = as.data.frame(surv_cox_int),
                               DeepHit = as.data.frame(surv_deephit_int),
                               DeepSurv = as.data.frame(surv_deepsurv_int),
                               Coxtime = as.data.frame(surv_coxtime_int),
                               covariates = test_covariates)
  
  fold_results <- data.frame(cv_fold = k,
                             patients_ids = rownames(mb_test), 
                             test_time = mb_test$time,
                             test_status = mb_test$status,
                             ExpMort.RSF = rf_exp_mort,
                             ExpMort.CoxPH = cox_exp_mort,
                             ExpMort.DeepHit = deephit_exp_mort,
                             ExpMort.DeepSurv = deepsurv_exp_mort,
                             ExpMort.CoxTime = coxtime_exp_mort,
                             Risk.RSF = rf_risk_score,
                             Risk.CoxPH = cox_risk_score,
                             Risk.DeepHit = deephit_risk_score,
                             Risk.DeepSurv = deepsurv_risk_score,
                             Risk.CoxTime = coxtime_risk_score,
                             RSF = as.data.frame(surv_rf_int),
                             CoxPH = as.data.frame(surv_cox_int),
                             DeepHit = as.data.frame(surv_deephit_int), 
                             DeepSurv = as.data.frame(surv_deepsurv_int),
                             CoxTime = as.data.frame(surv_coxtime_int), 
                             covariates = test_covariates)
    

  # Append fold results to the list
 all_predictions[[paste("Fold", k)]] <- fold_results

}

folds_stacked_predictions <- do.call(rbind, all_predictions)

#saveRDS(folds_stacked_predictions, "./Results/5FoldStackedPredictions2.rds")
#saveRDS(survival_curves, "./Results/5FoldStackedPredictionsCurves2.rds")
```

```{r Plot expected mortality, fig.height=10, fig.width=8}

folds_long <- folds_stacked_predictions %>%
  pivot_longer(cols = starts_with("ExpMort"), names_to = "Model", values_to = "ExpectedMortality") %>%
  mutate(Model = gsub("ExpMort\\.", "", Model)) %>%
  mutate(
    facet_group = case_when(
      grepl("_arb$", Model) ~ "2_arb",
      grepl("120", Model)   ~ "3_120",
      TRUE                  ~ "1_named"
    ),
    facet_order = paste0(facet_group, "_", Model)
  )
folds_long <- folds_long %>%
  mutate(Model = factor(Model, levels = folds_long %>%
                          distinct(facet_order, Model) %>%
                          arrange(facet_order) %>%
                          pull(Model)))
# In color
ggplot(folds_long, aes(x = as.factor(cv_fold), y = ExpectedMortality, fill = as.factor(cv_fold))) +
  geom_violin(trim = FALSE) +
  facet_wrap(~ Model, nrow = 3) +
  labs(
    title = "",
    x = "Cross-validation Fold",
    y = "Expected Mortality"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Pastel1")
# In black
ggplot(folds_long, aes(x = as.factor(cv_fold), y = ExpectedMortality, fill = as.factor(cv_fold))) +
   geom_violin(trim = FALSE, fill = "grey60", color = "black") +
  facet_wrap(~ Model, nrow = 3) +
  labs(
    title = "",
    x = "Cross-validation Fold",
    y = "Expected Mortality"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none") 

```

### Censoring probabilities and IPCW

To facilitate the interpretation of the differences across C-index estimates,
we calculate the censoring probabilities for the training and test sets.

```{r IPCW per fold, fig.height=10, fig.width=8}
# Number of folds
folds <- unique(folds_stacked_predictions$cv_fold)
n_folds <- length(folds)

par(mfrow = c(3, 2), mar = c(4, 4, 3, 1))  # adjust margins

# Loop through folds
for (fold_n in folds) {
  cat("Processing Fold:", fold_n, "\n")

  # Test set
  test <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ]
  test$censor.status <- ifelse(test$test_status == 0, 1, 0)  # 1 = censored
  test <- test[order(test$test_time), ]

  weight.j.test <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = test,
    method = "marginal",
    times = unique(test$test_time),
    subjectTimes = test$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtest <- 1 / weight.j.test^2
  time_points_test <- unique(test$test_time)

  fit_test <- prodlim(Hist(test_time, test_status) ~ 1, data = test, reverse = TRUE)

  # Train set
  train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ]
  train$censor.status <- ifelse(train$test_status == 0, 1, 0)
  train <- train[order(train$test_time), ]

  weight.j.train <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = train,
    method = "marginal",
    times = unique(train$test_time),
    subjectTimes = train$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtrain <- 1 / weight.j.train^2
  time_points_train <- unique(train$test_time)

  fit_train <- prodlim(Hist(test_time, test_status) ~ 1, data = train, reverse = TRUE)

  # Plot both IPCW curves in black
  # plot(time_points_test, IPCWtest,
  #      type = "l", col = "black", lwd = 2, lty = 2,
  #      xlab = "Time (months)", ylab = "1 / G²(t)",
  #      main = paste0("IPCW - Fold ", fold_n))
  # lines(time_points_train, IPCWtrain, col = "black", lwd = 2)
  # 
  # legend("topleft", legend = c("Test", "Train"),
  #        col = c("black", "black"),
  #        lty = c(2, 1), lwd = 1, bty = "n")
  
  # Plot both IPCW in color
  plot(time_points_test, IPCWtest,
       type = "l", col = "darkorange", lty = c(1, 10), lwd = 2,
       xlab = "Time (months)", ylab = "1 / G²(t)",
       main = paste0("IPCW - Fold ", fold_n))
  lines(time_points_train, IPCWtrain, col = "steelblue", lwd = 2)

  legend("topleft", legend = c("Test", "Train"),
         col = c("darkorange", "steelblue"),
         lty = c(1, 1), lwd = 2, bty = "n")
  
}

```
```{r}
plot_list <- list()

for (i in seq_along(folds)) {
  fold_n <- folds[i]
  cat("Processing Fold:", fold_n, "\n")
  
  # Test set
  test <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ]
  test$censor.status <- ifelse(test$test_status == 0, 1, 0)
  test <- test[order(test$test_time), ]

  weight.j.test <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = test,
    method = "marginal",
    times = unique(test$test_time),
    subjectTimes = test$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtest <- 1 / weight.j.test^2
  time_points_test <- unique(test$test_time)

  # Train set
  train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ]
  train$censor.status <- ifelse(train$test_status == 0, 1, 0)
  train <- train[order(train$test_time), ]

  weight.j.train <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = train,
    method = "marginal",
    times = unique(train$test_time),
    subjectTimes = train$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtrain <- 1 / weight.j.train^2
  time_points_train <- unique(train$test_time)

  df_test <- data.frame(time = time_points_test, IPCW = IPCWtest, group = "Test")
  df_train <- data.frame(time = time_points_train, IPCW = IPCWtrain, group = "Train")
  df_all <- rbind(df_test, df_train)

  p <- ggplot(df_all, aes(x = time, y = IPCW, color = group, linetype = group)) +
    geom_line(size = 1) +
    scale_color_manual(values = c("Test" = "darkorange", "Train" = "steelblue")) +
    scale_linetype_manual(values = c("Test" = "solid", "Train" = "solid")) +
    labs(title = paste0("Fold ", fold_n),
         x = "Time (months)", y = expression("1 / G(t)^2")) +
    theme_minimal() +
    theme(legend.title = element_blank())

  plot_list[[paste0("p", i)]] <- p
}

# Combine with patchwork
# Assume 5 folds
(plot_list$p1 | plot_list$p2) / (plot_list$p3 | plot_list$p4) / (plot_list$p5 | plot_spacer())


```

```{r censoring-probabilities, fig.height=10, fig.width=8}

plot_censoring_with_risk_table <- function(fold_n, data) {
  test <- data[data$cv_fold == fold_n, ]
  train <- data[data$cv_fold != fold_n, ]
  
  fit_test <- prodlim(Hist(test_time, test_status) ~ 1, data = test, reverse = TRUE)
  fit_train <- prodlim(Hist(test_time, test_status) ~ 1, data = train, reverse = TRUE)
  
  # Extract survival curves
  surv_train <- summary(fit_train, times = fit_train$time)
  surv_test <- summary(fit_test, times = fit_test$time)
  
  df_train <- data.frame(time = fit_train$time,
                         surv = 1 - fit_train$surv,
                         group = "Train")
  df_test <- data.frame(time = fit_test$time,
                        surv = 1 - fit_test$surv,
                        group = "Test")
  
  df <- rbind(df_train, df_test)
  
  p <- ggplot(df, aes(x = time, y = surv, color = group, linetype = group)) +
    geom_step(size = 1) +
    ylim(0, 1) +
    labs(title = paste0("Fold ", fold_n),
         x = "Time (months)",
         y = "Censoring Probability P(C < t)") +
    scale_color_manual(values = c("Train" = "steelblue", "Test" = "darkorange")) +
    scale_linetype_manual(values = c("Train" = "solid", "Test" = "solid")) +
    theme_minimal() +
    theme(legend.title = element_blank())
  return(p)
}

# Example usage
p1 <- plot_censoring_with_risk_table(fold_n = 1, 
                                     data = folds_stacked_predictions)
p2 <- plot_censoring_with_risk_table(fold_n = 2, 
                                     data = folds_stacked_predictions)
p3 <- plot_censoring_with_risk_table(fold_n = 3,
                                     data = folds_stacked_predictions)
p4 <- plot_censoring_with_risk_table(fold_n = 4, 
                                     data = folds_stacked_predictions)
p5<- plot_censoring_with_risk_table(fold_n = 5, 
                                    data = folds_stacked_predictions)

(p1 | p2) / (p3 | p4) / (p5 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")
```

```{r, fig.width=10}
(p1 | plot_list$p1) + plot_layout(guides = "collect") & theme(legend.position = "bottom",
        plot.title = element_blank())
```

### Predicted survival curves

Here, we plot the predicted survival curves

```{r survival curves, fig.height=11, fig.width=10}

#survival_curves <- readRDS("./Results/5FoldStackedPredictionsCurves.rds")
p1 <- plot_survival_curves(survival_curves[[1]]$DeepHit, title = "DeepHit", seed=123)
p2 <- plot_survival_curves(survival_curves[[1]]$DeepSurv, title = "DeepSurv", seed=123)
p3 <- plot_survival_curves(survival_curves[[1]]$CoxPH, title = "CoxPH", seed=123)
p4 <- plot_survival_curves(survival_curves[[1]]$Coxtime, title = "CoxTime", seed=123)
p5 <- plot_survival_curves(survival_curves[[1]]$RSF, title = "RSF", seed=123)

(p1 | p2) / (p3 | p4) / (p5 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

```
```{r}
ggplot(folds_stacked_predictions, aes(x = Risk.DeepSurv, y = ExpMort.DeepSurv)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(
    title = "Relationship between Risk.DeepSurv and ExpMort.DeepSurv",
    x = "AUSC (-sum (S(t) * dt))",
    y = "Expected Mortality (RSF) "
  ) +
  theme_minimal()

```

### Comparison of C-index estimates. 

Here, we apply the difference C-index implementations to quantify predictive 
performance in each test set. To quantify uncertainty for the C-index estimates, 
we use bootstraping. For each test set, 100 bootstrap samples (same sample size 
as the test set) were generated and the same C-index implementations were 
applied for each bootstrap sample. Results are summarised using ... 


```{r  Maximum times}
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ]
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])

  cat("Test set maximum time in fold ", fold_n, " is ", max(stacked_predictions$test_time) ," and maximum uncensored time is ",max_uncensored_time, "\n")
}
```

The max of uncensored time would vary per fold, to ensure that we do not set tau 

```{r load holdout}
hold_results <- readRDS("./Results/holdout_CindexResults.rds")
```

```{r c-index bootstrap, eval=FALSE}

hold_results <- vector("list", 5) 
set.seed(123)

for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  # Separate by fold test (stacked_predictions) and train 
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ] 
  mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ] 
  # Extract the maximum uncensored time per fold
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])

  # Set the number of boostraps
  n_bootstraps <- 100
  # Length of test
  dataset_size <- nrow(stacked_predictions) 
  # resample by patient id
  resample_indices <- replicate(n_bootstraps, 
                                sample(stacked_predictions$patients_ids, 
                                       size = dataset_size, replace = TRUE), 
                                simplify = FALSE)
 
  #print(head(resample_indices[[1]]))
  # Create empty lists to store results
  subset_expm <- list()
  subset_surv <- list()
  #subset_risk <- list()
  # Create resample sets
  for (r in seq_along(resample_indices)) {
    subset_expm[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                   model_names = "all",
                   input_type = "ExpectedMortality",
                   bootstrap_patient_ids = resample_indices[[r]])
    subset_surv[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                  model_names = "all",
                  input_type = "Distribution",
                  bootstrap_patient_ids = resample_indices[[r]])
    subset_risk[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions,
                        model_names = "all",
                        input_type = "RiskScore",
                        bootstrap_patient_ids = resample_indices[[r]])

  }

  # Expected mortality and estimating C
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm1 <- list() # boostrap results
  results_expm1_point_estim <- list() # point estimate results
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
     # Bootstrap
     results_expm1[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                    eval.times = NULL, #C
                                    sampled_data = subset_expm)
     # Point estimate
     results_expm1_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                          censoring = stacked_predictions$test_status,
                                                          time = stacked_predictions$test_time,
                                                          implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                                          eval.times = NULL)

  }

  # Expected mortality and estimating C tau with max uncensored time
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm2 <- list() # boostrap results
  results_expm2_point_estim <- list() # point estimate
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
    # Bootstrap
     results_expm2[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    #eval.times = round(max(mb$time[mb$status == 1])),
                                    eval.times = max_uncensored_time,
                                    sampled_data = subset_expm,
                                    additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
      # Point estimate
     results_expm2_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                                      #eval.times =  round(max(mb$time[mb$status == 1])),
                                                      eval.times =  max_uncensored_time,
                                                      sksurv_train_time = mb_train$test_time,
                                                      sksurv_train_status = mb_train$test_status,
                                                      sksurv_tied_tol = 1e-8)
  }
    # Expected mortality and estimating C tau at 10 years
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm3 <- list()
  results_expm3_point_estim <- list()
  for (exps in select_exps) {
    cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
    # Bootstrap
    results_expm3[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    eval.times = 120, # 10 years
                                    sampled_data = subset_expm,
                                  additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
     # Point estimate
     results_expm3_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                censoring = stacked_predictions$test_status,
                                                time = stacked_predictions$test_time,
                                                implementation = list("pec::cindex",
                                                    "survC1::Est.Cval",
                                                    "survival.n",
                                                    "survival.n/G2",
                                                    "sksurv.ipcw"),
                                                eval.times =  120, # 10 years
                                                sksurv_train_time = mb_train$test_time,
                                                sksurv_train_status = mb_train$test_status,
                                                sksurv_tied_tol = 1e-8)
  }
    # Expected mortality and estimating C
  select_risk <- grep("Risk\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk1 <- list() # boostrap results
  results_risk1_point_estim <- list() # point estimate results
  for (risk in select_risk) {
     cat("Calculating bootstrap for ExpMort for Model = ", risk, "\n")
     # Bootstrap
     results_risk1[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                    eval.times = NULL, #C
                                    sampled_data = subset_risk)
     # Point estimate
     results_risk1_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                          censoring = stacked_predictions$test_status,
                                                          time = stacked_predictions$test_time,
                                                          implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                                          eval.times = NULL)

  }

  select_risk <- grep("Risk\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk2 <- list() # boostrap results
  results_risk2_point_estim <- list() # point estimate
  for (risk in select_risk) {
     cat("Calculating bootstrap for ExpMort for Model = ", risk, "\n")
    # Bootstrap
     results_risk2[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    #eval.times = round(max(mb$time[mb$status == 1])),
                                    eval.times = max_uncensored_time,
                                    sampled_data = subset_risk,
                                    additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
      # Point estimate
     results_risk2_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                                      #eval.times =  round(max(mb$time[mb$status == 1])),
                                                      eval.times =  max_uncensored_time,
                                                      sksurv_train_time = mb_train$test_time,
                                                      sksurv_train_status = mb_train$test_status,
                                                      sksurv_tied_tol = 1e-8)
  }

  select_risk <- grep("Risk\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk3 <- list()
  results_risk3_point_estim <- list()
  for (risk in select_risk) {
    cat("Calculating bootstrap for ExpMort for Model = ", risk, "\n")
    # Bootstrap
    results_risk3[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    eval.times = 120, # 10 years
                                    sampled_data = subset_risk,
                                  additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
     # Point estimate
     results_risk3_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                censoring = stacked_predictions$test_status,
                                                time = stacked_predictions$test_time,
                                                implementation = list("pec::cindex",
                                                    "survC1::Est.Cval",
                                                    "survival.n",
                                                    "survival.n/G2",
                                                    "sksurv.ipcw"),
                                                eval.times =  120, # 10 years
                                                sksurv_train_time = mb_train$test_time,
                                                sksurv_train_status = mb_train$test_status,
                                                sksurv_tied_tol = 1e-8)
  }
  model_names <- unique(sub("\\..*", "", grep("^[A-Za-z]+\\.\\d+$",
                                              colnames(subset_surv[[1]]), value = TRUE)))
  results_surv <- list() # Bootstrap result
  results_surv_point_estim <- list() # Point estimate
  # Based on survival probabilities
  for (model in model_names) {
    cat("Calculating bootstrap for Distribution for Model = ", model, "\n")
    # Bootstrap
    results_surv[[model]] <-  bootstrap.metric(metrics.wrapper,
                                    dataset=list(
                                       predicted = model,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pycox.Ant", "pycox.Adj.Ant"),
                                    sampled_data = subset_surv)
    # Generate matrix
    surv_mat = stacked_predictions[, grep(paste0("^", model), names(stacked_predictions),
                                          value = TRUE), drop = FALSE]
    colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
    # Point estimate
    results_surv_point_estim[[model]] <- metrics.wrapper(surv_matrix = surv_mat,
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pycox.Ant",
                                                                            "pycox.Adj.Ant"))
  }

  # Store it:
  hold_results[[fold_n]] <- list( results_expm1 = results_expm1, #C
                                  results_expm1_point_estim = results_expm1_point_estim,
                                  results_expm2 = results_expm2, #C tau at max uncensored time
                                  results_expm2_point_estim = results_expm2_point_estim,
                                  results_expm3 = results_expm3, #C tau at 10 years
                                  results_expm3_point_estim = results_expm3_point_estim,
                                  results_surv  = results_surv,
                                  results_surv_point_estim = results_surv_point_estim,
                                  results_risk1 = results_risk1,
                                  results_risk1_point_estim = results_risk1_point_estim,
                                  results_risk2 = results_risk2,
                                  results_risk2_point_estim = results_risk2_point_estim,
                                  results_risk3 = results_risk3,
                                  results_risk3_point_estim = results_risk3_point_estim
  )

}

#saveRDS(hold_results, "./Results/holdout_CindexResults2.rds")
```


```{r Plot results,  fig.width=13, fig.height=20}

plot_list <- list()
# Extract the plots
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  results_expm1 = hold_results[[fold_n]]$results_expm1
  results_expm2 = hold_results[[fold_n]]$results_expm2
  results_expm3 = hold_results[[fold_n]]$results_expm3
  results_risk1 = hold_results[[fold_n]]$results_risk1
  results_risk2 = hold_results[[fold_n]]$results_risk2
  results_risk3 = hold_results[[fold_n]]$results_risk3
  results_surv = hold_results[[fold_n]]$results_surv
  
  results_expm1_point_estim = hold_results[[fold_n]]$results_expm1_point_estim
  results_expm2_point_estim = hold_results[[fold_n]]$results_expm2_point_estim
  results_expm3_point_estim = hold_results[[fold_n]]$results_expm3_point_estim
  results_risk1_point_estim = hold_results[[fold_n]]$results_risk1_point_estim
  results_risk2_point_estim = hold_results[[fold_n]]$results_risk2_point_estim
  results_risk3_point_estim = hold_results[[fold_n]]$results_risk3_point_estim
  results_surv_point_estim = hold_results[[fold_n]]$results_surv_point_estim

  expm_df_plot1 <- make_expm_plot_entries(results_expm1, results_expm1_point_estim)
  expm_df_plot2 <- make_expm_plot_entries(results_expm2,  results_expm2_point_estim)
  expm_df_plot3 <- make_expm_plot_entries(results_expm3,  results_expm3_point_estim)
  risk_df_plot1 <- make_riskscore_plot_entries(results_risk1,  results_risk1_point_estim)
  risk_df_plot2 <- make_riskscore_plot_entries(results_risk2,  results_risk2_point_estim)
  risk_df_plot3 <- make_riskscore_plot_entries(results_risk3,  results_risk3_point_estim)
  surv_df_plot  <- make_surv_plot_entries(results_surv,  results_surv_point_estim)

  expm_df_plot1$Notation <- "C"
  expm_df_plot2$Notation <- "C_tau"
  expm_df_plot3$Notation <- "C_tau_10"  
  risk_df_plot1$Notation <- "C_risk"
  risk_df_plot2$Notation <- "C_risk_tau"
  risk_df_plot3$Notation <- "C_risk_tau_10"  
  surv_df_plot$Notation  <- "C_td" 
  
  df_plot <- bind_rows(expm_df_plot1, risk_df_plot1, 
                       expm_df_plot2, risk_df_plot2, 
                       expm_df_plot3, risk_df_plot3,
                       surv_df_plot)
  
  df_plot$Metric <- sub("::.*", "", df_plot$Metric)
  
  df_plot$Notation <- factor(df_plot$Notation,
    levels = c("C", "C_risk", "C_tau_10", "C_risk_tau_10", "C_tau", "C_risk_tau", "C_td"),
    labels = c(
      "tilde(C)~(Expected~Mortality)",
      "tilde(C)~(RiskScore)",
      "tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
      "tilde(C)[tau]~(RiskScore~tau==10~years)",
      "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
      "tilde(C)[tau]~(RiskScore~tau==max*(T:~Delta==1))",
      "tilde(C)[td]~(Survival~Distribution)"
    )
  )
  
  df_plot$Model <- factor(df_plot$Model, levels = c("DeepSurv", "CoxTime", "CoxPH", "RSF", "DeepHit"))
  
  df_plot$Metric <- factor(df_plot$Metric, levels = unique(df_plot$Metric))
  # Get numeric positions of each Metric for vertical lines
  metric_levels <- levels(df_plot$Metric)
  n_metrics <- length(metric_levels)
  
  # Define where to put the vertical lines between metrics
  separator_positions <- seq(1.5, n_metrics - 0.5, by = 1)
  
  ### Version with color
  p <- ggplot(df_plot, aes(x = Metric, y = cindex, color = Model)) +
    geom_pointrange(aes(ymin = lower, ymax = upper),
                    position = position_dodge(width = 0.4), size = 0.5) +
    facet_wrap(~ Notation, nrow = 4, ncol=2, scales = "free_x", labeller = label_parsed)+
    ylim(0.5, 0.75) +
    geom_vline(xintercept = separator_positions, 
                linetype = "dashed", color = "grey70") +
     #geom_hline(linetype = "dashed", yintercept = 0.5) +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
    labs(#title = paste0("Fold ", fold_n),
         y = "C-index", x = NULL) +
    theme_minimal(base_size = 14) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank())
  
  plot_list[[paste0("fold_", fold_n)]] <- p
  
  ### Version for paper
  # p <- ggplot(df_plot, aes(x = Metric, y = cindex)) +
  #   geom_pointrange(aes(ymin = lower, ymax = upper, shape=Model),
  #                   position = position_dodge(width = 0.6), size = 0.5, color = "black") +
  #   facet_wrap(~ Notation, nrow = 1, scales = "free_x", labeller = label_parsed)+
  #   scale_shape_manual(values = c("RSF" = 15, "DeepHit" = 23,
  #                                  "CoxPH" = 19, "DeepSurv" = 22, "CoxTime" = 24)) +
  #   geom_vline(xintercept = separator_positions,
  #               linetype = "dashed", color = "grey70") +
  #   ylim(0.5, 0.75) +
  #   #geom_hline(linetype = "dashed", yintercept = 0.5) +
  #   geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
  #   labs(title = paste0("Fold ", fold_n),
  #        y = "C-index", x = NULL) +
  #   theme_minimal(base_size = 14) +
  #   theme(axis.text.x = element_text(angle = 45, hjust = 1),
  #         legend.position = "bottom",
  #   panel.grid.major.x = element_blank(),
  #   panel.grid.minor.x = element_blank())
  # 
  # plot_list[[paste0("fold_", fold_n)]] <- p
  
}

combined_plot <- wrap_plots(plot_list)
#print(combined_plot)

```
```{r plot fold one, fig.width=12, fig.height=15}
plot_list$fold_1 + theme(plot.title = element_blank())
```



### Table with results


```{r Table generation}

notation_map <- data.frame(
  Metric = c("pycox.Ant", "pycox.Adj.Ant", 
             "pec", "survC1", "survival.n/G2", "sksurv.ipcw","survival.n", 
             "Hmisc", "SurvMetrics", "lifelines", "pysurvival", 
             "sksurv.censored"),
  Notation = c("$C_{td}$", "$C_{td}$", 
               "$C_{\\tau}$", "$C_{\\tau}$", "$C_{\\tau}$", 
               "$C_{\\tau}$", "$C_{\\tau}$",
               "$C$", "$C$", "$C$", "$C$", "$C$"),
  stringsAsFactors = FALSE
)

all_folds <- list()

for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  results_expm1 = hold_results[[fold_n]]$results_expm1
  results_expm2 = hold_results[[fold_n]]$results_expm2
  results_expm3 = hold_results[[fold_n]]$results_expm3
  results_surv = hold_results[[fold_n]]$results_surv
  
  results_expm1_point_estim = hold_results[[fold_n]]$results_expm1_point_estim
  results_expm2_point_estim = hold_results[[fold_n]]$results_expm2_point_estim
  results_expm3_point_estim = hold_results[[fold_n]]$results_expm3_point_estim
  results_surv_point_estim = hold_results[[fold_n]]$results_surv_point_estim

  a <- make_expm_table(results_expm1, results_expm1_point_estim)
  a$Fold <- fold_n
  b <- make_expm_table(results_expm2, results_expm2_point_estim)
  b$InputType <- "Exp.Mort tau=max(T, delta=1)"
  b$Fold <- fold_n
  c <- make_expm_table(results_expm3, results_expm3_point_estim)
  c$InputType <- "Exp.Mort tau=10y"
  c$Fold <- fold_n
  d <- make_surv_table(results_surv, results_surv_point_estim)
  d$Fold <- fold_n
  
  df_combined <- bind_rows(a, b, c, d)
  
  df_combined$Metric <- sub("::.*", "", df_combined$Metric)
  df_combined <- merge(df_combined, notation_map, by= "Metric", all.x = TRUE)

  all_folds[[fold_n]] <- df_combined

} 

final_table <- bind_rows(all_folds)


```




#### Save table


```{r latex table}
library(kableExtra)

df_combined_bolded <- final_table

for (i in 1:nrow(df_combined_bolded)) {
  row <- df_combined_bolded[i, model_names]
  # only the mean, not the ci
  numeric_values <- as.numeric(sub(" .*", "", row))

  max_index <- which.max(numeric_values)
  
  # only bold the mean
  row[max_index] <- sub("^(\\d+\\.\\d+)", "\\\\textbf{\\1}", row[max_index])
  
  df_combined_bolded[i, model_names] <- row
}

df_combined_bolded <- df_combined_bolded %>% arrange(factor(InputType, levels = c("Distrib", "Exp.Mort", "Exp.Mort tau=10y", "Exp.Mort tau=max(T, delta=1)" )))


latex_code <- kable(df_combined_bolded, format = "latex", 
                    booktabs = TRUE, 
                    escape = FALSE)

#writeLines(latex_code, "Paper/Tables/results_tableHoldOut.tex")
#writeLines(latex_code, "Results/PaperTables/results_table5fold_Seed123.tex")

```

