---
title: "Hold-Out Validation for METABRIC"
output: html_document
date: "2025-04-20"
---

# Introduction

Here we perform a hold-out validation of METABRIC, and compute the C-index with multiple implementations and input transformations. 

First we load the libraries

```{r Load libraries, include=FALSE}
# Survival metrics
library(reticulate)
library(arrow)
library(caret)
library(riskRegression)
library(prodlim)
library(pec)
library(survival)
library(survC1)
library(SurvMetrics)
library(rhdf5)
library(randomForestSRC)
library(Hmisc)
library(dplyr)
# Plotting
library(gridExtra)
# Parallelization
library(doFuture)
library(future)
library(progressr)
library(foreach)
library(MASS)
library(flexsurv)
library(furrr)
library(pysurvivalR)
library(survivalmodels)
library(tidyr)
library(ggplot2)
library(patchwork)
library(ggalluvial)
library(cowplot)
```

```{r setup, echo=TRUE, include=TRUE}
Sys.setenv(OMP_NUM_THREADS = "1")       # Limits OpenMP to 1 thread
Sys.setenv(NUMBA_NUM_THREADS = "1")     # Limits Numba to 1 thread
Sys.setenv(MKL_NUM_THREADS = "1")       # Limits Intel MKL to 1 thread
Sys.setenv(KMP_WARNINGS = "0")          # Disables OpenMP warnings
Sys.setenv(OPENBLAS_NUM_THREADS = "1")  # Limits OpenBLAS to 1 thread

library(reticulate)
py_config()

```

We also load a set of helper functions used throuhgout our analysis:

```{r load functions}
source("./CindexHelperFunctions.R")
```

## Load the data

Here, we load the pre-processed METABRIC data that was generated using the 
`preprocessing_biocportal_metabric.R` script.

```{r load data}
# read the processed file
mb <- read.table("./Datasets/metabric_preprocess_multiverse.csv", 
                 sep = "," , header = TRUE)

# Keep the id as index
rownames(mb) <- mb$PATIENT_ID

# Remove the id column
mb <- mb[,2:12]

# Mapping
var_map <- c(
  "MKI67" = "X1",
  "EGFR" = "X2",
  "ERBB2" = "X3",
  "PGR" = "X4",
  "HORMONE_THERAPY" = "X5",
  "RADIO_THERAPY" = "X6",
  "CHEMOTHERAPY" = "X7",
  "ER_IHC" = "X8",
  "AGE_AT_DIAGNOSIS" = "X9",
  "OS_MONTHS" = "time",
  "OS_STATUS" = "status"
)

# Mapping colnames
names(mb)[names(mb) %in% names(var_map)] <- var_map[names(mb)[names(mb) %in% names(var_map)]]


summary(mb$time)


#mb <- mb[order(mb$time, -mb$status),]
```


The data contains information about `r nrow(mb)` individuals, with 
`r sum(mb$status == 1)` events and `r sum(mb$status == 0)` censored 
observations. The survival times are measured in months.

## Evaluation of predictive performance using a data holdout approach

Here, we will evaluate the predictive performance of various survival models 
using a holdout approach. We will split the data into a training set (80%) and a 
test set (20%), fit the models on the training set, and then evaluate their 
performance on the test set. We repeated this 5 times to ensure robustness of 
the results. The data splits used here match the ones used in the 5-fold 
cross-validation results. For each data split, bootstrap samples of the test set
will be generated to estimate the uncertainty of the estimated C-index values. 

### Model hyperparameters

For the hyperparameters of deep learning models that have been trained with 
`survivalmodels` package is relevant to look at the pytorch optimizer: https://raphaels1.github.io/survivalmodels/reference/get_pycox_optim.html 

Also the parameters for each model:

1) Deephit: https://raphaels1.github.io/survivalmodels/reference/deephit.html
2) DeepSurv: https://raphaels1.github.io/survivalmodels/reference/deepsurv.html
3) CoxTime: https://raphaels1.github.io/survivalmodels/reference/coxtime.html

In our analysis, we consider the same hyperparameter values used when analysing
the METABRIC in the original publications for DeepHit, DeepSurv, and CoxTime. 
For RSF, we use the default hyperparameters from the `randomForestSRC` package.


\begin{table}[H]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Hyper-parameter}     & \textbf{DeepSurv} & \textbf{Cox-Time} & \textbf{DeepHit}  &  \textbf{RSF} \\ \midrule
Optimizer                    & adam              & adam             & adam              & - \\
Activation                   & selu              & relu             & relu              & -\\
\# Dense Layers              & 1                 & 2                & 2                 & - \\
\# Nodes / Layer             & 41                & 32, 32           & 32, 32            & 100 trees\\
Learning Rate                & 0.0103            & 0.01             & 0.001             & - \\
Dropout                      & 0.1601            & 0.1              & 0.6               & - \\
LR Decay                     & 0.00417           & 0                & 0                 & - \\
Batch Norm                   & True             & True             & True              & - \\
Batch Size                   & 256               & 256              & 50                & - \\
Epochs                       & 500               & 512              & 100               & - \\
Early Stopping               & False             & True             & True              & - \\
mod\_alpha                   & -                & -               & 0.2               & - \\
sigma                        & -                & -               & 0.1               & - \\
cuts                         & -                & -               & 300               & - \\ \bottomrule
\end{tabular}
\caption{Hyperparameters across DeepSurv, Cox-Time, DeepHit and RSF. For DeepSurv, Cox-Time and DeepHit, hyperparameters were specified as in the analysis of the METABRIC data presented in the corresponding original publications.}
\label{tab:hyperparams}
\end{table}


### Model training

As the training of the models can take a long time, we save the results to an 
Rds file which we load here. We can use the results from 5-fold cross-validation and analyse each fold separately. 


You can load the results:

```{r load results}

folds_stacked_predictions <-  readRDS("./Results/HoldOutStackedPredictions.rds")

survival_curves <- readRDS("./Results/HoldOutStackedPredictionsCurves.rds")

```

Alternatively, perform the validation as follows:

We perform hold out validation for 5 folds with stratification of outcomes between folds. We standardised numerical variables. Moreover, we interpolate predictions to a common time-grid. 


```{r model predictions, eval=FALSE}
## Reproducibility
reset_torch_seed <- function(seed = 123) {
  reticulate::py_run_string(sprintf("
import torch
import numpy as np
import random
torch.manual_seed(%d)
np.random.seed(%d)
random.seed(%d)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
", seed, seed, seed))
}

set.seed(123)
survivalmodels::set_seed(seed_R=123, seed_np = 123, seed_torch = 123) 

### Crossval
# Number of folds
K <- 5 
n <- nrow(mb)  

# Interesting times for comparison
#ts <- sort(unique(round(mb$time)))
all_predictions <- list()

# List of numeric column for standarization
numeric_cols <- c("X1", "X2", "X3", "X4", "X9")

# Create empty list for survival curves
survival_curves <- vector("list", K)

# Shuffle indices and create folds
#indices <- sample(seq_len(n))
#folds <- cut(indices, breaks = K, labels = FALSE)
folds <- createFolds(mb$status, k = K, list = TRUE) # return test sets

# Interesting intervals for prediction
mb_t_max <- round(max(mb$time))
ts_scaled <- seq(0, mb_t_max, 1)

for (k in 1:K) {

  # Define test and training indices
  #mb_test_idx  <- which(folds == k)
  #mb_train_idx <- setdiff(seq_len(n), mb_test_idx)
  
  # Define test and training indices
  mb_test_idx  <- folds[[k]]
  mb_train_idx <- setdiff(seq_len(nrow(mb)), mb_test_idx)
  
  # Subset the dataset
  mb_train <- mb[mb_train_idx, ]
  mb_test  <- mb[mb_test_idx, ]
  
  t_train_max <- max(mb_train$time)
  
  # Scale continuous 
  means <- sapply(mb_train[, numeric_cols], mean)
  sds   <- sapply(mb_train[, numeric_cols], sd)

  mb_train[, numeric_cols] <- scale(mb_train[, numeric_cols],
                                    center = means, scale = sds)
  mb_test[, numeric_cols]  <- scale(mb_test[, numeric_cols],
                                    center = means, scale = sds)

  mb_train$time <- mb_train$time
  mb_test$time <- mb_test$time
  
  # Extract only the covariates used for predictions
  test_covariates <- mb_test[, c("X1", "X2", "X3", "X4",
                                 "X5", "X6", "X7", "X8", "X9")]

  # Fit models
  #cat("Dataset:", i, ".", j, '\n')
  cat("Fold:", k, '\n')
  cat("tmax:", t_train_max, "\n")
  cat('Train set dimensions:', dim(mb_train), '\n')
  cat('Train set event:', mean(mb_train$status == 1)*100, '\n')
  cat('Train set censoring:', mean(mb_train$status == 0)*100, '\n')
  cat('Test set dimensions:', dim(mb_test), '\n')
  cat('Test set event:', mean(mb_test$status == 1)*100, '\n')
  cat('Test set censoring:', mean(mb_test$status == 0)*100, '\n')
  cat('\n')
  
  # 1) Random Forest:
  # Fit the model
  rf <- randomForestSRC::rfsrc(Surv(time, status) ~ X1 + X2 +
                                 X3 + X4 + X5 + X6 + 
                                 X7 + X8 + X9, 
                               data = mb_train, 
                               ntree = 100, 
                               ntime = 1400)
  # Predict survival object
  surv_rf_object <- predict(rf, newdata = mb_test, type = "surv")
  surv_rf <- surv_rf_object$survival
  time_grid <- surv_rf_object$time.interest
  # Interpolate to have same times in all survival curves
  surv_rf_int <- apply(surv_rf, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_rf_int <- t(surv_rf_int)
  rownames(surv_rf_int) <- rownames(mb_test)
  colnames(surv_rf_int) <- ts_scaled
  # Calculate expected mortality

  if (any(as.vector(surv_rf_int) == 0)) {
    cat("There is a 0 in RSF survival preds \n")
    second_smallest_survival_prob = unique(sort(as.vector(surv_rf_int), partial = 2))[2]/10
    print(second_smallest_survival_prob)
    surv_rf_int <- surv_rf_int + second_smallest_survival_prob
  } 
  rf_exp_mort <- rowSums(-log(surv_rf_int))
  rf_rmst <- (-rowSums(surv_rf_int))
  
  # 2) Predict risk Cox PH
  # Fit with cox ph 
  cox_ph <- survival::coxph(Surv(time, status) ~ X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                               data = mb_train)
  
  # Predict survival object
  sf <- survfit(cox_ph, newdata = mb_test)
  surv_cox <- t(sf$surv)
  time_grid <- sf$time 
  #Interpolate
  surv_cox_int <- apply(surv_cox, 1, function(s){
     approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  
  surv_cox_int <- t(surv_cox_int)
  rownames(surv_cox_int) <- rownames(mb_test)
  colnames(surv_cox_int)  <- ts_scaled
  # Calculate expected mortality
  if (any(as.vector(surv_cox_int) == 0))  {
    cat("There is a 0 in cph survival preds \n")
    second_smallest_survival_prob = unique(sort(as.vector(surv_cox_int), partial = 2))[2]/10
    print(second_smallest_survival_prob)   
    surv_cox_int <- surv_cox_int + second_smallest_survival_prob
  } 
  cox_exp_mort <- rowSums(-log(surv_cox_int))
  cox_rmst <- (-rowSums(surv_cox_int))

  reset_torch_seed()
  
  # 3) DeepSurv
  # Hyperparameters: https://github.com/jaredleekatzman/DeepSurv/blob/41eed003e5b892c81e7855e400861fa7a2d9da4f/experiments/deepsurv/models/metabric_IHC4_clinical_adam_0.json
  deepsurv_model <- survivalmodels::deepsurv(Surv(time, status) ~ 
                                               X1 + X2 + 
                                               X3 + X4 + X5 + X6 + 
                                               X7 + X8 + X9, 
                                             data = mb_train, 
                                             frac = 0.2, 
                                             dropout = 0.160087890625,
                                             optimizer = "adam",
                                             activation = "selu", 
                                             batch_norm = TRUE,
                                             num_nodes = c(41), 
                                             batch_size = 256L, 
                                             learning_rate = 0.010289691253027908, 
                                             lr_decay = 0.0041685546875, 
                                             momentum =  0.8439658203125,
                                             #weight_decay = 10.890986328125, # this is too high it cannot be 
                                             epochs = 500) 
  # Predict survival object
  surv_deepsurv <- predict(deepsurv_model,
                           newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deepsurv))
  # Interpolate
  surv_deepsurv_int <- apply(surv_deepsurv, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deepsurv_int <- t(surv_deepsurv_int)
  rownames(surv_deepsurv_int) <- rownames(mb_test)
  colnames(surv_deepsurv_int) <- ts_scaled
  # Calculate expected mortality
  if (any(as.vector(surv_deepsurv_int) == 0)) {
    cat("There is a 0 in DeepSurv survival preds \n")
    second_smallest_survival_prob = unique(sort(as.vector(surv_deepsurv_int), partial = 2))[2]/10
    print(second_smallest_survival_prob)
    surv_deepsurv_int<- surv_deepsurv_int + second_smallest_survival_prob
  } 
  deepsurv_exp_mort <- rowSums(-log(surv_deepsurv_int))
  deepsurv_rmst <- (-rowSums(surv_deepsurv_int))
  
  reset_torch_seed()
  # 4) Coxtime
  # Hyperparameters from:
  # https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/cox-time.ipynb
  coxtime_model <- survivalmodels::coxtime(Surv(time, status) ~ 
                                             X1 + X2 + X3 + 
                                             X4 + X5 + X6 + 
                                              X7 + X8 + X9, 
                                           data = mb_train, 
                                           optimizer = "adam",
                                           learning_rate = 0.01,       
                                           betas = c(0.9, 0.999), # defaults
                                           activation = "relu",
                                           num_nodes = c(32L, 32L),
                                           batch_norm = TRUE,
                                           dropout = 0.1,
                                           batch_size = 256,
                                           epochs = 512,
                                           early_stopping = TRUE, 
                                           frac = 0.2)
  
  # Predict survival object
  surv_coxtime <- predict(coxtime_model,
                          newdata = mb_test, type = "surv")
  time_grid <-  as.numeric(colnames(surv_coxtime))
  # Interpolate
  surv_coxtime_int <- apply(surv_coxtime, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_coxtime_int <- t(surv_coxtime_int)
  rownames(surv_coxtime_int) <- rownames(mb_test)
  colnames(surv_coxtime_int) <- ts_scaled
  # Calculate expected mortality
  if (any(as.vector(surv_coxtime_int) == 0)) {
    cat("There is a 0 in Coxtime survival preds \n")
    second_smallest_survival_prob = unique(sort(as.vector(surv_coxtime_int), partial = 2))[2]/10
    print(second_smallest_survival_prob)
    surv_coxtime_int <- surv_coxtime_int + second_smallest_survival_prob
  } 
  coxtime_exp_mort <- rowSums(-log(surv_coxtime_int))
  coxtime_rmst <- (-rowSums(surv_coxtime_int))
  
  reset_torch_seed()
  
  # 5) Deephit
  # Hyperparameters: https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/deephit.ipynb and DeepHit paper, and also double checked with DeSurv paper (some parameters are slightly different)
  deephit_model <- survivalmodels::deephit(Surv(time, status) ~ 
                             X1 + X2 + 
                             X3 + X4 + X5 + X6 + 
                             X7 + X8 + X9, 
                             data = mb_train, 
                             optimizer = "adam",
                             activation = "relu", # in DeSurv, ipynb
                             num_nodes = c(32L, 32L), # in ipynb and DeSurv
                             batch_norm = TRUE, # in ipynb
                             dropout = 0.6, #  0.1 in ipynb and 0.6 in DeepHit paper
                             #cuts = 1400, # in DeSurv and DeepHit, 10 and interpolation in ipynb
                             mod_alpha = 0.2, # in DeSurv, ipynb
                             early_stopping = TRUE, # in DeepHit and ipynb
                             #tolerance = 3,
                             cuts = 1400,
                             sigma = 0.1, # in DeSurv and ipynb
                             batch_size = 50, # 50 in Deephit, 256 in ipynb
                             epochs = 100, # in ipynb
                             #verbose = TRUE,
                             learning_rate = 0.001, # 0.001 in Deephit, 0.01 in ipynb
                             #betas = c(0.9, 0.999), # default adam
                             frac = 0.2) # validation 

  # Survival object
  surv_deephit <- predict(deephit_model,
                          newdata = mb_test, type = "surv")
  time_grid <- as.numeric(colnames(surv_deephit))# * t_train_max
  # Interpolate
  surv_deephit_int <- apply(surv_deephit, 1, function(s){
    approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
  })
  surv_deephit_int <- t(surv_deephit_int)
  rownames(surv_deephit_int) <- rownames(mb_test)
  colnames(surv_deephit_int) <- ts_scaled
  # Calculate expected mortality
  if (any(as.vector(surv_deephit_int) == 0))  {
    cat("There is a 0 in Deephit survival preds \n")
    second_smallest_survival_prob = unique(sort(as.vector(surv_deephit_int), partial = 2))[2]
    print(second_smallest_survival_prob)
    surv_deephit_int <- surv_deephit_int + second_smallest_survival_prob
  } 
  deephit_exp_mort <- rowSums(-log(surv_deephit_int))
  deephit_rmst <- (-rowSums(surv_deephit_int))

  # Gather the survival survs too
  survival_curves[[k]] <- list(patients_ids = rownames(mb_test),
                               test_time = mb_test$time,
                               test_status = mb_test$status,
                               RSF = as.data.frame(surv_rf_int),
                               CoxPH = as.data.frame(surv_cox_int),
                               DeepHit = as.data.frame(surv_deephit_int),
                               DeepSurv = as.data.frame(surv_deepsurv_int),
                               Coxtime = as.data.frame(surv_coxtime_int),
                               covariates = test_covariates)
  
  fold_results <- data.frame(cv_fold = k,
                             patients_ids = rownames(mb_test), 
                             test_time = mb_test$time,
                             test_status = mb_test$status,
                             ExpMort.RSF = rf_exp_mort,
                             ExpMort.CoxPH = cox_exp_mort,
                             ExpMort.DeepHit = deephit_exp_mort,
                             ExpMort.DeepSurv = deepsurv_exp_mort,
                             ExpMort.CoxTime = coxtime_exp_mort,
                             RMST.RSF = rf_rmst,
                             RMST.CoxPH = cox_rmst,
                             RMST.DeepHit = deephit_rmst,
                             RMST.DeepSurv = deepsurv_rmst,
                             RMST.CoxTime = coxtime_rmst,
                             RSF = as.data.frame(surv_rf_int),
                             CoxPH = as.data.frame(surv_cox_int),
                             DeepHit = as.data.frame(surv_deephit_int), 
                             DeepSurv = as.data.frame(surv_deepsurv_int),
                             CoxTime = as.data.frame(surv_coxtime_int), 
                             covariates = test_covariates)
    

  # Append fold results to the list
 all_predictions[[paste("Fold", k)]] <- fold_results

}

folds_stacked_predictions <- do.call(rbind, all_predictions)

#saveRDS(folds_stacked_predictions, "./Results/HoldOutStackedPredictions.rds")
#saveRDS(survival_curves, "./Results/HoldOutStackedPredictionsCurves.rds")
```

```{r Plot RMST, fig.height=10, fig.width=8}

folds_long_rmst <- folds_stacked_predictions %>%
  pivot_longer(
    cols = starts_with("RMST"),
    names_to = "Model",
    values_to = "RMST"
  ) %>%
  mutate(Model = gsub("RMST\\.", "", Model))

# In color
ggplot(folds_long_rmst, aes(x = as.factor(cv_fold), y = RMST, fill = as.factor(cv_fold))) +
  geom_violin(trim = FALSE, alpha=0.4) +
  stat_summary(fun = median, geom = "point", shape = 21, size = 2, color = "black", fill = "black") +
  facet_wrap(~ Model, nrow = 3) +
  labs(
    title = "",
    x = "Cross-validation Fold",
    y = "Restricted Mean Survival Time (RMST)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Pastel1")
```

### Censoring probabilities and IPCW

To facilitate the interpretation of the differences across C-index estimates,
we calculate the censoring probabilities for the training and test sets.

```{r IPCW per fold, fig.height=10, fig.width=8}
# Number of folds
folds <- unique(folds_stacked_predictions$cv_fold)
n_folds <- length(folds)

par(mfrow = c(3, 2), mar = c(4, 4, 3, 1))  # adjust margins

# Loop through folds
for (fold_n in folds) {
  cat("Processing Fold:", fold_n, "\n")

  # Test set
  test <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ]
  test$censor.status <- ifelse(test$test_status == 0, 1, 0)  # 1 = censored
  test <- test[order(test$test_time), ]

  weight.j.test <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = test,
    method = "marginal",
    times = unique(test$test_time),
    subjectTimes = test$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtest <- 1 / weight.j.test^2
  time_points_test <- unique(test$test_time)

  fit_test <- prodlim(Hist(test_time, test_status) ~ 1, data = test, reverse = TRUE)

  # Train set
  train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ]
  train$censor.status <- ifelse(train$test_status == 0, 1, 0)
  train <- train[order(train$test_time), ]

  weight.j.train <- pec::ipcw(
    formula = Surv(test_time, censor.status) ~ 1,
    data = train,
    method = "marginal",
    times = unique(train$test_time),
    subjectTimes = train$test_time,
    subjectTimesLag = 0,
    what = "IPCW.times"
  )$IPCW.times

  IPCWtrain <- 1 / weight.j.train^2
  time_points_train <- unique(train$test_time)

  fit_train <- prodlim(Hist(test_time, test_status) ~ 1, data = train, reverse = TRUE)

  # Plot both IPCW curves in black
  # plot(time_points_test, IPCWtest,
  #      type = "l", col = "black", lwd = 2, lty = 2,
  #      xlab = "Time (months)", ylab = "1 / G²(t)",
  #      main = paste0("IPCW - Fold ", fold_n))
  # lines(time_points_train, IPCWtrain, col = "black", lwd = 2)
  # 
  # legend("topleft", legend = c("Test", "Train"),
  #        col = c("black", "black"),
  #        lty = c(2, 1), lwd = 1, bty = "n")
  
  # Plot both IPCW in color
  plot(time_points_test, IPCWtest,
       type = "l", col = "darkorange", lty = c(1, 10), lwd = 2,
       xlab = "Time (months)", ylab = "1 / G²(t)",
       main = paste0("IPCW - Fold ", fold_n))
  lines(time_points_train, IPCWtrain, col = "steelblue", lwd = 2)

  legend("topleft", legend = c("Test", "Train"),
         col = c("darkorange", "steelblue"),
         lty = c(1, 1), lwd = 2, bty = "n")
  
}

```

```{r censoring-probabilities, fig.height=10, fig.width=8}

plot_censoring_with_risk_table <- function(fold_n, data) {
  test <- data[data$cv_fold == fold_n, ]
  train <- data[data$cv_fold != fold_n, ]
  
  fit_test <- prodlim(Hist(test_time, test_status) ~ 1, data = test, reverse = TRUE)
  fit_train <- prodlim(Hist(test_time, test_status) ~ 1, data = train, reverse = TRUE)
  
  # Extract survival curves
  surv_train <- summary(fit_train, times = fit_train$time)
  surv_test <- summary(fit_test, times = fit_test$time)
  
  df_train <- data.frame(time = fit_train$time,
                         surv = 1 - fit_train$surv,
                         group = "Train")
  df_test <- data.frame(time = fit_test$time,
                        surv = 1 - fit_test$surv,
                        group = "Test")
  
  df <- rbind(df_train, df_test)
  
  p <- ggplot(df, aes(x = time, y = surv, color = group, linetype = group)) +
    geom_step(size = 1) +
    ylim(0, 1) +
    labs(title = paste0("Fold ", fold_n),
         x = "Time (months)",
         y = "Censoring Probability P(C < t)") +
    scale_color_manual(values = c("Train" = "steelblue", "Test" = "darkorange")) +
    scale_linetype_manual(values = c("Train" = "solid", "Test" = "solid")) +
    theme_minimal() +
    theme(legend.title = element_blank())
  return(p)
}

# Example usage
p1 <- plot_censoring_with_risk_table(fold_n = 1, 
                                     data = folds_stacked_predictions)
p2 <- plot_censoring_with_risk_table(fold_n = 2, 
                                     data = folds_stacked_predictions)
p3 <- plot_censoring_with_risk_table(fold_n = 3,
                                     data = folds_stacked_predictions)
p4 <- plot_censoring_with_risk_table(fold_n = 4, 
                                     data = folds_stacked_predictions)
p5<- plot_censoring_with_risk_table(fold_n = 5, 
                                    data = folds_stacked_predictions)

(p1 | p2) / (p3 | p4) / (p5 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")
```


### Predicted survival curves

Here, we plot the predicted survival curves

```{r survival curves, fig.height=11, fig.width=10}

p1 <- plot_survival_curves(survival_curves[[1]]$DeepHit, title = "DeepHit", seed=123)
p2 <- plot_survival_curves(survival_curves[[1]]$DeepSurv, title = "DeepSurv", seed=123)
p3 <- plot_survival_curves(survival_curves[[1]]$CoxPH, title = "CPH", seed=123)
p4 <- plot_survival_curves(survival_curves[[1]]$Coxtime, title = "CoxTime", seed=123)
p5 <- plot_survival_curves(survival_curves[[1]]$RSF, title = "RSF", seed=123)

(p1 | p2) / (p3 | p4) / (p5 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

```
### Bootstraping.

Here, we apply the difference C-index implementations to quantify predictive 
performance in each test set. To quantify uncertainty for the C-index estimates, 
we use bootstraping. For each test set, 100 bootstrap samples (same sample size 
as the test set) were generated and the same C-index implementations were 
applied for each bootstrap sample. 


```{r  Maximum times}
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ]
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])

  cat("Test set maximum time in fold ", fold_n, " is ", max(stacked_predictions$test_time) ," and maximum uncensored time is ",max_uncensored_time, "\n")
}
```

The max of uncensored time would vary per fold. 

We can load the results of the bootstrap:

```{r load holdout}
hold_results <- readRDS("./Results/holdout_CindexResults.rds")
```

Alternatively, we can run the following to perform the C-index evaluations:

```{r c-index bootstrap, eval=FALSE}

hold_results <- vector("list", 5) 
set.seed(123)

for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  # Separate by fold test (stacked_predictions) and train 
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ] 
  mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ] 
  # Extract the maximum uncensored time per fold
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])

  # Set the number of boostraps
  n_bootstraps <- 100
  # Length of test
  dataset_size <- nrow(stacked_predictions) 
  # resample by patient id
  resample_indices <- replicate(n_bootstraps, 
                                sample(stacked_predictions$patients_ids, 
                                       size = dataset_size, replace = TRUE), 
                                simplify = FALSE)
 
  #print(head(resample_indices[[1]]))
  # Create empty lists to store results
  subset_expm <- list()
  subset_surv <- list()
  subset_risk <- list()
  # Create resample sets
  for (r in seq_along(resample_indices)) {
    subset_expm[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                   model_names = "all",
                   input_type = "ExpectedMortality",
                   bootstrap_patient_ids = resample_indices[[r]])
    subset_surv[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                  model_names = "all",
                  input_type = "Distribution",
                  bootstrap_patient_ids = resample_indices[[r]])
    subset_risk[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions,
                        model_names = "all",
                        input_type = "RMST",
                        bootstrap_patient_ids = resample_indices[[r]])

  }

  # Expected mortality and estimating C
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm1 <- list() # boostrap results
  results_expm1_point_estim <- list() # point estimate results
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
     # Bootstrap
     results_expm1[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                    eval.times = NULL, #C
                                    sampled_data = subset_expm)
     # Point estimate
     results_expm1_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                          censoring = stacked_predictions$test_status,
                                                          time = stacked_predictions$test_time,
                                                          implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                                          eval.times = NULL)

  }

  # Expected mortality and estimating C tau with max uncensored time
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm2 <- list() # boostrap results
  results_expm2_point_estim <- list() # point estimate
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
    # Bootstrap
     results_expm2[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    #eval.times = round(max(mb$time[mb$status == 1])),
                                    eval.times = max_uncensored_time,
                                    sampled_data = subset_expm,
                                    additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
      # Point estimate
     results_expm2_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                                      #eval.times =  round(max(mb$time[mb$status == 1])),
                                                      eval.times =  max_uncensored_time,
                                                      sksurv_train_time = mb_train$test_time,
                                                      sksurv_train_status = mb_train$test_status,
                                                      sksurv_tied_tol = 1e-8)
  }
    # Expected mortality and estimating C tau at 10 years
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm3 <- list()
  results_expm3_point_estim <- list()
  for (exps in select_exps) {
    cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
    # Bootstrap
    results_expm3[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    eval.times = 120, # 10 years
                                    sampled_data = subset_expm,
                                  additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
     # Point estimate
     results_expm3_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]],
                                                censoring = stacked_predictions$test_status,
                                                time = stacked_predictions$test_time,
                                                implementation = list("pec::cindex",
                                                    "survC1::Est.Cval",
                                                    "survival.n",
                                                    "survival.n/G2",
                                                    "sksurv.ipcw"),
                                                eval.times =  120, # 10 years
                                                sksurv_train_time = mb_train$test_time,
                                                sksurv_train_status = mb_train$test_status,
                                                sksurv_tied_tol = 1e-8)
  }
    # Expected mortality and estimating C
  select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk1 <- list() # boostrap results
  results_risk1_point_estim <- list() # point estimate results
  for (risk in select_risk) {
     cat("Calculating bootstrap for RMST for Model = ", risk, "\n")
     # Bootstrap
     results_risk1[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                    eval.times = NULL, #C
                                    sampled_data = subset_risk)
     # Point estimate
     results_risk1_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                          censoring = stacked_predictions$test_status,
                                                          time = stacked_predictions$test_time,
                                                          implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex",
                                                          "lifelines",
                                                          "sksurv.censored"),
                                                          eval.times = NULL)

  }

  select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk2 <- list() # boostrap results
  results_risk2_point_estim <- list() # point estimate
  for (risk in select_risk) {
     cat("Calculating bootstrap for RMST for Model = ", risk, "\n")
    # Bootstrap
     results_risk2[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    #eval.times = round(max(mb$time[mb$status == 1])),
                                    eval.times = max_uncensored_time,
                                    sampled_data = subset_risk,
                                    additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
      # Point estimate
     results_risk2_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                                      #eval.times =  round(max(mb$time[mb$status == 1])),
                                                      eval.times =  max_uncensored_time,
                                                      sksurv_train_time = mb_train$test_time,
                                                      sksurv_train_status = mb_train$test_status,
                                                      sksurv_tied_tol = 1e-8)
  }

  select_risk <- grep("RMST\\.",colnames(subset_risk[[1]]), value = TRUE)
  results_risk3 <- list()
  results_risk3_point_estim <- list()
  for (risk in select_risk) {
    cat("Calculating bootstrap for RMST for Model = ", risk, "\n")
    # Bootstrap
    results_risk3[[risk]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = risk,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval",
                                                          "survival.n",
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    eval.times = 120, # 10 years
                                    sampled_data = subset_risk,
                                  additional=list(
                                sksurv_train_time = mb_train$test_time,
                                sksurv_train_status = mb_train$test_status,
                                sksurv_tied_tol = 1e-8))
     # Point estimate
     results_risk3_point_estim[[risk]] <- metrics.wrapper(predicted = stacked_predictions[[risk]],
                                                censoring = stacked_predictions$test_status,
                                                time = stacked_predictions$test_time,
                                                implementation = list("pec::cindex",
                                                    "survC1::Est.Cval",
                                                    "survival.n",
                                                    "survival.n/G2",
                                                    "sksurv.ipcw"),
                                                eval.times =  120, # 10 years
                                                sksurv_train_time = mb_train$test_time,
                                                sksurv_train_status = mb_train$test_status,
                                                sksurv_tied_tol = 1e-8)
  }
  model_names <- unique(sub("\\..*", "", grep("^[A-Za-z]+\\.\\d+$",
                                              colnames(subset_surv[[1]]), value = TRUE)))
  results_surv <- list() # Bootstrap result
  results_surv_point_estim <- list() # Point estimate
  # Based on survival probabilities
  for (model in model_names) {
    cat("Calculating bootstrap for Distribution for Model = ", model, "\n")
    # Bootstrap
    results_surv[[model]] <-  bootstrap.metric(metrics.wrapper,
                                    dataset=list(
                                       predicted = model,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pycox.Ant", "pycox.Adj.Ant"),
                                    sampled_data = subset_surv)
    # Generate matrix
    surv_mat = stacked_predictions[, grep(paste0("^", model), names(stacked_predictions),
                                          value = TRUE), drop = FALSE]
    colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
    # Point estimate
    results_surv_point_estim[[model]] <- metrics.wrapper(surv_matrix = surv_mat,
                                                      censoring = stacked_predictions$test_status,
                                                      time = stacked_predictions$test_time,
                                                      implementation = list("pycox.Ant",
                                                                            "pycox.Adj.Ant"))
  }

  # Store it:
  hold_results[[fold_n]] <- list( results_expm1 = results_expm1, #C
                                  results_expm1_point_estim = results_expm1_point_estim,
                                  results_expm2 = results_expm2, #C tau at max uncensored time
                                  results_expm2_point_estim = results_expm2_point_estim,
                                  results_expm3 = results_expm3, #C tau at 10 years
                                  results_expm3_point_estim = results_expm3_point_estim,
                                  results_surv  = results_surv,
                                  results_surv_point_estim = results_surv_point_estim,
                                  results_risk1 = results_risk1,
                                  results_risk1_point_estim = results_risk1_point_estim,
                                  results_risk2 = results_risk2,
                                  results_risk2_point_estim = results_risk2_point_estim,
                                  results_risk3 = results_risk3,
                                  results_risk3_point_estim = results_risk3_point_estim
  )

}

#saveRDS(hold_results, "./Results/holdout_CindexResults.rds")
```

#### Risk at t

```{r Rist at t in fold 1}

set.seed(123)
# Separate by fold test (stacked_predictions) and train 
stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == 1, ] 
mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != 1, ] 
# Extract the maximum uncensored time per fold
max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])

# Set the number of boostraps
n_bootstraps <- 100
# Length of test
dataset_size <- nrow(stacked_predictions) 
# resample by patient id
resample_indices <- replicate(n_bootstraps, 
                              sample(stacked_predictions$patients_ids, 
                                     size = dataset_size, replace = TRUE), 
                              simplify = FALSE)


subset_riskT <- list()

ts = sort(unique(round(mb$time)))
for (r in seq_along(resample_indices)) {
  subset_riskT[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                 model_names = "all",
                 input_type = "RiskAtT",
                 specific_time = append(10, ts[seq(51, length(ts), by = 50)]), # 36 time points
                 bootstrap_patient_ids = resample_indices[[r]])
}

# List for all models
selected_risk <- colnames(subset_riskT[[1]])[4:length(colnames(subset_riskT[[1]]))]

# Create lists
results_risk_tau_maxUT <- list()
results_risk_tau_maxUT_pe <- list()

# Loop
for (riskT in selected_risk) {
   cat("Calculating bootstrap at eval.time = ", median(ts), 
       "for Model = ", riskT, "\n")

   results_risk_tau_maxUT[[riskT]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = riskT,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("Hmisc::rcorr.cens",
                                                        "pysurvival",
                                                        "survC1::Est.Cval", 
                                                        "pec::cindex", 
                                                        "SurvMetrics::Cindex", 
                                                        "lifelines", 
                                                        "sksurv.censored", 
                                                        "survival.n", 
                                                        "survival.n/G2", 
                                                        "sksurv.ipcw"),
                                  eval.times = "resample_max_uncensored_time", 
                                  sampled_data = subset_riskT, 
                                  additional=list(
                                   sksurv_train_time = mb_train$test_time,
                                   sksurv_train_status = mb_train$test_status,
                                   sksurv_tied_tol = 1e-8))
   cat("Calculatin point estimates at eval.time max uncensored time by samplefor Model = ", riskT, "\n")

   results_risk_tau_maxUT_pe[[riskT]] <- metrics.wrapper(predicted = 1-stacked_predictions[[riskT]], 
                                                          censoring = stacked_predictions$test_status, 
                                                          time = stacked_predictions$test_time, 
                                                          implementation = list("Hmisc::rcorr.cens",
                                                                                "pysurvival",
                                                                                "survC1::Est.Cval",
                                                                                "pec::cindex",
                                                                                "SurvMetrics::Cindex",
                                                                                "lifelines",
                                                                                "sksurv.censored",
                                                                                "survival.n",
                                                                                "survival.n/G2", 
                                                                                "sksurv.ipcw"), 
                                                          eval.times = "resample_max_uncensored_time",
                                                          sksurv_train_time = mb_train$test_time,
                                                          sksurv_train_status = mb_train$test_status,
                                                          sksurv_tied_tol = 1e-8)

}
```

```{r Plot risk at max uncensored, fig.height=10, fig.width=8}

#risk_t_plot        <- make_risk_plot_entries(results_risk_t)
risk_tau_maxU_plot <- make_risk_plot_entries(results_risk_tau_maxUT, results_risk_tau_maxUT_pe)

ggplot(risk_tau_maxU_plot, aes(x = as.numeric(Time), y = cindex, color = Model, group = Model)) +
  geom_line(linewidth = 1) +
  geom_pointrange(aes(ymin = lower, ymax = upper), size = 0.3) +
  facet_wrap(~ Metric) +
  #ylim(0.5, 0.72) +
  theme_minimal(base_size = 12) +
  labs(x = "Time (t)", y = "C-index", color = "Implementation",
       title = "") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(risk_tau_maxU_plot[(risk_tau_maxU_plot$Metric == "Hmisc::rcorr.cens" | risk_tau_maxU_plot$Metric == "pec::cindex"),], aes(x = as.numeric(Time), y = cindex, color = Metric, group = Metric)) +
  geom_line(linewidth = 1, position = position_dodge(width = 0.3)) +
  geom_errorbar(
    position = position_dodge(width = 0.4),
    aes(ymin = lower, ymax = upper)
  ) +
  geom_point(position = position_dodge(0.4)) +
  facet_wrap(~ Model, ncol = 2) +
  ylim(0.5, 0.72) +
  theme_minimal(base_size = 12) +
  labs(x = "Time (t)", y = "C-index", color = "Implementation",
       title = "") +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```

#### Alternative transformations

RMST, Expected Mortality and Survival distribution as input for C-index. 

```{r Create plots}

plot_list <- list()
df_list <- list()
# Extract the plots
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  results_expm1 = hold_results[[fold_n]]$results_expm1
  results_expm2 = hold_results[[fold_n]]$results_expm2
  results_expm3 = hold_results[[fold_n]]$results_expm3
  results_risk1 = hold_results[[fold_n]]$results_risk1
  results_risk2 = hold_results[[fold_n]]$results_risk2
  results_risk3 = hold_results[[fold_n]]$results_risk3
  results_surv = hold_results[[fold_n]]$results_surv
  
  results_expm1_point_estim = hold_results[[fold_n]]$results_expm1_point_estim
  results_expm2_point_estim = hold_results[[fold_n]]$results_expm2_point_estim
  results_expm3_point_estim = hold_results[[fold_n]]$results_expm3_point_estim
  results_risk1_point_estim = hold_results[[fold_n]]$results_risk1_point_estim
  results_risk2_point_estim = hold_results[[fold_n]]$results_risk2_point_estim
  results_risk3_point_estim = hold_results[[fold_n]]$results_risk3_point_estim
  results_surv_point_estim = hold_results[[fold_n]]$results_surv_point_estim

  expm_df_plot1 <- make_expm_plot_entries(results_expm1, results_expm1_point_estim)
  expm_df_plot2 <- make_expm_plot_entries(results_expm2,  results_expm2_point_estim)
  expm_df_plot3 <- make_expm_plot_entries(results_expm3,  results_expm3_point_estim)
  risk_df_plot1 <- make_rmst_plot_entries(results_risk1,  results_risk1_point_estim)
  risk_df_plot2 <- make_rmst_plot_entries(results_risk2,  results_risk2_point_estim)
  risk_df_plot3 <- make_rmst_plot_entries(results_risk3,  results_risk3_point_estim)
  surv_df_plot  <- make_surv_plot_entries(results_surv,  results_surv_point_estim)

  expm_df_plot1$Notation <- "C"
  expm_df_plot2$Notation <- "C_tau"
  expm_df_plot3$Notation <- "C_tau_10"  
  risk_df_plot1$Notation <- "C_risk"
  risk_df_plot2$Notation <- "C_risk_tau"
  risk_df_plot3$Notation <- "C_risk_tau_10"  
  surv_df_plot$Notation  <- "C_td" 
  
  df_plot <- bind_rows(expm_df_plot1, risk_df_plot1, 
                       expm_df_plot2, risk_df_plot2, 
                       expm_df_plot3, risk_df_plot3,
                       surv_df_plot)
  
  df_plot$Metric <- sub("::.*", "", df_plot$Metric)
  
  df_plot$Notation <- factor(df_plot$Notation,
    levels = c("C", "C_risk", "C_tau_10", "C_risk_tau_10", "C_tau", "C_risk_tau", "C_td"),
    labels = c(
      "tilde(C)~(Expected~Mortality)",
      "tilde(C)~(RMST)",
      "tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
      "tilde(C)[tau]~(RMST~tau==10~years)",
      "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
      "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))",
      "tilde(C)[td]~(Survival~Distribution)"
    )
  )
  
  df_plot$Model[df_plot$Model == "CoxPH"] <- "CPH"
  #df_plot$Model <- factor(df_plot$Model, levels = c("DeepSurv", "CoxTime", "CoxPH", "RSF", "DeepHit"))
  df_plot$Model <- factor(df_plot$Model, levels = c("RSF", "CoxTime", 
                                                     "DeepSurv", "CPH", "DeepHit"))
  
  df_plot$Metric <- factor(df_plot$Metric, levels = unique(df_plot$Metric))
  # Get numeric positions of each Metric for vertical lines
  metric_levels <- levels(df_plot$Metric)
  n_metrics <- length(metric_levels)
  
  # Define where to put the vertical lines between metrics
  separator_positions <- seq(1.5, n_metrics - 0.5, by = 1)
  
  ### Version with color
  p <- ggplot(df_plot, aes(x = Metric, y = cindex, color = Model)) +
    geom_pointrange(aes(ymin = lower, ymax = upper),
                    position = position_dodge(width = 0.4), size = 0.5) +
    facet_wrap(~ Notation, nrow = 4, ncol=2, scales = "free_x", labeller = label_parsed)+
    ylim(0.5, 0.75) +
    geom_vline(xintercept = separator_positions, 
                linetype = "dashed", color = "grey70") +
     #geom_hline(linetype = "dashed", yintercept = 0.5) +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
    labs(#title = paste0("Fold ", fold_n),
         y = "C-index", x = NULL) +
    theme_minimal(base_size = 14) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank())
  
  plot_list[[paste0("fold_", fold_n)]] <- p
  
  # for aluvial plot
  df_plot$fold_n <- fold_n
  df_list[[fold_n]] <- df_plot

  
}

```

```{r plot by fold, fig.width=12, fig.height=15}
plot_list$fold_1 + theme(plot.title = element_blank())
plot_list$fold_2 + theme(plot.title = element_blank())
plot_list$fold_3 + theme(plot.title = element_blank())
plot_list$fold_4 + theme(plot.title = element_blank())
plot_list$fold_5 + theme(plot.title = element_blank())
```
```{r load 5-fold cross validation results}
### 5fold validation results:
results <- readRDS("./Results/results_5fold_facet_plots.rds")

results_risk1 = results$results_risk1
results_risk2 = results$results_risk2
results_risk3 = results$results_risk3
results_expm1 = results$results_expm1
results_expm2 = results$results_expm2
results_expm3 = results$results_expm3
results_surv  = results$results_surv


results_risk1_point_estim = results$results_risk1_point_estim
results_risk2_point_estim = results$results_risk2_point_estim
results_risk3_point_estim = results$results_risk3_point_estim
results_expm1_point_estim = results$results_expm1_point_estim
results_expm2_point_estim = results$results_expm2_point_estim
results_expm3_point_estim = results$results_expm3_point_estim
results_surv_point_estim = results$results_surv_point_estim


# Extract the plots
expm_df_plot1_5f <- make_expm_plot_entries(results_expm1,  results_expm1_point_estim) # C exp mort
expm_df_plot2_5f <- make_expm_plot_entries(results_expm2,  results_expm2_point_estim) # C tau exp mort
expm_df_plot3_5f <- make_expm_plot_entries(results_expm3,  results_expm3_point_estim) # C tau exp mort 10 ye
risk_df_plot1_5f <- make_rmst_plot_entries(results_risk1,  results_risk1_point_estim) # C RMST
risk_df_plot2_5f <- make_rmst_plot_entries(results_risk2,  results_risk2_point_estim) # C tau RMST
risk_df_plot3_5f <- make_rmst_plot_entries(results_risk3,  results_risk3_point_estim) # C tau RMST 10 ye
surv_df_plot_5f  <- make_rmst_plot_entries(results_surv,   results_surv_point_estim) # Surv

expm_df_plot1_5f$Notation <- "C"
expm_df_plot2_5f$Notation <- "C_tau"
expm_df_plot3_5f$Notation <- "C_tau_10"  
risk_df_plot1_5f$Notation <- "C_risk"
risk_df_plot2_5f$Notation <- "C_risk_tau"
risk_df_plot3_5f$Notation <- "C_risk_tau_10"  
surv_df_plot_5f$Notation  <- "C_td" 


df_plot <- bind_rows(expm_df_plot1_5f, risk_df_plot1_5f, 
                       expm_df_plot2_5f, risk_df_plot2_5f, 
                       expm_df_plot3_5f, risk_df_plot3_5f,
                       surv_df_plot_5f)
  
df_plot$Metric <- sub("::.*", "", df_plot$Metric)

df_plot$Notation <- factor(df_plot$Notation,
  levels = c("C", "C_risk", "C_tau_10", "C_risk_tau_10", "C_tau", "C_risk_tau", "C_td"),
  labels = c(
    "tilde(C)~(Expected~Mortality)",
    "tilde(C)~(RMST)",
    "tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
    "tilde(C)[tau]~(RMST~tau==10~years)",
    "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
    "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))",
    "tilde(C)[td]~(Survival~Distribution)"
  )
)

df_plot$Model[df_plot$Model == "CoxPH"] <- "CPH"
#df_plot$Model <- factor(df_plot$Model, levels = c("DeepSurv", "CoxTime", "CoxPH", "RSF", "DeepHit"))
df_plot$Model <- factor(df_plot$Model, levels = c("RSF", "CoxTime", 
                                                   "DeepSurv", "CPH", "DeepHit"))

df_plot$Metric <- factor(df_plot$Metric, levels = unique(df_plot$Metric))
# Get numeric positions of each Metric for vertical lines
metric_levels <- levels(df_plot$Metric)
n_metrics <- length(metric_levels)

df_plot$fold_n <- "5CV"

```

We bring the results from 5fol-cross validation, so that we can build the Alluvial plot:

```{r Alluvial ranking plots, fig.width=13, fig.height=16}
# Row binf the hold out
final_df_plot <- do.call(rbind, df_list)

# add the 5fcv results:
final_df_plot$fold_n <- as.character(final_df_plot$fold_n)
final_df_plot <- bind_rows(final_df_plot, df_plot)

model_colors <- c(
  "RSF" = "#FBB4AE",      
  "CoxTime" = "#FFFACD", 
  "DeepSurv" = "#CCEBC5", 
  "CPH" = "#B3CDE3",     
  "DeepHit" = "#DECBE4"   
)

theme_nolegend_noxtitle <- theme(
  legend.position = "none",
  axis.title.x = element_blank(),
  plot.title = element_text(hjust = 0.5),
  axis.text.y = element_blank(),
  panel.grid.major.y = element_blank()
)

p1 <- make_alluvial_plot(final_df_plot, "Hmisc", 
                         "tilde(C)~(Expected~Mortality)", 
                         "Hmisc~tilde(C)~(Expected~Mortality)", 
                         custom_colors = model_colors) +
  theme_nolegend_noxtitle
p2 <- make_alluvial_plot(final_df_plot, "Hmisc", 
                         "tilde(C)~(RMST)", 
                         "Hmisc~tilde(C)~(RMST)",
                          custom_colors = model_colors) + theme_nolegend_noxtitle

p3 <- make_alluvial_plot(final_df_plot, "pec", 
                         "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))", 
                         "pec~tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
                          custom_colors = model_colors) + theme_nolegend_noxtitle

p4 <- make_alluvial_plot(final_df_plot, "pec", 
                         "tilde(C)[tau]~(Expected~Mortality~tau==10~years)", 
                         "pec~tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
                          custom_colors = model_colors) + theme_nolegend_noxtitle

p5 <- make_alluvial_plot(final_df_plot, "pec", 
                         "tilde(C)[tau]~(RMST~tau==10~years)", 
                         "pec~tilde(C)[tau]~(RMST~tau==10~years)",
                          custom_colors = model_colors) + theme_nolegend_noxtitle

p6 <- make_alluvial_plot(final_df_plot, "pec", 
                         "tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))",
                         "pec~tilde(C)[tau]~(RMST~tau==max*(T:~Delta==1))",
                          custom_colors = model_colors) + theme_nolegend_noxtitle
p7 <- make_alluvial_plot(final_df_plot, "pycox.Ant", 
                         "tilde(C)[td]~(Survival~Distribution)",
                         "pycox.Ant~tilde(C)[td]~(Survival~Distribution)",
                          custom_colors = model_colors) 

combined_plot <- (p1 | p2)  / (p4 | p5) / (p3 | p6) / (p7 | patchwork::plot_spacer()) + plot_layout(guides = "collect") & theme(legend.position = "bottom")

final_plot <- ggdraw() +
  draw_plot(combined_plot, x = 0.09, y = 0, width = 0.94, height = 1) +
  draw_label("Model rank (higher = better)", 
             angle = 90, x = 0.1, y = 0.5, 
             vjust = 0.5, hjust = 0.5, size = 17) 
final_plot
```



#### Problems with Expected mortality

Expected mortality has instabilities in the end of the distribution due to the the logaritmic transformation: 

```{r,  fig.width=10, fig.height=10}
#set.seed(20)

model <- "Coxtime"
# Extract random patient curve
#set.seed(10)
#patient_index1 <- sample(nrow(survival_curves[[2]][[model]]), 1)

#set.seed(60)
#patient_index2 <- sample(nrow(survival_curves[[2]][[model]]), 1)

#CurveDeepSurv1 <- survival_curves[[1]]$DeepSurv[patient_index1, ]
#CurveDeepSurv2 <- survival_curves[[1]]$DeepSurv[patient_index2, ]

Curve1 <- survival_curves[[2]][[model]]["MB-0035", ]
Curve2 <- survival_curves[[2]][[model]]["MB-5401", ]


# Cumulative expected mortality (cumulative sum of -log(S(t)))
cumulative_expected_mort <- function(pred) {
  cumsum(-log(as.numeric(pred)))
}

# Cumulative AUSC (cumulative sum of -S(t))
cumulative_ausc <- function(pred) {
  cumsum(-as.numeric(pred))
}

# Create a named list of curves
curve_list <- list(
  Patient1 = Curve1,
  Patient2 = Curve2
)

df_list <- list()

for (patient in names(curve_list)) {
  curve <- curve_list[[patient]]
  df_list[[patient]] <- data.frame(
    time = as.numeric(names(curve)),
    survival = as.numeric(curve),
    patient = patient
  )
}

# Combine into one data frame
curve_long <- do.call(rbind, df_list)


p1 <- ggplot(curve_long, aes(x = time, y = survival, color = patient)) +
  geom_line(size = 1.2) +
  labs(title = paste0("Survival Curves with ", model),
       x = "Time", y = "Survival Probability") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
# Apply cumulative scoring functions
cum_em_df <- do.call(cbind, lapply(curve_list, cumulative_expected_mort))
cum_ausc_df <- do.call(cbind, lapply(curve_list, cumulative_ausc))

cum_em_long <- as.data.frame(cum_em_df) %>%
  mutate(time = 1:nrow(.)) %>%
  pivot_longer(-time, names_to = "patient", values_to = "CumulativeEM")


cum_ausc_long <- as.data.frame(cum_ausc_df) %>%
  mutate(time = 1:nrow(.)) %>%
  pivot_longer(-time, names_to = "patient", values_to = "CumulativeAUSC")


p2 <- ggplot(cum_em_long, aes(x = time, y = CumulativeEM, color = patient)) +
  geom_line(size = 1.2) +
  labs(title = "Cumulative Expected Mortality", x = "Time Index", y = expression(-sum(log(S(t))))) +
  theme_minimal(base_size = 14)

p3 <- ggplot(cum_ausc_long, aes(x = time, y = CumulativeAUSC, color = patient)) +
  geom_line(size = 1.2) +
  labs(title = "Cumulative RMST", x = "Time Index", y = expression(-sum(S(t)))) +
  theme_minimal(base_size = 14)


p1 / p2 / p3
```





