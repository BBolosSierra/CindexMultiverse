---
title: "HoldOutMetabric"
output: html_document
date: "2025-04-20"
---

# Concordance index multiverse - METABRIC analysis (hold-out)

## Setup the environment

Load libraries

```{r, include=FALSE}
# Survival metrics
library(reticulate)
library(arrow)
library(caret)
library(riskRegression)
library(prodlim)
library(pec)
library(survival)
library(rhdf5)
library(randomForestSRC)
library(survAUC)
library(Hmisc)
library(dplyr)
# Plotting
library(gridExtra)
# Parallelization
library(doFuture)
library(future)
library(progressr)
library(foreach)
library(MASS)
library(flexsurv)
library(furrr)
# install.packages("./pysurvivalR.tar.gz", repos = NULL, type = "source")
library(pysurvivalR)
library(survivalmodels)
library(mlr3)
```


```{r setup, include=FALSE}
#Sys.unsetenv("RETICULATE_PYTHON")
Sys.setenv(OMP_NUM_THREADS = "1")       # Limits OpenMP to 1 thread
Sys.setenv(NUMBA_NUM_THREADS = "1")     # Limits Numba to 1 thread
Sys.setenv(MKL_NUM_THREADS = "1")       # Limits Intel MKL to 1 thread
Sys.setenv(KMP_WARNINGS = "0")          # Disables OpenMP warnings
Sys.setenv(OPENBLAS_NUM_THREADS = "1")  # Limits OpenBLAS to 1 thread

# #Sys.setenv(NUMBA_DISABLE_JIT = "0")  
# library(reticulate)

#use_condaenv("/opt/homebrew/Caskroom/miniforge/base/envs/py-rstudio", required=TRUE)
#use_virtualenv("~/.virtualenvs/venv-DeSurv_python_R")
#use_python("/opt/homebrew/Caskroom/miniforge/base/envs/venv-DeSurv3/bin/python3.9")
#py_config()
```

We also load a set of helper functions used throuhgout our analysis:

```{r}
source("./CindexHelperFunctions.R")
```

## Load the data

Here, we load the pre-processed METABRIC data that was generated using the 
`preprocessing_biocportal_metabric.R` script.

```{r}
# read the processed file
mb <- read.table("./Datasets/metabric_preprocess_multiverse.csv", 
                 sep = "," , header = TRUE)

# Keep the id as index
rownames(mb) <- mb$PATIENT_ID

# Remove the id column
mb <- mb[,2:12]

# Store the previous colnames to keep track of them
cols <- colnames(mb)

# Assign new column names
newcols <- c("X1", "X2", "X3", "X4","X5", "X6", "X7", "X8", "X9", "time", "status")
colnames(mb) <- newcols

# Summary statistics for the survival times
summary(mb$time)
```

The data contains information about `r nrow(mb)` individuals, with 
`r sum(mb$status == 1)` events and `r sum(mb$status == 0)` censored 
observations. The survival times are measured in months.

## Evaluation of predictive performance using a data holdout approach

Here, we will evaluate the predictive performance of various survival models 
using a holdout approach. We will split the data into a training set (80%) and a 
test set (20%), fit the models on the training set, and then evaluate their 
performance on the test set. We repeated this 5 times to ensure robustness of 
the results. The data splits used here match the ones used in the 5-fold 
cross-validation results. For each data split, bootstrap samples of the test set
will be generated to estimate the uncertainty of the estimated C-index values. 

### Model hyperparameters

For the hyperparameters of deep learning models that have been trained with 
`survivalmodels` package is relevant to look at the pytorch optimizer: https://raphaels1.github.io/survivalmodels/reference/get_pycox_optim.html 

Also the parameters for each model:

1) Deephit: https://raphaels1.github.io/survivalmodels/reference/deephit.html
2) DeepSurv: https://raphaels1.github.io/survivalmodels/reference/deepsurv.html
3) CoxTime: https://raphaels1.github.io/survivalmodels/reference/coxtime.html

In our analysis, we consider the same hyperparameter values used when analysing
the METABRIC in the original publications for DeepHit, DeepSurv, and CoxTime. 
For RSF, we use the default hyperparameters from the `randomForestSRC` package.

[CAV: Bego is this correct?]

\begin{table}[ht]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Hyper-parameter}     & \textbf{DeepSurv} & \textbf{CoxTime} & \textbf{DeepHit}  & RSF \\ \midrule
Optimizer                    & adam              & adam             & adam              & NA \\
Activation                   & selu              & relu             & relu              & NA\\
\# Dense Layers              & 1                 & 2                & 2                 & - \\
\# Nodes / Layer             & 41                & 32, 32           & 32, 32            & 100 trees\\
Learning Rate                & 0.0103            & 0.01             & 0.001             & NA \\
L2 Reg (weight\_decay)       & 0                 & 0                & 0                 & NA \\
Dropout                      & 0.1601            & 0.1              & 0.6               & NA \\
LR Decay                     & 0.00417           & 0                & 0                 & NA \\
Batch Norm                   & True              & True             & True              & NA \\
Batch Size                   & 256               & 256              & 50                & NA \\
Epochs                       & 500               & 512              & 100               & NA \\
Early Stopping               & False             & True             & True              & NA \\
mod\_alpha                   & NA                & NA               & 0.2               & NA \\
sigma                        & NA                & NA               & 0.1               & NA \\
cuts                         & NA                & NA               & 300               & NA \\ \bottomrule
\end{tabular}
\caption{Hyperparameters across DeepSurv, CoxTime, and DeepHit models.}
\label{tab:hyperparams_comparison}
\end{table}

### Model training


```{r, eval=FALSE}

# We could keep the same folds and indices since the dataframes are the same
# Number of folds
#K <- 5 
n <- nrow(mb)  
# R <- 1
# Interesting times for comparison
all_predictions <- list()
survival_curves <- list()
#numeric_cols <- c("X1", "X2", "X3", "X4","X5", "X6", "X7", "X8", "X9")
numeric_cols <- c("X1", "X2", "X3", "X4", "X9")
#seed <- sample(1:1e6, 1)
#set.seed(seed)


#reticulate::py_run_string("import torch; torch.manual_seed(42)")

reset_torch_seed <- function(seed = 123) {
  reticulate::py_run_string(sprintf("
import torch
import numpy as np
import random
torch.manual_seed(%d)
np.random.seed(%d)
random.seed(%d)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
", seed, seed, seed))
}

set.seed(123)
survivalmodels::set_seed(seed_R=123, seed_np = 123, seed_torch = 123) ## important for reproducibility
# If we do not need to preserve the censoring/event proportions per fold iteration
#indices <- sample(seq_len(n))
#folds <- cut(indices, breaks = K, labels = FALSE)

#survival_curves[[r]] <- vector("list", K)

train_idx <- sample(1:n, size = floor(0.8 * n))
mb_train  <- mb[train_idx, ]
mb_test   <- mb[-train_idx,]

t_train_max <- max(mb$time)
# Scale continuous 
means <- sapply(mb_train[, numeric_cols], mean)
sds   <- sapply(mb_train[, numeric_cols], sd)

mb_train[, numeric_cols] <- scale(mb_train[, numeric_cols], center = means, scale = sds)
mb_test[, numeric_cols]  <- scale(mb_test[, numeric_cols], center = means, scale = sds)

#t_train_max <- max(mb_train$time)
mb_train$time <- mb_train$time / t_train_max
mb_test$time <- mb_test$time / t_train_max

ts <- sort(unique(mb_test$time))
# Extract only the covariates used for predictions
test_covariates <- mb_test[, c("X1", "X2", "X3", "X4",
                               "X5", "X6", "X7", "X8", "X9")]
# intervals for prediction with scaling
ts_scaled <- seq(0, round(t_train_max), 1)
approx_seq <- ts_scaled / t_train_max

# Fit models
#cat("Dataset:", i, ".", j, '\n')
#cat("Seed: ", seed, '\n')
#cat("Repetition ", r, '\n')
cat('Train set dimensions:', dim(mb_train), '\n')
cat('Train set event:', mean(mb_train$status == 1)*100, '\n')
cat('Train set censoring:', mean(mb_train$status == 0)*100, '\n')
cat('Test set dimensions:', dim(mb_test), '\n')
cat('Train set event:', mean(mb_test$status == 1)*100, '\n')
cat('Test set censoring:', mean(mb_test$status == 0)*100, '\n')
cat('\n')


# 1) Random Forest:
# Fit the model
rf <- randomForestSRC::rfsrc(Surv(time, status) ~ X1 + X2 +
                               X3 + X4 + X5 + X6 + 
                               X7 + X8 + X9, 
                             data = mb_train, ntree = 100)
# Predict survival object
surv_rf_object <- predict(rf, newdata = mb_test, type = "surv")
surv_rf <- surv_rf_object$survival
time_grid <- surv_rf_object$time.interest# * t_train_max
# Interpolate to have same times in all survival curves
surv_rf_int <- apply(surv_rf, 1, function(s){
  approx(x = time_grid, y=s, xout = approx_seq, rule = 2)$y
})
surv_rf_int <- t(surv_rf_int)
rownames(surv_rf_int) <- rownames(mb_test)
colnames(surv_rf_int) <- ts_scaled
# Calculate expected mortality
rf_exp_mort <- rowSums(-log(pmax(surv_rf_int, 1e-10)))


# 2) Predict risk Cox PH
# Fit with cox ph 
cox_ph <- survival::coxph(Surv(time, status) ~ X1 + X2 + 
                           X3 + X4 + X5 + X6 + 
                           X7 + X8 + X9, 
                             data = mb_train)

# Predict survival object
sf <- survfit(cox_ph, newdata = mb_test)
surv_cox <- t(sf$surv)
time_grid <- sf$time #* t_train_max
#Interpolate
surv_cox_int <- apply(surv_cox, 1, function(s){
   approx(x = time_grid, y=s, xout = approx_seq, rule = 2)$y
})

surv_cox_int <- t(surv_cox_int)
rownames(surv_cox_int) <- rownames(mb_test)
colnames(surv_cox_int)  <- ts_scaled
# Calculate expected mortality
cox_exp_mort <- rowSums(-log(pmax(surv_cox_int, 1e-10)))

reset_torch_seed()

# 3) DeepSurv
# Hyperparameters: https://github.com/jaredleekatzman/DeepSurv/blob/41eed003e5b892c81e7855e400861fa7a2d9da4f/experiments/deepsurv/models/metabric_IHC4_clinical_adam_0.json
deepsurv_model <- survivalmodels::deepsurv(Surv(time, status) ~ 
                                             X1 + X2 + 
                                             X3 + X4 + X5 + X6 + 
                                             X7 + X8 + X9, 
                                           data = mb_train, 
                                           frac = 0.2, 
                                           dropout = 0.160087890625,
                                           optimizer = "adam",
                                           activation = "selu", 
                                           batch_norm = TRUE,
                                           num_nodes = c(41), 
                                           #verbose = TRUE,
                                           batch_size = 256L, # ? default, nowhere to be found
                                           learning_rate = 0.010289691253027908, 
                                           lr_decay = 0.0041685546875, 
                                           momentum =  0.8439658203125,# if adam this is not used, is used for sgd
                                           #weight_decay = 1.269e-3, # ? weight_decay > 0 for L2 regularization
                                           #weight_decay = 10.890986328125, # this is too high it cannot be 
                                           epochs = 500) 
# Predict survival object
surv_deepsurv <- predict(deepsurv_model,
                         newdata = mb_test, type = "surv")
time_grid <- as.numeric(colnames(surv_deepsurv))# * t_train_max

# Interpolate
surv_deepsurv_int <- apply(surv_deepsurv, 1, function(s){
  approx(x = time_grid, y=s, xout = approx_seq, rule = 2)$y
})
surv_deepsurv_int <- t(surv_deepsurv_int)
rownames(surv_deepsurv_int) <- rownames(mb_test)
colnames(surv_deepsurv_int) <- ts_scaled
# Calculate expected mortality
deepsurv_exp_mort <- rowSums(-log(pmax(surv_deepsurv_int, 1e-10)))

reset_torch_seed()

# 4) Coxtime
# Hyperparameters from:
# https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/cox-time.ipynb
coxtime_model <- survivalmodels::coxtime(Surv(time, status) ~ 
                                           X1 + X2 + X3 + 
                                           X4 + X5 + X6 + 
                                            X7 + X8 + X9, 
                                         data = mb_train, 
                                         optimizer = "adam",
                                         learning_rate = 0.01,       
                                         betas = c(0.9, 0.999), # defaults
                                         activation = "relu",
                                         num_nodes = c(32L, 32L),
                                         batch_norm = TRUE,
                                         dropout = 0.1,
                                         batch_size = 256,
                                         epochs = 512,
                                         early_stopping = TRUE, 
                                         frac = 0.2)

# Predict survival object
surv_coxtime <- predict(coxtime_model,
                        newdata = mb_test, type = "surv")
time_grid <-  as.numeric(colnames(surv_coxtime))# * t_train_max
# Interpolate
surv_coxtime_int <- apply(surv_coxtime, 1, function(s){
  approx(x = time_grid, y=s, xout = approx_seq, rule = 2)$y
})
surv_coxtime_int <- t(surv_coxtime_int)
rownames(surv_coxtime_int) <- rownames(mb_test)
colnames(surv_coxtime_int) <- ts_scaled
# Calculate expected mortality
coxtime_exp_mort <- rowSums(-log(pmax(surv_coxtime_int, 1e-10)))


# scale back the train time too 
mb_train$time <- mb_train$time * t_train_max
mb_test$time <- mb_test$time * t_train_max

reset_torch_seed()

# 5) Deephit
# Original scale for times applied 
# Hyperparameters: https://github.com/havakv/pycox/blob/3eccdd7fd9844a060f50fdcc315659f33a2d2dc1/examples/deephit.ipynb and DeepHit paper, and also double checked with DeSurv paper (some parameters are slightly different)
deephit_model <- survivalmodels::deephit(Surv(time, status) ~ 
                           X1 + X2 + 
                           X3 + X4 + X5 + X6 + 
                           X7 + X8 + X9, 
                           data = mb_train, 
                           optimizer = "adam",
                           activation = "relu", # in DeSurv, ipynb
                           num_nodes = c(32L, 32L), # in ipynb and DeSurv
                           batch_norm = TRUE, # in ipynb
                           dropout = 0.6, #  0.1 in ipynb and 0.6 in DeepHit paper
                           cuts = 300, # in DeSurv and DeepHit, 10 and interpolation in ipynb
                           mod_alpha = 0.2, # in DeSurv, ipynb
                           early_stopping = TRUE, # in DeepHit and ipynb
                           #tolerance = 3,
                           sigma = 0.1, # in DeSurv and ipynb
                           batch_size = 50, # 50 in Deephit, 256 in ipynb
                           epochs = 100, # in ipynb
                           #verbose = TRUE,
                           learning_rate = 0.001, # 0.001 in Deephit, 0.01 in ipynb
                           #betas = c(0.9, 0.999), # default adam
                           frac = 0.2) # validation 
# Survival object
surv_deephit <- predict(deephit_model,
                        newdata = mb_test, type = "surv")
time_grid <- as.numeric(colnames(surv_deephit))# * t_train_max
# Interpolate
surv_deephit_int <- apply(surv_deephit, 1, function(s){
  approx(x = time_grid, y=s, xout = ts_scaled, rule = 2)$y
})
surv_deephit_int <- t(surv_deephit_int)
rownames(surv_deephit_int) <- rownames(mb_test)
colnames(surv_deephit_int) <- ts_scaled
# Calculate expected mortality
deephit_exp_mort <- rowSums(-log(pmax(surv_deephit_int, 1e-10)))


## Gather the survival curves for plotting
survival_curves <- list(patients_ids = rownames(mb_test),
                               test_time = mb_test$time,
                               test_status = mb_test$status,
                               RSF = as.data.frame(surv_rf_int),
                               CoxPH = as.data.frame(surv_cox_int),
                               DeepHit = as.data.frame(surv_deephit_int),
                               DeepSurv = as.data.frame(surv_deepsurv_int),
                               Coxtime = as.data.frame(surv_coxtime_int),
                               covariates = test_covariates)

# Store the results:
fold_results <- data.frame(ho_repeat = "DeepSurvSplits",
                           #cv_fold = k,
                           patients_ids = rownames(mb_test), 
                           test_time = mb_test$time,
                           test_status = mb_test$status,
                           ExpMort.RSF = rf_exp_mort,
                           ExpMort.CoxPH = cox_exp_mort,
                           ExpMort.DeepHit = deephit_exp_mort,
                           ExpMort.DeepSurv = deepsurv_exp_mort,
                           ExpMort.CoxTime = coxtime_exp_mort,
                           RSF = as.data.frame(surv_rf_int),
                           CoxPH = as.data.frame(surv_cox_int),
                           DeepHit = as.data.frame(surv_deephit_int), 
                           DeepSurv = as.data.frame(surv_deepsurv_int),
                           CoxTime = as.data.frame(surv_coxtime_int), 
                           covariates = test_covariates)
 

# Append fold results to the list
all_predictions[[paste("Split")]] <- fold_results


stacked_predictions <- do.call(rbind, all_predictions)

```

```{r, fig.height=8, fig.width=13}

fit <- prodlim(Hist(time, status) ~ 1, data = mb_train)

plot(fit,
     xlab = "Time (months)",
     ylab = "Censoring probability",
     confint = TRUE,
     legend = FALSE,
     col = "black",
     lwd = 2)

tmp <- mb_train
tmp$censor.status <- ifelse(tmp$status %in% c(1, 2), 1, 0)

tmp <- tmp[order(tmp$time),]

weight.i <- pec::ipcw(formula=Surv(time,censor.status)~1,
                 data=tmp,
                 method="marginal",
                 times=unique(tmp$time),
                 subjectTimes=tmp$time, 
                 what = "IPCW.subjectTimes")$IPCW.subjectTimes
weight.j <- pec::ipcw(formula=Surv(time,censor.status)~1,
                 data=tmp,
                 method="marginal",
                 times=unique(tmp$time),
                 subjectTimes=tmp$time,
                 subjectTimesLag=0,
                 what="IPCW.times")$IPCW.times


time_points <- unique(tmp$time)

# Compute G2
G2 <- (1 / weight.j)^2

# Plot G2 vs time
plot(time_points, G2,
     type = "l",
     col = "black",
     lwd = 2,
     xlab = "Time",
     ylab = expression(G[2](t) == (1 / w[t])^2),
     main = expression("Variation of " ~ G[2](t) ~ "over time"))

# Compute G2
G2 <- (1 / weight.j)^2

# Plot G2 vs time
plot(time_points, log(G2),
     type = "l",
     col = "black",
     lwd = 2,
     xlab = "Time",
     ylab = expression(log(G[2](t) == (1 / w[t])^2)),
     main = expression("Variation of " ~ log(G[2](t)) ~ "over time"))

```
```{r, fig.height=8, fig.width=13}

fit <- prodlim(Hist(time, status) ~ 1, data = mb_test)

plot(fit,
     xlab = "Time (months)",
     ylab = "Censoring probability",
     confint = TRUE,
     legend = FALSE,
     col = "black",
     lwd = 2)


tmp <- mb_test
tmp$censor.status <- ifelse(tmp$status %in% c(1, 2), 1, 0)

tmp <- tmp[order(tmp$time),]

weight.i <- pec::ipcw(formula=Surv(time,censor.status)~1,
                 data=tmp,
                 method="marginal",
                 times=unique(tmp$time),
                 subjectTimes=tmp$time, 
                 what = "IPCW.subjectTimes")$IPCW.subjectTimes
weight.j <- pec::ipcw(formula=Surv(time,censor.status)~1,
                 data=tmp,
                 method="marginal",
                 times=unique(tmp$time),
                 subjectTimes=tmp$time,
                 subjectTimesLag=0,
                 what="IPCW.times")$IPCW.times


time_points <- unique(tmp$time)

# Compute G2
G2 <- (1 / weight.j)^2

# Plot G2 vs time
plot(time_points, G2,
     type = "l",
     col = "black",
     lwd = 2,
     xlab = "Time",
     ylab = expression(G[2](t) == (1 / w[t])^2),
     main = expression("Variation of " ~ G[2](t) ~ "over time"))

# Compute G2
G2 <- (1 / weight.j)^2

# Plot G2 vs time
plot(time_points, log(G2),
     type = "l",
     col = "black",
     lwd = 2,
     xlab = "Time",
     ylab = expression(log(G[2](t) == (1 / w[t])^2)),
     main = expression("Variation of " ~ log(G[2](t)) ~ "over time"))
```

```{r}
#plot_survival_curves(surv_deepsurv, title = "DeepSurv Survival Curves")
```

```{r}

# Plot 
plot_survival_curves(survival_curves$DeepHit, title = "DeepHit Survival Curves")
plot_survival_curves(survival_curves$DeepSurv, title = "DeepSurv Survival Curves")
plot_survival_curves(survival_curves$CoxPH, title = "Cox PH Survival Curves")
plot_survival_curves(survival_curves$Coxtime, title = "Cox Time Survival Curves")
plot_survival_curves(survival_curves$RSF, title = "RSF Survival Curves")
```


Bootstrap 100 times the test set that is stacked_predictions, the size of the bootstrapped sample is equal to the test set. 

```{r}
folds_stacked_predictions <- readRDS("./Results/5foldCV_MetabricStackedPredictions.rds")
```


```{r}
set.seed(123)

stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == 1, ] 

mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != 1, ] 

n_bootstraps <- 100

dataset_size <- nrow(stacked_predictions) 

# re sample by patient id
resample_indices <- replicate(n_bootstraps, 
                              sample(stacked_predictions$patients_ids, 
                                     size = dataset_size, replace = TRUE), 
                              simplify = FALSE)
subset_expm <- list()
subset_surv <- list()

for (r in seq_along(resample_indices)) {
  subset_expm[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                 model_names = "all",
                 input_type = "ExpectedMortality",
                 bootstrap_patient_ids = resample_indices[[r]])
  subset_surv[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                model_names = "all",
                input_type = "Distribution",
                bootstrap_patient_ids = resample_indices[[r]])
  
  
}
  
select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
results_expm2 <- list()
for (exps in select_exps) {
   cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
   results_expm2[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                  dataset=list(
                                     predicted = exps,
                                     censoring = "test_status",
                                     time = "test_time"),
                                  implementation = list("pec::cindex",
                                                        "survC1::Est.Cval", 
                                                        "survival.n", 
                                                        "survival.n/G2",
                                                        "sksurv.ipcw"),
                                  eval.times = round(max(mb$time[mb$status == 1])), # C tau.
                                  sampled_data = subset_expm, 
                                  additional=list(
                              sksurv_train_time = mb_train$test_time, 
                              sksurv_train_status = mb_train$test_status, 
                              sksurv_tied_tol = 1e-8))
}


```
```{r}
point_estim <- vector("list", 5)

#for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == 1, ] 

predicted <- stacked_predictions$ExpMort.CoxPH
censoring <-  stacked_predictions$test_status
time <- stacked_predictions$test_time

implementation <- "pec::cindex"
eval.times <- 120


metrics.wrapper(predicted = predicted, censoring = censoring, 
                time = time, implementation = implementation, eval.times = eval.times)


```
```{r}

stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == 1, ] 

model_names <- "RSF"
results_surv <- list()
results_surv_point_estim <- list()
for (model in model_names) {
  # Antolinis

  surv_mat = stacked_predictions[, grep(paste0("^", model), names(stacked_predictions), 
                                        value = TRUE), drop = FALSE]
  colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
  
  results_surv_point_estim[[model]] <- metrics.wrapper(surv_matrix = surv_mat, 
                                                    censoring = stacked_predictions$test_status, 
                                                    time = stacked_predictions$test_time, 
                                                    implementation = list("pycox.Ant", "pycox.Adj.Ant"))
}
```

```{r}
max(folds_stacked_predictions[(folds_stacked_predictions$cv_fold == 1 & folds_stacked_predictions$test_status == 1), ]$test_time)
max(folds_stacked_predictions[(folds_stacked_predictions$cv_fold == 2 & folds_stacked_predictions$test_status == 1), ]$test_time)
max(folds_stacked_predictions[(folds_stacked_predictions$cv_fold == 3 & folds_stacked_predictions$test_status == 1), ]$test_time)
max(folds_stacked_predictions[(folds_stacked_predictions$cv_fold == 4 & folds_stacked_predictions$test_status == 1), ]$test_time)
max(folds_stacked_predictions[(folds_stacked_predictions$cv_fold == 5 & folds_stacked_predictions$test_status == 1), ]$test_time)
```

```{r}
max(folds_stacked_predictions[folds_stacked_predictions$cv_fold == 1, ]$test_time)
max(folds_stacked_predictions[folds_stacked_predictions$cv_fold == 2, ]$test_time)
max(folds_stacked_predictions[folds_stacked_predictions$cv_fold == 3, ]$test_time)
max(folds_stacked_predictions[folds_stacked_predictions$cv_fold == 4, ]$test_time)
max(folds_stacked_predictions[folds_stacked_predictions$cv_fold == 5, ]$test_time)
```



```{r}
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ] 
  mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ] 
  print(max(stacked_predictions$test_time))
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1]) # C tau.
  print(max_uncensored_time)
}
```

```{r}

hold_results <- vector("list", 5) 
set.seed(123)

for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  stacked_predictions <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ] 
  mb_train <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ] 
  
  max_uncensored_time = max(stacked_predictions$test_time[stacked_predictions$test_status == 1])
  n_bootstraps <- 100
  
  dataset_size <- nrow(stacked_predictions) 
  
  # re sample by patient id
  resample_indices <- replicate(n_bootstraps, 
                                sample(stacked_predictions$patients_ids, 
                                       size = dataset_size, replace = TRUE), 
                                simplify = FALSE)
  
  subset_expm <- list()
  subset_surv <- list()
  
  for (r in seq_along(resample_indices)) {
    subset_expm[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                   model_names = "all",
                   input_type = "ExpectedMortality",
                   bootstrap_patient_ids = resample_indices[[r]])
    subset_surv[[r]] <- get_model_preds2(stacked_predictions = stacked_predictions, 
                  model_names = "all",
                  input_type = "Distribution",
                  bootstrap_patient_ids = resample_indices[[r]])
    
    
  }

  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm1 <- list()
  results_expm1_point_estim <- list()
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
     results_expm1[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex", 
                                                          "lifelines", 
                                                          "sksurv.censored"),
                                    eval.times = NULL, #C
                                    sampled_data = subset_expm)
     
     results_expm1_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                          censoring = stacked_predictions$test_status, 
                                                          time = stacked_predictions$test_time, 
                                                          implementation = list("Hmisc::rcorr.cens",
                                                          "pysurvival",
                                                          "SurvMetrics::Cindex", 
                                                          "lifelines", 
                                                          "sksurv.censored"), 
                                                          eval.times = NULL)
     
  }
  
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm2 <- list()
  results_expm2_point_estim <- list()
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
     results_expm2[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval", 
                                                          "survival.n", 
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    #eval.times = round(max(mb$time[mb$status == 1])), # C tau.
                                    #eval.times = 298,
                                    eval.times = max_uncensored_time,
                                    sampled_data = subset_expm, 
                                    additional=list(
                                sksurv_train_time = mb_train$test_time, 
                                sksurv_train_status = mb_train$test_status, 
                                sksurv_tied_tol = 1e-8))
      results_expm2_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                      censoring = stacked_predictions$test_status, 
                                                      time = stacked_predictions$test_time, 
                                                      implementation = list("pec::cindex",
                                                          "survC1::Est.Cval", 
                                                          "survival.n", 
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"), 
                                                      #eval.times =  round(max(mb$time[mb$status == 1])),
                                                      eval.times =  max_uncensored_time,
                                                      sksurv_train_time = mb_train$test_time,
                                                      sksurv_train_status = mb_train$test_status,
                                                      sksurv_tied_tol = 1e-8)
  }
  
  model_names <- unique(sub("\\..*", "", grep("^[A-Za-z]+\\.\\d+$",
                                              colnames(subset_surv[[1]]), value = TRUE)))
  results_surv <- list()
  results_surv_point_estim <- list()
  for (model in model_names) {
    # Antolinis
    cat("Calculating bootstrap for Distribution for Model = ", model, "\n")
    results_surv[[model]] <-  bootstrap.metric(metrics.wrapper,
                                    dataset=list(
                                       predicted = model,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pycox.Ant", "pycox.Adj.Ant"),
                                    sampled_data = subset_surv)
    surv_mat = stacked_predictions[, grep(paste0("^", model), names(stacked_predictions), 
                                          value = TRUE), drop = FALSE]
    colnames(surv_mat) <- as.numeric(sub(".*\\.", "", colnames(surv_mat)))
    results_surv_point_estim[[model]] <- metrics.wrapper(surv_matrix = surv_mat, 
                                                      censoring = stacked_predictions$test_status, 
                                                      time = stacked_predictions$test_time, 
                                                      implementation = list("pycox.Ant",
                                                                            "pycox.Adj.Ant"))
  }
  
  select_exps <- grep("ExpMort\\.",colnames(subset_expm[[1]]), value = TRUE)
  results_expm3 <- list()
  results_expm3_point_estim <- list()
  for (exps in select_exps) {
     cat("Calculating bootstrap for ExpMort for Model = ", exps, "\n")
     results_expm3[[exps]] <- bootstrap.metric.parallel(metrics.wrapper,
                                    dataset=list(
                                       predicted = exps,
                                       censoring = "test_status",
                                       time = "test_time"),
                                    implementation = list("pec::cindex",
                                                          "survC1::Est.Cval", 
                                                          "survival.n", 
                                                          "survival.n/G2",
                                                          "sksurv.ipcw"),
                                    eval.times = 120, #C
                                    sampled_data = subset_expm, 
                                  additional=list(
                                sksurv_train_time = mb_train$test_time, 
                                sksurv_train_status = mb_train$test_status, 
                                sksurv_tied_tol = 1e-8))
     results_expm3_point_estim[[exps]] <- metrics.wrapper(predicted = stacked_predictions[[exps]], 
                                                censoring = stacked_predictions$test_status, 
                                                time = stacked_predictions$test_time, 
                                                implementation = list("pec::cindex",
                                                    "survC1::Est.Cval", 
                                                    "survival.n", 
                                                    "survival.n/G2",
                                                    "sksurv.ipcw"), 
                                                eval.times =  120,
                                                sksurv_train_time = mb_train$test_time,
                                                sksurv_train_status = mb_train$test_status,
                                                sksurv_tied_tol = 1e-8)
  }

  hold_results[[fold_n]] <- list( results_expm1 = results_expm1,
                                  results_expm1_point_estim = results_expm1_point_estim,
                                  results_expm2 = results_expm2,
                                  results_expm2_point_estim = results_expm2_point_estim,
                                  results_expm3 = results_expm3, 
                                  results_expm3_point_estim = results_expm3_point_estim,
                                  results_surv  = results_surv, 
                                  results_surv_point_estim = results_surv_point_estim)

}
```


```{r}
make_expm_plot_entries(results_expm1, results_expm1_point_estim)
```


```{r, fig.width=13, fig.height=6}
# Extract the plots
for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  results_expm1 = hold_results[[fold_n]]$results_expm1
  results_expm2 = hold_results[[fold_n]]$results_expm2
  results_expm3 = hold_results[[fold_n]]$results_expm3
  results_surv = hold_results[[fold_n]]$results_surv
  
  results_expm1_point_estim = hold_results[[fold_n]]$results_expm1_point_estim
  results_expm2_point_estim = hold_results[[fold_n]]$results_expm2_point_estim
  results_expm3_point_estim = hold_results[[fold_n]]$results_expm3_point_estim
  results_surv_point_estim = hold_results[[fold_n]]$results_surv_point_estim

  expm_df_plot1 <- make_expm_plot_entries(results_expm1, results_expm1_point_estim)
  expm_df_plot2 <- make_expm_plot_entries(results_expm2,  results_expm2_point_estim)
  expm_df_plot3 <- make_expm_plot_entries(results_expm3,  results_expm3_point_estim)
  surv_df_plot  <- make_surv_plot_entries(results_surv,  results_surv_point_estim)

  expm_df_plot1$Notation <- "C"
  expm_df_plot2$Notation <- "C_tau"
  expm_df_plot3$Notation <- "C_tau_10"  
  surv_df_plot$Notation  <- "C_td" 
  
  df_plot <- bind_rows(expm_df_plot1, expm_df_plot2, expm_df_plot3, surv_df_plot)
  df_plot$Metric <- sub("::.*", "", df_plot$Metric)
  
  df_plot$Notation <- factor(df_plot$Notation,
    levels = c("C", "C_tau_10", "C_tau", "C_td"),
    labels = c(
      "tilde(C)~(Expected~Mortality)",
      "tilde(C)[tau]~(Expected~Mortality~tau==10~years)",
      "tilde(C)[tau]~(Expected~Mortality~tau==max*(T:~Delta==1))",
      "tilde(C)[td]~(Survival~Distribution)"
    )
  )
  
  df_plot$Model <- factor(df_plot$Model, levels = c("DeepSurv", "CoxTime", "CoxPH", "RSF", "DeepHit"))
  
  df_plot$Metric <- factor(df_plot$Metric, levels = unique(df_plot$Metric))
  # Get numeric positions of each Metric for vertical lines
  metric_levels <- levels(df_plot$Metric)
  n_metrics <- length(metric_levels)
  
  # Define where to put the vertical lines between metrics
  separator_positions <- seq(1.5, n_metrics - 0.5, by = 1)
  
  ### Version with color
   g <- ggplot(df_plot, aes(x = Metric, y = cindex, color = Model)) +
    geom_pointrange(aes(ymin = lower, ymax = upper),
                    position = position_dodge(width = 0.4), size = 0.5) +
     facet_wrap(~ Notation, nrow = 1, scales = "free_x", labeller = label_parsed)+
     geom_vline(xintercept = separator_positions, 
                linetype = "dashed", color = "grey70") +
     #geom_hline(linetype = "dashed", yintercept = 0.5) +
    #geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
    labs(title = paste0("Fold ", fold_n),
         y = "C-index", x = NULL) +
    theme_minimal(base_size = 14) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank())
  
   print(g)
   
  ### Version for paper
  p <- ggplot(df_plot, aes(x = Metric, y = cindex)) +
    geom_pointrange(aes(ymin = lower, ymax = upper, shape=Model),
                    position = position_dodge(width = 0.6), size = 0.5, color = "black") +
     facet_wrap(~ Notation, nrow = 1, scales = "free_x", labeller = label_parsed)+
     scale_shape_manual(values = c("RSF" = 15, "DeepHit" = 23, 
                                   "CoxPH" = 19, "DeepSurv" = 22, "CoxTime" = 24)) +
     geom_vline(xintercept = separator_positions, 
                linetype = "dashed", color = "grey70") +
     #geom_hline(linetype = "dashed", yintercept = 0.5) +
     #geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
     labs(title = paste0("Fold ", fold_n),
         y = "C-index", x = NULL) +
     theme_minimal(base_size = 14) +
     theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom",
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank())
  
  print(p)
  
  ### plot also the censoring probs and the ipcw:
  test <- folds_stacked_predictions[folds_stacked_predictions$cv_fold == fold_n, ] 
  
  tmp <- test

  tmp$censor.status <- ifelse(tmp$test_time %in% c(1, 2), 1, 0)
  
  tmp <- tmp[order(tmp$test_time),]
  
  weight.i <- pec::ipcw(formula=Surv(test_time,censor.status)~1,
                   data=tmp,
                   method="marginal",
                   times=unique(tmp$test_time),
                   subjectTimes=tmp$test_time, 
                   what = "IPCW.subjectTimes")$IPCW.subjectTimes
  weight.j <- pec::ipcw(formula=Surv(test_time,censor.status)~1,
                   data=tmp,
                   method="marginal",
                   times=unique(tmp$test_time),
                   subjectTimes=tmp$test_time,
                   subjectTimesLag=0,
                   what="IPCW.times")$IPCW.times
  
   time_points_test <- unique(tmp$test_time)
  
  
   fit_test <- prodlim(Hist(test_time, test_status) ~ 1, data = tmp, reverse = TRUE)

  # plot(time_points, weight.j,
  #      type = "l",
  #      col = "black",
  #      lwd = 2,
  #      xlab = "Time",
  #      ylab = expression(G[2](t) == (1 / w[t])^2),
  #      main = expression("Variation of " ~ G[2](t) ~ "over time"))
  # 
  # Compute G2
  IPCWtest <- 1 / weight.j^2
  
  tmp <- folds_stacked_predictions[folds_stacked_predictions$cv_fold != fold_n, ] 
  
  fit_train <- prodlim(Hist(test_time, test_status) ~ 1, data = tmp, reverse = TRUE)

  tmp$censor.status <- ifelse(tmp$test_time %in% c(1, 2), 1, 0)
  
  tmp <- tmp[order(tmp$test_time),]
  
  weight.i <- pec::ipcw(formula=Surv(test_time,censor.status)~1,
                   data=tmp,
                   method="marginal",
                   times=unique(tmp$test_time),
                   subjectTimes=tmp$test_time, 
                   what = "IPCW.subjectTimes")$IPCW.subjectTimes
  weight.j <- pec::ipcw(formula=Surv(test_time,censor.status)~1,
                   data=tmp,
                   method="marginal",
                   times=unique(tmp$test_time),
                   subjectTimes=tmp$test_time,
                   subjectTimesLag=0,
                   what="IPCW.times")$IPCW.times
  
  IPCWtrain <- 1 / weight.j^2
  
  
  time_points <- unique(tmp$test_time)
  
  # Plot G2 vs time
  plot(time_points_test, IPCWtest,
       type = "l",
       col = "black",
       lwd = 2,
       xlab = paste0("Fold ", fold_n))
  lines(time_points, IPCWtrain ,col="blue")

  plot(fit_test,
       xlab = "Time (months)",
       ylab = "Censoring probability",
       confint = TRUE,
       legend = FALSE,
       col = "steelblue",
       lwd = 2,
       ylim = c(0, 1), 
       main = paste0("Fold ", fold_n))
  
  plot(fit_train,
       add = TRUE,
       confint = TRUE,
       legend = FALSE,
       col = "darkorange",
       lwd = 2)
}

#ggsave("./Results/PaperTables/ExpectedMortalityBlack.png", plot = p, width = 12, height = 6, dpi = 300)
```


### Table with results


```{r}


notation_map <- data.frame(
  Metric = c("pycox.Ant", "pycox.Adj.Ant", 
             "pec", "survC1", "survival.n/G2", "sksurv.ipcw","survival.n", 
             "Hmisc", "SurvMetrics", "lifelines", "pysurvival", 
             "sksurv.censored"),
  Notation = c("$C_{td}$", "$C_{td}$", 
               "$C_{\\tau}$", "$C_{\\tau}$", "$C_{\\tau}$", 
               "$C_{\\tau}$", "$C_{\\tau}$",
               "$C$", "$C$", "$C$", "$C$", "$C$"),
  stringsAsFactors = FALSE
)

all_folds <- list()

for (fold_n in unique(folds_stacked_predictions$cv_fold)) {
  
  results_expm1 = hold_results[[fold_n]]$results_expm1
  results_expm2 = hold_results[[fold_n]]$results_expm2
  results_expm3 = hold_results[[fold_n]]$results_expm3
  results_surv = hold_results[[fold_n]]$results_surv
  
  results_expm1_point_estim = hold_results[[fold_n]]$results_expm1_point_estim
  results_expm2_point_estim = hold_results[[fold_n]]$results_expm2_point_estim
  results_expm3_point_estim = hold_results[[fold_n]]$results_expm3_point_estim
  results_surv_point_estim = hold_results[[fold_n]]$results_surv_point_estim

  a <- make_expm_table(results_expm1, results_expm1_point_estim)
  a$Fold <- fold_n
  b <- make_expm_table(results_expm2, results_expm2_point_estim)
  b$InputType <- "Exp.Mort tau=max(T, delta=1)"
  b$Fold <- fold_n
  c <- make_expm_table(results_expm3, results_expm3_point_estim)
  c$InputType <- "Exp.Mort tau=10y"
  c$Fold <- fold_n
  d <- make_surv_table(results_surv, results_surv_point_estim)
  d$Fold <- fold_n
  
  df_combined <- bind_rows(a, b, c, d)
  
  df_combined$Metric <- sub("::.*", "", df_combined$Metric)
  df_combined <- merge(df_combined, notation_map, by= "Metric", all.x = TRUE)

  all_folds[[fold_n]] <- df_combined

} 

final_table <- bind_rows(all_folds)


```




#### Save table


```{r}
library(kableExtra)

df_combined_bolded <- final_table

for (i in 1:nrow(df_combined_bolded)) {
  row <- df_combined_bolded[i, model_names]
  # only the mean, not the ci
  numeric_values <- as.numeric(sub(" .*", "", row))

  max_index <- which.max(numeric_values)
  
  # only bold the mean
  row[max_index] <- sub("^(\\d+\\.\\d+)", "\\\\textbf{\\1}", row[max_index])
  
  df_combined_bolded[i, model_names] <- row
}

df_combined_bolded <- df_combined_bolded %>% arrange(factor(InputType, levels = c("Distrib", "Exp.Mort", "Exp.Mort tau=10y", "Exp.Mort tau=max(T, delta=1)" )))


latex_code <- kable(df_combined_bolded, format = "latex", 
                    booktabs = TRUE, 
                    escape = FALSE)

writeLines(latex_code, "Paper/Tables/results_tableHoldOut.tex")
#writeLines(latex_code, "Results/PaperTables/results_table5fold_Seed123.tex")

```

